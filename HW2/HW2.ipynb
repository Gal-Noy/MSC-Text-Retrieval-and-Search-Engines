{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66836ee9",
   "metadata": {},
   "source": [
    "# HW2 - WET PART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c86b5",
   "metadata": {},
   "source": [
    "### Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0ba90115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import logging\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.analysis import get_lucene_analyzer\n",
    "from pyserini.index.lucene import IndexReader\n",
    "from pyserini.search import get_topics_with_reader\n",
    "\n",
    "# Suppress warnings and Java logging\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "# Part A\n",
    "PART_A_DIR = BASE_DIR / \"data\" / \"WET_PART_A\"\n",
    "PART_A_INDEX = BASE_DIR / \"indexes\" / \"part_a_index\"\n",
    "\n",
    "# Part B\n",
    "AP_COLL_PATH = BASE_DIR / \"data\" / \"AP_Coll\"\n",
    "PART_B_DIR = BASE_DIR / \"data\" / \"WET_PART_B\"\n",
    "QUERIES_PATH = PART_B_DIR / \"queries.txt\"\n",
    "STOPWORDS_PATH = PART_B_DIR / \"StopWords.txt\"\n",
    "QRELS_PATH = PART_B_DIR / \"qrels_AP\"\n",
    "\n",
    "INDEX_STEMMED = BASE_DIR / \"indexes\" / \"ap_stemmed\"\n",
    "INDEX_UNSTEMMED = BASE_DIR / \"indexes\" / \"ap_unstemmed\"\n",
    "\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "RESULTS_STEMMED = RESULTS_DIR / \"stemmed.trec\"\n",
    "RESULTS_UNSTEMMED = RESULTS_DIR / \"unstemmed.trec\"\n",
    "EVAL_STEMMED = RESULTS_DIR / \"stemmed_eval.txt\"\n",
    "EVAL_UNSTEMMED = RESULTS_DIR / \"unstemmed_eval.txt\"\n",
    "\n",
    "# Create directories\n",
    "for path in [PART_A_INDEX, INDEX_STEMMED, INDEX_UNSTEMMED, RESULTS_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088323f",
   "metadata": {},
   "source": [
    "## Part A: Index docs.txt and test queries\n",
    "Build index with `--keepStopwords --stemmer none` and test with specific queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c1209fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index at /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/indexes/part_a_index...\n",
      "Index built successfully\n",
      "Index built successfully\n"
     ]
    }
   ],
   "source": [
    "def build_index(collection_path, index_path, stemmer='none', stopwords_path=None, keep_stopwords=False):\n",
    "    \"\"\"Build a Lucene index using pyserini.\"\"\"\n",
    "    if index_path.exists() and any(index_path.iterdir()):\n",
    "        print(f\"Index already exists: {index_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Building index at {index_path}...\")\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"pyserini.index.lucene\",\n",
    "        \"--collection\", \"TrecCollection\",\n",
    "        \"--input\", str(collection_path),\n",
    "        \"--index\", str(index_path),\n",
    "        \"--stemmer\", stemmer,\n",
    "        \"--storePositions\",\n",
    "        \"--storeDocvectors\",\n",
    "        \"--storeRaw\",\n",
    "        \"--optimize\"\n",
    "    ]\n",
    "    \n",
    "    if keep_stopwords:\n",
    "        cmd.append(\"--keepStopwords\")\n",
    "    elif stopwords_path:\n",
    "        cmd.extend([\"--stopwords\", str(stopwords_path)])\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Index built successfully\")\n",
    "    else:\n",
    "        print(f\"Build failed: {result.stderr}\")\n",
    "\n",
    "build_index(PART_A_DIR, PART_A_INDEX, stemmer='none', keep_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a18bf",
   "metadata": {},
   "source": [
    "### Check index statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "218b0827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part A Index Statistics:\n",
      "total_terms              : 212\n",
      "documents                : 4\n",
      "non_empty_documents      : 4\n",
      "unique_terms             : 140\n"
     ]
    }
   ],
   "source": [
    "reader = IndexReader(str(PART_A_INDEX))\n",
    "stats = reader.stats()\n",
    "\n",
    "print(\"\\nPart A Index Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key:25s}: {value:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17958051",
   "metadata": {},
   "source": [
    "### Configure searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1057995b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher configured for Part A queries\n"
     ]
    }
   ],
   "source": [
    "searcher = LuceneSearcher(str(PART_A_INDEX))\n",
    "analyzer = get_lucene_analyzer(stemmer='none', stopwords=False)\n",
    "searcher.set_analyzer(analyzer)\n",
    "searcher.set_bm25(k1=0.9, b=0.4)\n",
    "\n",
    "print(\"Searcher configured for Part A queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf868c1",
   "metadata": {},
   "source": [
    "### Question 1: Query \"corporation\"\n",
    "\n",
    "**1a. How many documents did you retrieve?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "036a30a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'corporation'\n",
      "Number of documents retrieved: 1\n",
      "\n",
      "1. D2 - Score: 0.6747\n"
     ]
    }
   ],
   "source": [
    "query = 'corporation'\n",
    "hits = searcher.search(query, k=4)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Number of documents retrieved: {len(hits)}\\n\")\n",
    "for i, hit in enumerate(hits, start=1):\n",
    "    print(f\"{i}. {hit.docid} - Score: {hit.score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e78bf",
   "metadata": {},
   "source": [
    "**1b. How many documents did you expect to retrieve?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3758884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents containing 'corporation' (or variants):\n",
      "\n",
      "D2: IBM corporation started with the IBM 1410, a member of the  IBM 1400 series, was a variable wordlength decimal computer that was announced by IBM on September 12 1960 and marketed as a midrange \"Business Computer\"\n",
      "D3: At the same time, Burroughs was very much a competitor in which these two corporations tried to supply a complete answer for its customers\n",
      "\n",
      "Expected: 2 documents\n",
      "Retrieved: 1 documents\n",
      "\n",
      "Missing 1 document(s)!\n",
      "Reason: D3 contains 'corporations' (plural), but index has no stemming.\n",
      "Without stemming, 'corporation' ≠ 'corporations'\n"
     ]
    }
   ],
   "source": [
    "# Read docs.txt\n",
    "docs_path = PART_A_DIR / \"docs.txt\"\n",
    "with open(docs_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Extract all documents\n",
    "docs = re.findall(r'<DOC>.*?</DOC>', content, re.DOTALL)\n",
    "\n",
    "print(\"Documents containing 'corporation' (or variants):\\n\")\n",
    "expected_count = 0\n",
    "for doc in docs:\n",
    "    docno = re.search(r'<DOCNO>(.*?)</DOCNO>', doc).group(1)\n",
    "    text = re.search(r'<TEXT>(.*?)</TEXT>', doc, re.DOTALL).group(1)\n",
    "    \n",
    "    if 'corporation' in text.lower():\n",
    "        expected_count += 1\n",
    "        # Find the sentence with corporation\n",
    "        for sent in text.split('.'):\n",
    "            if 'corporation' in sent.lower():\n",
    "                print(f\"{docno}: {sent.strip()}\")\n",
    "                break\n",
    "\n",
    "print(f\"\\nExpected: {expected_count} documents\")\n",
    "print(f\"Retrieved: {len(hits)} documents\")\n",
    "\n",
    "if expected_count > len(hits):\n",
    "    print(f\"\\nMissing {expected_count - len(hits)} document(s)!\")\n",
    "    print(\"Reason: D3 contains 'corporations' (plural), but index has no stemming.\")\n",
    "    print(\"Without stemming, 'corporation' ≠ 'corporations'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76feedd2",
   "metadata": {},
   "source": [
    "### Question 2: Query to return D1 first (max 2 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d9be2909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Nobel Prize'\n",
      "\n",
      "1. D1 - Score: 1.9240\n",
      "\n",
      "Explanation:\n",
      "'Nobel Prize' appears 4 times in D1 and nowhere else in the collection.\n",
      "BM25 rewards terms with high frequency in a document and low frequency across the collection.\n",
      "Therefore, D1 gets the highest score and ranks first.\n"
     ]
    }
   ],
   "source": [
    "# D1 is about the Nobel Prize - use these distinctive terms\n",
    "query = 'Nobel Prize'\n",
    "hits = searcher.search(query, k=10)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "for i, hit in enumerate(hits, start=1):\n",
    "    print(f\"{i}. {hit.docid} - Score: {hit.score:.4f}\")\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"'Nobel Prize' appears 4 times in D1 and nowhere else in the collection.\")\n",
    "print(\"BM25 rewards terms with high frequency in a document and low frequency across the collection.\")\n",
    "print(\"Therefore, D1 gets the highest score and ranks first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bc56f9",
   "metadata": {},
   "source": [
    "### Question 3: Is D4 relevant to \"Michael Jackson\"?\n",
    "\n",
    "**3a. Run query and analyze relevance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "095f393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Michael Jackson'\n",
      "\n",
      "1. D4 - Score: 1.1867\n",
      "\n",
      "======================================================================\n",
      "D4 Content:\n",
      "======================================================================\n",
      "Biography Stefania Gabriella Germanotta (AKA Lady GaGa) was born in March 1986, in New York, to an Italian family. She attended Convent of the Sacred Heart school and, as a little girl, she remembers singing into a plastic tape recorder to the likes of   Cyndi Lauper and Michael Jackson. \n",
      "By age 4, Lady Gaga had taught herself to play the piano by ear, and when she was a teenager she penned her first song.\n",
      "\n",
      "======================================================================\n",
      "Answer: NO, D4 is NOT relevant to 'Michael Jackson'\n",
      "======================================================================\n",
      "Reason: D4 is about Lady Gaga. Michael Jackson is only mentioned once\n",
      "as someone she listened to as a child - a brief reference, not the main topic.\n"
     ]
    }
   ],
   "source": [
    "query = 'Michael Jackson'\n",
    "hits = searcher.search(query, k=10)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "for i, hit in enumerate(hits, start=1):\n",
    "    print(f\"{i}. {hit.docid} - Score: {hit.score:.4f}\")\n",
    "\n",
    "# Show D4 content\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"D4 Content:\")\n",
    "print(\"=\"*70)\n",
    "d4_match = re.search(r'<DOCNO>D4</DOCNO>.*?<TEXT>(.*?)</TEXT>', content, re.DOTALL)\n",
    "if d4_match:\n",
    "    print(d4_match.group(1).strip())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Answer: NO, D4 is NOT relevant to 'Michael Jackson'\")\n",
    "print(\"=\"*70)\n",
    "print(\"Reason: D4 is about Lady Gaga. Michael Jackson is only mentioned once\")\n",
    "print(\"as someone she listened to as a child - a brief reference, not the main topic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae33c9c",
   "metadata": {},
   "source": [
    "**3b. Query for which D4 IS relevant (max 2 words):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "957068c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Lady Gaga'\n",
      "\n",
      "1. D4 - Score: 1.5899\n",
      "\n",
      "Explanation:\n",
      "'Lady Gaga' is the main topic of D4, appearing 3 times throughout the document.\n",
      "This query correctly identifies D4 as highly relevant with a much higher score.\n"
     ]
    }
   ],
   "source": [
    "# D4 is about Lady Gaga - this should retrieve it with high relevance\n",
    "query = 'Lady Gaga'\n",
    "hits = searcher.search(query, k=10)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "for i, hit in enumerate(hits, start=1):\n",
    "    print(f\"{i}. {hit.docid} - Score: {hit.score:.4f}\")\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"'Lady Gaga' is the main topic of D4, appearing 3 times throughout the document.\")\n",
    "print(\"This query correctly identifies D4 as highly relevant with a much higher score.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70bcc85",
   "metadata": {},
   "source": [
    "## Part B: Build 2 indexes and compare retrieval effectiveness\n",
    "1. **Index 1**: WITH stopwords removal AND WITH Krovetz stemming\n",
    "2. **Index 2**: WITH stopwords removal AND WITHOUT stemming\n",
    "3. Compare MAP, P@5, P@10 for both indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa9e4c",
   "metadata": {},
   "source": [
    "### Index 1: WITH Krovetz stemming + WITH stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e7e73aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index at /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/indexes/ap_stemmed...\n",
      "Index built successfully\n",
      "Index built successfully\n"
     ]
    }
   ],
   "source": [
    "build_index(AP_COLL_PATH, INDEX_STEMMED, stemmer='krovetz', stopwords_path=STOPWORDS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61126876",
   "metadata": {},
   "source": [
    "### Index 2: WITHOUT stemming + WITH stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "06bb2904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index at /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/indexes/ap_unstemmed...\n",
      "Index built successfully\n",
      "Index built successfully\n"
     ]
    }
   ],
   "source": [
    "build_index(AP_COLL_PATH, INDEX_UNSTEMMED, stemmer='none', stopwords_path=STOPWORDS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527b5ba1",
   "metadata": {},
   "source": [
    "### Load Queries and Perform Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2e29e228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 queries\n",
      "Sample: [('088', 'crude  oil  price  trends'), ('089', 'downstream  investments  opec  member  states'), ('190', 'instances  fraud  involving  computer')]\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics_with_reader('io.anserini.search.topicreader.TsvIntTopicReader', str(QUERIES_PATH))\n",
    "\n",
    "queries = {}\n",
    "for topic_id, topic in topics.items():\n",
    "    qid = f\"0{topic_id}\" if len(str(topic_id)) == 2 else str(topic_id)\n",
    "    queries[qid] = topic['title']\n",
    "\n",
    "assert len(queries) == 150, \"Expected 150 queries\"\n",
    "\n",
    "print(f\"Loaded {len(queries)} queries\")\n",
    "print(f\"Sample: {list(queries.items())[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e208b",
   "metadata": {},
   "source": [
    "### Configure search and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8034b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_save(index_path, queries, output_path, stemmer='none', k=1000):\n",
    "    \"\"\"Search index and save results in TREC format.\"\"\"\n",
    "    searcher = LuceneSearcher(str(index_path))\n",
    "    analyzer = get_lucene_analyzer(stemmer=stemmer, stopwords=False)\n",
    "    searcher.set_analyzer(analyzer)\n",
    "    searcher.set_bm25(k1=0.9, b=0.4)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        for qid in sorted(queries.keys()):\n",
    "            hits = searcher.search(queries[qid], k=k)\n",
    "            for rank, hit in enumerate(hits, start=1):\n",
    "                f.write(f\"{qid} Q0 {hit.docid} {rank} {hit.score:.4f} pyserini\\n\")\n",
    "    \n",
    "    print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad25786",
   "metadata": {},
   "source": [
    "### Search Index 1 (stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ad10eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/results/stemmed.trec\n"
     ]
    }
   ],
   "source": [
    "search_and_save(INDEX_STEMMED, queries, RESULTS_STEMMED, stemmer='krovetz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b4cccf",
   "metadata": {},
   "source": [
    "### Search Index 2 (unstemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ca659953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/results/unstemmed.trec\n"
     ]
    }
   ],
   "source": [
    "search_and_save(INDEX_UNSTEMMED, queries, RESULTS_UNSTEMMED, stemmer='none', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e90f4",
   "metadata": {},
   "source": [
    "## Evaluate effectiveness of the 2 retrieved lists\n",
    "\n",
    "### Run trec_eval using pyserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ce843ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trec_eval(qrels_path, results_path, output_path):\n",
    "    \"\"\"Run trec_eval using pyserini and save output.\"\"\"\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"pyserini.eval.trec_eval\",\n",
    "        \"-q\",\n",
    "        str(qrels_path),\n",
    "        str(results_path)\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        # Save full output to file\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(result.stdout)\n",
    "        print(f\"Evaluation saved to {output_path}\")\n",
    "        return result.stdout\n",
    "    else:\n",
    "        print(f\"Error running trec_eval: {result.stderr}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5491fbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating stemmed index...\n",
      "Evaluation saved to /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/results/stemmed_eval.txt\n",
      "\n",
      "Evaluating unstemmed index...\n",
      "Evaluation saved to /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/results/stemmed_eval.txt\n",
      "\n",
      "Evaluating unstemmed index...\n",
      "Evaluation saved to /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/results/unstemmed_eval.txt\n",
      "Evaluation saved to /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/results/unstemmed_eval.txt\n"
     ]
    }
   ],
   "source": [
    "# Run trec_eval for stemmed results\n",
    "print(\"Evaluating stemmed index...\")\n",
    "eval_output_stemmed = run_trec_eval(QRELS_PATH, RESULTS_STEMMED, EVAL_STEMMED)\n",
    "\n",
    "# Run trec_eval for unstemmed results\n",
    "print(\"\\nEvaluating unstemmed index...\")\n",
    "eval_output_unstemmed = run_trec_eval(QRELS_PATH, RESULTS_UNSTEMMED, EVAL_UNSTEMMED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72305ea7",
   "metadata": {},
   "source": [
    "### Parse and display comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dcb9fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATION RESULTS\n",
      "======================================================================\n",
      "Configuration                              MAP        P@5       P@10\n",
      "----------------------------------------------------------------------\n",
      "WITH Krovetz + WITH Stopwords           0.2144     0.4121     0.3913\n",
      "WITHOUT Stemming + WITH Stopwords       0.1896     0.4094     0.3758\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def parse_trec_eval(output):\n",
    "    \"\"\"Parse trec_eval output to extract MAP, P@5, and P@10.\"\"\"\n",
    "    metrics = {}\n",
    "    for line in output.split('\\n'):\n",
    "        if line.startswith('map ') and 'all' in line:\n",
    "            metrics['MAP'] = float(line.split()[-1])\n",
    "        elif line.startswith('P_5 ') and 'all' in line:\n",
    "            metrics['P@5'] = float(line.split()[-1])\n",
    "        elif line.startswith('P_10 ') and 'all' in line:\n",
    "            metrics['P@10'] = float(line.split()[-1])\n",
    "    return metrics\n",
    "\n",
    "if eval_output_stemmed and eval_output_unstemmed:\n",
    "    metrics_stemmed = parse_trec_eval(eval_output_stemmed)\n",
    "    metrics_unstemmed = parse_trec_eval(eval_output_unstemmed)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Configuration':<35} {'MAP':>10} {'P@5':>10} {'P@10':>10}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'WITH Krovetz + WITH Stopwords':<35} {metrics_stemmed['MAP']:>10.4f} {metrics_stemmed['P@5']:>10.4f} {metrics_stemmed['P@10']:>10.4f}\")\n",
    "    print(f\"{'WITHOUT Stemming + WITH Stopwords':<35} {metrics_unstemmed['MAP']:>10.4f} {metrics_unstemmed['P@5']:>10.4f} {metrics_unstemmed['P@10']:>10.4f}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"Error: Could not run trec_eval. Check that pyserini is installed correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f6251",
   "metadata": {},
   "source": [
    "### Determine best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bf239f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best configuration by MAP: WITH Krovetz stemming\n",
      "MAP improvement: 0.0248\n",
      "\n",
      "Explanation:\n",
      "Krovetz stemming reduces words to their root forms, improving recall by matching\n",
      "different morphological variants (e.g., 'running', 'runs', 'ran' → 'run').\n",
      "This typically improves retrieval effectiveness when queries and documents use\n",
      "different word forms for the same concepts.\n"
     ]
    }
   ],
   "source": [
    "if metrics_stemmed and metrics_unstemmed:\n",
    "    if metrics_stemmed['MAP'] > metrics_unstemmed['MAP']:\n",
    "        winner = \"WITH Krovetz stemming\"\n",
    "        diff = metrics_stemmed['MAP'] - metrics_unstemmed['MAP']\n",
    "    else:\n",
    "        winner = \"WITHOUT stemming\"\n",
    "        diff = metrics_unstemmed['MAP'] - metrics_stemmed['MAP']\n",
    "    \n",
    "    print(f\"\\nBest configuration by MAP: {winner}\")\n",
    "    print(f\"MAP improvement: {diff:.4f}\")\n",
    "    print(\"\\nExplanation:\")\n",
    "    print(\"Krovetz stemming reduces words to their root forms, improving recall by matching\")\n",
    "    print(\"different morphological variants (e.g., 'running', 'runs', 'ran' → 'run').\")\n",
    "    print(\"This typically improves retrieval effectiveness when queries and documents use\")\n",
    "    print(\"different word forms for the same concepts.\")\n",
    "else:\n",
    "    print(\"Metrics not available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
