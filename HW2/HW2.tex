\documentclass[12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\geometry{margin=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.8em}

\title{Dry Part – Assignment 2\\Text Retrieval and Search Engines}
\author{Gal Noy \\ 209346485}
\date{}

\begin{document}
\maketitle

\section*{1. Vector Space Model}

\[
\begin{array}{c|cccc}
      & d_1 & d_2 & d_3 & d_4 \\ \hline
a     & 0   & 1   & 1   & 1   \\
b     & 1   & 2   & 0   & 1   \\
c     & 2   & 0   & 0   & 0   \\
d     & 0   & 0   & 0   & 0   \\
e     & 1   & 0   & 1   & 1   \\
f     & 7   & 5   & 7   & 2   \\
\end{array}
\]

\[
p(a)=0.1,\quad p(b)=0.1,\quad p(c)=0.2,\quad p(d)=0.05,\quad p(e)=0.5,\quad p(f)=0.9.
\]

Let
\[
\mathrm{idf}_t = \ln\!\left(\frac{1}{p(t)}\right),
\]
so
\[
\begin{aligned}
\mathrm{idf}_a &= \mathrm{idf}_b = \ln 10 \approx 2.303,\\
\mathrm{idf}_c &= \ln 5 \approx 1.609,\\
\mathrm{idf}_d &= \ln 20 \approx 2.996,\\
\mathrm{idf}_e &= \ln 2 \approx 0.693,\\
\mathrm{idf}_f &= \ln(1/0.9) \approx 0.105.
\end{aligned}
\]

\subsection*{1.1 Cosine similarity between \(d_1\) and \(d_2\)}

tf–idf weighting:
\[
w_{t,d} = \mathrm{tf}_{t,d}\,\mathrm{idf}_t.
\]

\[
\begin{aligned}
d_1 &: (0,1,2,0,1,7),\\
d_2 &: (1,2,0,0,0,5).
\end{aligned}
\]

\[
\begin{aligned}
\vec{d}_1 &=
(0,\;\ln 10,\;2\ln 5,\;0,\;\ln 2,\;7\ln(1/0.9)) \\
&\approx (0,\;2.303,\;3.219,\;0,\;0.693,\;0.738),\\
\vec{d}_2 &=
(\ln 10,\;2\ln 10,\;0,\;0,\;0,\;5\ln(1/0.9)) \\
&\approx (2.303,\;4.606,\;0,\;0,\;0,\;0.527).
\end{aligned}
\]

Dot product:
\[
\begin{aligned}
\vec{d}_1\cdot\vec{d}_2
&= (\ln 10)(2\ln 10) + (7\ln(1/0.9))(5\ln(1/0.9))\\
&= 2(\ln 10)^2 + 35(\ln(1/0.9))^2 \approx 10.992.
\end{aligned}
\]

Norms:
\[
\begin{aligned}
\|\vec{d}_1\|^2
&= (\ln 10)^2 + (2\ln 5)^2 + (\ln 2)^2 + (7\ln(1/0.9))^2 \\
&\approx 16.688,\quad \|\vec{d}_1\|\approx 4.085,\\
\|\vec{d}_2\|^2
&= (\ln 10)^2 + (2\ln 10)^2 + (5\ln(1/0.9))^2 \\
&\approx 26.787,\quad \|\vec{d}_2\|\approx 5.176.
\end{aligned}
\]

Cosine:
\[
\cos(\vec{d}_1,\vec{d}_2)
= \frac{10.992}{4.085\cdot 5.176}
\approx 0.52.
\]

\subsection*{1.2 Ranking \(d_1,\dots,d_4\) for query \(q = \text{``a b f''}\)}

Documents: raw tf. Query: tf–idf.

\[
\vec{q} = (\mathrm{idf}_a,\mathrm{idf}_b,0,0,0,\mathrm{idf}_f)
\approx (2.303,2.303,0,0,0,0.105),
\quad \|\vec{q}\|\approx 3.258.
\]

\[
\begin{aligned}
\vec{d}_1 &= (0,1,2,0,1,7), & \|\vec{d}_1\| &= \sqrt{55}\approx 7.416,\\
\vec{d}_2 &= (1,2,0,0,0,5), & \|\vec{d}_2\| &= \sqrt{30}\approx 5.477,\\
\vec{d}_3 &= (1,0,0,0,1,7), & \|\vec{d}_3\| &= \sqrt{51}\approx 7.141,\\
\vec{d}_4 &= (1,1,0,0,1,2), & \|\vec{d}_4\| &= \sqrt{7}\approx 2.646.
\end{aligned}
\]

Dot products (only \(a,b,f\) matter):
\[
\begin{aligned}
\vec{d}_1\cdot\vec{q} &\approx 2.303 + 7\cdot 0.105 \approx 3.04,\\
\vec{d}_2\cdot\vec{q} &\approx 3\cdot 2.303 + 5\cdot 0.105 \approx 7.44,\\
\vec{d}_3\cdot\vec{q} &\approx 2.303 + 7\cdot 0.105 \approx 3.04,\\
\vec{d}_4\cdot\vec{q} &\approx 2\cdot 2.303 + 2\cdot 0.105 \approx 4.82.
\end{aligned}
\]

Cosine similarities:
\[
\begin{aligned}
\cos(d_1,q) &\approx \frac{3.04}{7.416\cdot 3.258} \approx 0.13,\\
\cos(d_2,q) &\approx \frac{7.44}{5.477\cdot 3.258} \approx 0.42,\\
\cos(d_3,q) &\approx \frac{3.04}{7.141\cdot 3.258} \approx 0.13,\\
\cos(d_4,q) &\approx \frac{4.82}{2.646\cdot 3.258} \approx 0.56.
\end{aligned}
\]

Ranking:
\[
d_4 \succ d_2 \succ d_3 \succ d_1.
\]

\section*{2. Term Weighting}

\subsection*{2.1 Short-document bias}

Cosine similarity normalizes by \(\|\vec{d}\|\). For two documents with similar raw tf on query terms, the shorter document has smaller \(\|\vec{d}\|\), so its normalized weights are larger and its cosine score tends to be higher. This favors short documents even when longer ones may be more informative.

\subsection*{2.2 Weighted tf function}

Given
\[
W_{t,d} = \alpha + (1-\alpha)\frac{\mathrm{tf}_{t,d}}{\mathrm{tf}_{\max}(d)},
\quad 0\le \alpha\le 1.
\]

\textbf{Usefulness:}  
Normalizes tf into \([\alpha,1]\) per document via \(\mathrm{tf}_{t,d}/\mathrm{tf}_{\max}(d)\), reducing dominance of extremely frequent terms and partly compensating for document length.

\textbf{Issue:}  
If applied to all vocabulary terms, terms with \(\mathrm{tf}_{t,d}=0\) also get weight \(\alpha>0\), meaning documents get non-zero weight for absent terms, which blurs the distinction between presence and absence and can hurt precision.

\section*{3. Relevance Feedback and Evaluation}

\subsection*{3.1 AP, P@5 and Recall@5}

Top-5 list:

\begin{center}
\begin{tabular}{c|c}
\textbf{DocID} & \textbf{Rel.} \\ \hline
5 & 4 \\
2 & 1 \\
1 & 1 \\
3 & 3 \\
4 & 0 \\
\end{tabular}
\end{center}

Relevant grades \(>0\); total relevant \(R=10\).  
Relevant at ranks \(1,2,3,4\).

\[
P@5 = \frac{4}{5} = 0.8,\qquad
\text{Recall@5} = \frac{4}{10} = 0.4.
\]

Precisions at relevant ranks:
\[
P@1=1,\;P@2=1,\;P@3=1,\;P@4=1.
\]

Average Precision:
\[
\mathrm{AP} = \frac{1}{R}\sum_{k} P@k \cdot \mathrm{rel}(k)
= \frac{1}{10}(1+1+1+1) = 0.4.
\]

\subsection*{3.2 Rocchio with graded relevance}

In standard Rocchio, all relevant documents contribute equally:

\[
\vec{q}_m
= \alpha\,\vec{q}_0
+ \frac{\beta}{|D_R|}\sum_{d\in D_R}\vec{d}
- \frac{\gamma}{|D_{NR}|}\sum_{d\in D_{NR}}\vec{d}.
\]

With graded relevance, each document has a grade  
\(\mathrm{grade}(d) \in \{0,\dots,r_{\max}\}\).  
To incorporate this information, we normalize the grade:

\[
\mathrm{rel}(d)=\frac{\mathrm{grade}(d)}{r_{\max}},
\]

so the most relevant document receives weight \(1\), and others receive proportionally smaller weights.  
This allows Rocchio to give stronger influence to documents judged as more relevant.

The modified formula becomes:

\[
\vec{q}_m
= \alpha\,\vec{q}_0
+ \beta\,\frac{1}{Z_R}\sum_{d\in D_R}\mathrm{rel}(d)\,\vec{d}
- \gamma\,\frac{1}{|D_{NR}|}\sum_{d\in D_{NR}}\vec{d},
\qquad
Z_R = \sum_{d\in D_R} \mathrm{rel}(d).
\]

The normalization factor \(Z_R\) ensures that the weighted contribution of relevant documents remains on a consistent scale across queries (e.g., regardless of how many relevant documents exist or how large their grades are).

\subsection*{3.3 Rocchio with rank-based relevance}

Another approach is to weight relevant documents based on their rank position.  
A higher-ranked document should influence the query more than one that appears later.  
A simple decreasing weighting function is:

\[
g(\mathrm{rank}(d)) = \frac{1}{\mathrm{rank}(d)}.
\]

Thus, the first relevant document receives weight \(1\), the second receives \(1/2\), the third \(1/3\), and so on.  
This reflects the intuition that early-ranked results are generally better signals of what the user prefers.

The Rocchio update becomes:

\[
\vec{q}_m
= \alpha\,\vec{q}_0
+ \beta\,\frac{1}{Z}
\sum_{d\in D_R} g(\mathrm{rank}(d))\,\vec{d}
- \gamma\,\frac{1}{|D_{NR}|}\sum_{d\in D_{NR}}\vec{d},
\qquad
Z = \sum_{d\in D_R} g(\mathrm{rank}(d)).
\]

The normalization factor \(Z\) keeps the weighted sum of ranked contributions comparable across queries, preventing queries with many relevant documents from overwhelming the update.

\section*{4. Evaluation}

\subsection*{4.1 ERR vs NDCG}

ERR is defined as:
\[
\mathrm{ERR} = \sum_{i=1}^{k}
\frac{1}{i}
\left(\prod_{j=1}^{i-1}(1-R_j)\right) R_i,
\qquad
R_i = \frac{\mathrm{rel}_i}{\mathrm{rel}_{\max}}.
\]

Here \(R_i\) is a normalized satisfaction probability:  
a document with the highest grade receives \(R_i = 1\), and lower grades receive proportionally smaller values.  
ERR models a user who scans results from the top and stops at rank \(i\) with probability \(R_i\), so highly relevant documents at early ranks strongly affect the score.

We compare two ranked lists (with \(\mathrm{rel}_{\max}=3\)):

\[
A = [3,0,0,0],
\qquad
B = [2,2,2,2].
\]

\textbf{NDCG comparison.}  
Using the simple form of DCG:
\[
\mathrm{DCG} = \sum_{i=1}^{4} \frac{\mathrm{rel}_i}{\log_2(i+1)}.
\]

\[
\mathrm{DCG}_A = \frac{3}{\log_2 2} = 3.
\]

\[
\mathrm{DCG}_B
= \frac{2}{\log_2 2}
+ \frac{2}{\log_2 3}
+ \frac{2}{\log_2 4}
+ \frac{2}{\log_2 5}
\approx 5.123.
\]

The ideal ranking places the grade 3 document first:
\[
\mathrm{DCG}_{\text{ideal}}
= \frac{3}{\log_2 2}
+ \frac{2}{\log_2 3}
+ \frac{2}{\log_2 4}
+ \frac{2}{\log_2 5}
\approx 6.123.
\]

\[
\mathrm{NDCG}_A = \frac{3}{6.123} \approx 0.49,
\qquad
\mathrm{NDCG}_B = \frac{5.123}{6.123} \approx 0.84.
\]

NDCG prefers B, because it rewards having several moderately relevant documents across the top ranks.

\textbf{ERR comparison.}  
Normalized satisfaction values:
\[
R(3)=\frac{3}{3}=1,\qquad
R(2)=\frac{2}{3},\qquad
R(0)=0.
\]

For A:
\[
\mathrm{ERR}_A = 1.
\]

For B (all \(R_i = 2/3\)):
\[
\mathrm{ERR}_B
= 1\cdot \frac{2}{3}
+ \frac{1}{2}(1-\tfrac{2}{3})\tfrac{2}{3}
+ \frac{1}{3}(1-\tfrac{2}{3})^2 \tfrac{2}{3}
+ \frac{1}{4}(1-\tfrac{2}{3})^3 \tfrac{2}{3}
\approx 0.81.
\]

Thus:
\[
\mathrm{ERR}_A = 1 \;>\; \mathrm{ERR}_B \approx 0.81.
\]

ERR prefers A, because the user is assumed to stop once a highly relevant document appears at the top.  
This fits tasks where a single excellent result early in the list is more valuable than several moderately relevant ones spread throughout the ranking.

\subsection*{4.2 When does MAP = MRR?}

MAP equals MRR when, for every query, the Average Precision reduces to the precision at the first relevant document. This happens in the following cases:

\begin{itemize}
\item Each query has exactly one relevant document in the entire collection.
\item The system retrieves at most one relevant document for each query (even if more exist).
\item All documents are relevant (or all are non-relevant). If all are relevant, AP = RR = 1 for every query. If none are relevant, both AP and RR are 0.
\item No query has more than one retrieved relevant document at different ranks (so the contribution to AP comes only from the first relevant match).
\end{itemize}

\subsection*{4.3 Stopword removal}

\subsubsection*{(a) Lower recall}

\begin{itemize}
\item ``to be or not to be'': removing stopwords erases most of the query terms, making it impossible to retrieve documents that contain the full phrase, which lowers recall.

\item ``The Who discography'': removing \texttt{the} removes part of the band’s actual name ``The Who'', causing the system to miss relevant documents that contain the exact entity.
\end{itemize}

\subsubsection*{(b) Lower precision}

\begin{itemize}
\item ``Bank of America'' \(\to\) ``bank america'': removing \texttt{of} breaks the specific organization name, leading the system to retrieve general documents about banks in America rather than the actual company, lowering precision.

\item ``vitamin A deficiency'' \(\to\) ``vitamin deficiency'': the reduced query matches deficiencies of any vitamin (B, C, D, etc.), producing many broader, non-relevant documents and reducing precision.
\end{itemize}


\subsection*{4.4 R-precision, F1 and AP bounds}

There are \(R = 3\) relevant documents in the collection.  
Given R-precision \(= \frac{2}{3}\), exactly 2 of the top 3 retrieved documents are relevant.

\subsubsection*{4.4.1 F1@3}

At rank 3:
\[
P@3 = \frac{2}{3}, \qquad \text{Recall@3} = \frac{2}{3}.
\]

The F1 score is:
\[
F_1 = \frac{2PR}{P+R}
= \frac{2 \cdot \frac{2}{3} \cdot \frac{2}{3}}{\frac{2}{3} + \frac{2}{3}}
= \frac{2}{3}.
\]

\subsubsection*{4.4.2 AP bounds at rank 3}

Since 2 of the top 3 positions are relevant, the possible patterns for the first three ranks are:

\[
\text{RRN},\quad \text{RNR},\quad \text{NRR}.
\]

AP at rank 3 is:
\[
\mathrm{AP} = \frac{1}{3} \sum_{k:\mathrm{rel}(k)=1} P@k.
\]

\begin{itemize}
\item \textbf{RRN} (ranks 1 and 2 relevant):  
      \[
      P@1 = 1,\quad P@2 = 1.
      \]
      \[
      \mathrm{AP} = \frac{1 + 1}{3} = \frac{2}{3}.
      \]

\item \textbf{RNR} (ranks 1 and 3 relevant):  
      \[
      P@1 = 1,\quad P@3 = \frac{2}{3}.
      \]
      \[
      \mathrm{AP} = \frac{1 + \frac{2}{3}}{3} = \frac{5}{9}.
      \]

\item \textbf{NRR} (ranks 2 and 3 relevant):  
      \[
      P@2 = \frac{1}{2},\quad P@3 = \frac{2}{3}.
      \]
      \[
      \mathrm{AP} = \frac{\frac{1}{2} + \frac{2}{3}}{3} = \frac{7}{18}.
      \]
\end{itemize}

Therefore:
\[
\mathrm{AP}_{\max} = \frac{2}{3}, \qquad
\mathrm{AP}_{\min} = \frac{7}{18}.
\]

\section*{5. True/False}

\subsection*{5.1 \(P@k\) is a monotonic non-decreasing function of \(k\).}

\textbf{False.}  
Precision@\(k\) is
\[
P@k = \frac{\text{\# relevant in top }k}{k}.
\]
If the next retrieved document is non-relevant, the numerator stays the same while the denominator increases, so precision can drop (e.g., \(P@1=1\), \(P@2=\frac{1}{2}\)).

\subsection*{5.2 Vector space retrieval is always more effective than Boolean retrieval.}

\textbf{False.}  
Effectiveness depends on the task and user needs. Boolean retrieval can be better when exact filtering is required (e.g., legal or patent search), while vector space retrieval is better for ranked, similarity-based searching. Neither is always superior.

\subsection*{5.3 In the vector space model, a higher normalization factor reduces a document's retrieval chances.}

\textbf{True.}  
Cosine similarity divides by the document norm:
\[
\cos(\vec{q},\vec{d}) = \frac{\vec{q}\cdot\vec{d}}{\|\vec{q}\|\,\|\vec{d}\|}.
\]
For a fixed dot product, a larger \(\|\vec{d}\|\) lowers the similarity score. Long documents with large norms are therefore penalized and less likely to appear high in the ranking.

\subsection*{5.4 Stemming increases the number of unique terms in the index.}

\textbf{False.}  
Stemming maps multiple surface forms to a single stem (e.g., \texttt{running}, \texttt{ran}, \texttt{runs} \(\to\) \texttt{run}). This reduces the vocabulary size and decreases the number of unique index terms.

\subsection*{5.5 Values of \(\beta>1\) in the \(F_\beta\) measure emphasize precision.}

\textbf{False.}  
The \(F_\beta\) formula is:
\[
F_\beta = \frac{(1+\beta^2)PR}{\beta^2 P + R}.
\]
When \(\beta>1\), the term \(\beta^2\) increases the weight of recall in the denominator. Thus \(\beta>1\) emphasizes recall, while \(\beta<1\) emphasizes precision.

\subsection*{5.6 In Rocchio's model, \(\vec{q}_0\) might be closer to the relevant centroid than \(\vec{q}_m\).}

\textbf{True.}  
Rocchio updates the query by pulling it toward the relevant centroid and pushing it away from the non-relevant centroid. With poorly chosen parameters \((\alpha,\beta,\gamma)\) or noisy relevance judgments, the negative push from non-relevant documents can move \(\vec{q}_m\) farther away from the true relevant centroid than the original \(\vec{q}_0\).  

\end{document}
