\documentclass[12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\geometry{margin=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.8em}

% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10}
}

\title{Assignment 2\\Text Retrieval and Search Engines}
\author{Gal Noy \\ 209346485}
\date{}

\begin{document}
\maketitle

\section*{Dry Part}

\subsection*{1. Vector Space Model}

\[
\begin{array}{c|cccc}
      & d_1 & d_2 & d_3 & d_4 \\ \hline
a     & 0   & 1   & 1   & 1   \\
b     & 1   & 2   & 0   & 1   \\
c     & 2   & 0   & 0   & 0   \\
d     & 0   & 0   & 0   & 0   \\
e     & 1   & 0   & 1   & 1   \\
f     & 7   & 5   & 7   & 2   \\
\end{array}
\]

\[
p(a)=0.1,\quad p(b)=0.1,\quad p(c)=0.2,\quad p(d)=0.05,\quad p(e)=0.5,\quad p(f)=0.9.
\]

Let
\[
\mathrm{idf}_t = \ln\!\left(\frac{1}{p(t)}\right),
\]
so
\[
\begin{aligned}
\mathrm{idf}_a &= \mathrm{idf}_b = \ln 10 \approx 2.303,\\
\mathrm{idf}_c &= \ln 5 \approx 1.609,\\
\mathrm{idf}_d &= \ln 20 \approx 2.996,\\
\mathrm{idf}_e &= \ln 2 \approx 0.693,\\
\mathrm{idf}_f &= \ln(1/0.9) \approx 0.105.
\end{aligned}
\]

\subsubsection*{1.1 Cosine similarity between \(d_1\) and \(d_2\)}

tf–idf weighting:
\[
w_{t,d} = \mathrm{tf}_{t,d}\,\mathrm{idf}_t.
\]

\[
\begin{aligned}
d_1 &: (0,1,2,0,1,7),\\
d_2 &: (1,2,0,0,0,5).
\end{aligned}
\]

\[
\begin{aligned}
\vec{d}_1 &=
(0,\;\ln 10,\;2\ln 5,\;0,\;\ln 2,\;7\ln(1/0.9)) \\
&\approx (0,\;2.303,\;3.219,\;0,\;0.693,\;0.738),\\
\vec{d}_2 &=
(\ln 10,\;2\ln 10,\;0,\;0,\;0,\;5\ln(1/0.9)) \\
&\approx (2.303,\;4.606,\;0,\;0,\;0,\;0.527).
\end{aligned}
\]

Dot product:
\[
\begin{aligned}
\vec{d}_1\cdot\vec{d}_2
&= (\ln 10)(2\ln 10) + (7\ln(1/0.9))(5\ln(1/0.9))\\
&= 2(\ln 10)^2 + 35(\ln(1/0.9))^2 \approx 10.992.
\end{aligned}
\]

Norms:
\[
\begin{aligned}
\|\vec{d}_1\|^2
&= (\ln 10)^2 + (2\ln 5)^2 + (\ln 2)^2 + (7\ln(1/0.9))^2 \\
&\approx 16.688,\quad \|\vec{d}_1\|\approx 4.085,\\
\|\vec{d}_2\|^2
&= (\ln 10)^2 + (2\ln 10)^2 + (5\ln(1/0.9))^2 \\
&\approx 26.787,\quad \|\vec{d}_2\|\approx 5.176.
\end{aligned}
\]

Cosine:
\[
\cos(\vec{d}_1,\vec{d}_2)
= \frac{10.992}{4.085\cdot 5.176}
\approx 0.52.
\]

\subsubsection*{1.2 Ranking \(d_1,\dots,d_4\) for query \(q = \text{``a b f''}\)}

Documents: raw tf. Query: tf–idf.

\[
\vec{q} = (\mathrm{idf}_a,\mathrm{idf}_b,0,0,0,\mathrm{idf}_f)
\approx (2.303,2.303,0,0,0,0.105),
\quad \|\vec{q}\|\approx 3.258.
\]

\[
\begin{aligned}
\vec{d}_1 &= (0,1,2,0,1,7), & \|\vec{d}_1\| &= \sqrt{55}\approx 7.416,\\
\vec{d}_2 &= (1,2,0,0,0,5), & \|\vec{d}_2\| &= \sqrt{30}\approx 5.477,\\
\vec{d}_3 &= (1,0,0,0,1,7), & \|\vec{d}_3\| &= \sqrt{51}\approx 7.141,\\
\vec{d}_4 &= (1,1,0,0,1,2), & \|\vec{d}_4\| &= \sqrt{7}\approx 2.646.
\end{aligned}
\]

Dot products (only \(a,b,f\) matter):
\[
\begin{aligned}
\vec{d}_1\cdot\vec{q} &\approx 2.303 + 7\cdot 0.105 \approx 3.04,\\
\vec{d}_2\cdot\vec{q} &\approx 3\cdot 2.303 + 5\cdot 0.105 \approx 7.44,\\
\vec{d}_3\cdot\vec{q} &\approx 2.303 + 7\cdot 0.105 \approx 3.04,\\
\vec{d}_4\cdot\vec{q} &\approx 2\cdot 2.303 + 2\cdot 0.105 \approx 4.82.
\end{aligned}
\]

Cosine similarities:
\[
\begin{aligned}
\cos(d_1,q) &\approx \frac{3.04}{7.416\cdot 3.258} \approx 0.13,\\
\cos(d_2,q) &\approx \frac{7.44}{5.477\cdot 3.258} \approx 0.42,\\
\cos(d_3,q) &\approx \frac{3.04}{7.141\cdot 3.258} \approx 0.13,\\
\cos(d_4,q) &\approx \frac{4.82}{2.646\cdot 3.258} \approx 0.56.
\end{aligned}
\]

Ranking:
\[
d_4 \succ d_2 \succ d_3 \succ d_1.
\]

\subsection*{2. Term Weighting}

\subsubsection*{2.1 Short-document bias}

Cosine similarity normalizes by \(\|\vec{d}\|\). For two documents with similar raw tf on query terms, the shorter document has smaller \(\|\vec{d}\|\), so its normalized weights are larger and its cosine score tends to be higher. This favors short documents even when longer ones may be more informative.

\subsubsection*{2.2 Weighted tf function}

Given
\[
W_{t,d} = \alpha + (1-\alpha)\frac{\mathrm{tf}_{t,d}}{\mathrm{tf}_{\max}(d)},
\quad 0\le \alpha\le 1.
\]

\textbf{Usefulness:}  
Normalizes tf into \([\alpha,1]\) per document via \(\mathrm{tf}_{t,d}/\mathrm{tf}_{\max}(d)\), reducing dominance of extremely frequent terms and partly compensating for document length.

\textbf{Issue:}  
If applied to all vocabulary terms, terms with \(\mathrm{tf}_{t,d}=0\) also get weight \(\alpha>0\), meaning documents get non-zero weight for absent terms, which blurs the distinction between presence and absence and can hurt precision.

\subsection*{3. Relevance Feedback and Evaluation}

\subsubsection*{3.1 AP, P@5 and Recall@5}

Top-5 list:

\begin{center}
\begin{tabular}{c|c}
\textbf{DocID} & \textbf{Rel.} \\ \hline
5 & 4 \\
2 & 1 \\
1 & 1 \\
3 & 3 \\
4 & 0 \\
\end{tabular}
\end{center}

Relevant grades \(>0\); total relevant \(R=10\).  
Relevant at ranks \(1,2,3,4\).

\[
P@5 = \frac{4}{5} = 0.8,\qquad
\text{Recall@5} = \frac{4}{10} = 0.4.
\]

Precisions at relevant ranks:
\[
P@1=1,\;P@2=1,\;P@3=1,\;P@4=1.
\]

Average Precision:
\[
\mathrm{AP} = \frac{1}{R}\sum_{k} P@k \cdot \mathrm{rel}(k)
= \frac{1}{10}(1+1+1+1) = 0.4.
\]

\subsubsection*{3.2 Rocchio with graded relevance}

In standard Rocchio, all relevant documents contribute equally:

\[
\vec{q}_m
= \alpha\,\vec{q}_0
+ \frac{\beta}{|D_R|}\sum_{d\in D_R}\vec{d}
- \frac{\gamma}{|D_{NR}|}\sum_{d\in D_{NR}}\vec{d}.
\]

With graded relevance, each document has a grade  
\(\mathrm{grade}(d) \in \{0,\dots,r_{\max}\}\).  
To incorporate this information, we normalize the grade:

\[
\mathrm{rel}(d)=\frac{\mathrm{grade}(d)}{r_{\max}},
\]

so the most relevant document receives weight \(1\), and others receive proportionally smaller weights.  
This allows Rocchio to give stronger influence to documents judged as more relevant.

The modified formula becomes:

\[
\vec{q}_m
= \alpha\,\vec{q}_0
+ \beta\,\frac{1}{Z_R}\sum_{d\in D_R}\mathrm{rel}(d)\,\vec{d}
- \gamma\,\frac{1}{|D_{NR}|}\sum_{d\in D_{NR}}\vec{d},
\qquad
Z_R = \sum_{d\in D_R} \mathrm{rel}(d).
\]

The normalization factor \(Z_R\) ensures that the weighted contribution of relevant documents remains on a consistent scale across queries (e.g., regardless of how many relevant documents exist or how large their grades are).

\newpage
\subsubsection*{3.3 Rocchio with rank-based relevance}

Another approach is to weight relevant documents based on their rank position.  
A higher-ranked document should influence the query more than one that appears later.  
A simple decreasing weighting function is:

\[
g(\mathrm{rank}(d)) = \frac{1}{\mathrm{rank}(d)}.
\]

Thus, the first relevant document receives weight \(1\), the second receives \(1/2\), the third \(1/3\), and so on.  
This reflects the intuition that early-ranked results are generally better signals of what the user prefers.

The Rocchio update becomes:

\[
\vec{q}_m
= \alpha\,\vec{q}_0
+ \beta\,\frac{1}{Z}
\sum_{d\in D_R} g(\mathrm{rank}(d))\,\vec{d}
- \gamma\,\frac{1}{|D_{NR}|}\sum_{d\in D_{NR}}\vec{d},
\qquad
Z = \sum_{d\in D_R} g(\mathrm{rank}(d)).
\]

The normalization factor \(Z\) keeps the weighted sum of ranked contributions comparable across queries, preventing queries with many relevant documents from overwhelming the update.

\subsection*{4. Evaluation}

\subsubsection*{4.1 ERR vs NDCG}

ERR is defined as:
\[
\mathrm{ERR} = \sum_{i=1}^{k}
\frac{1}{i}
\left(\prod_{j=1}^{i-1}(1-R_j)\right) R_i,
\qquad
R_i = \frac{\mathrm{rel}_i}{\mathrm{rel}_{\max}}.
\]

Here \(R_i\) is a normalized satisfaction probability:  
a document with the highest grade receives \(R_i = 1\), and lower grades receive proportionally smaller values.  
ERR models a user who scans results from the top and stops at rank \(i\) with probability \(R_i\), so highly relevant documents at early ranks strongly affect the score.

We compare two ranked lists (with \(\mathrm{rel}_{\max}=3\)):

\[
A = [3,0,0,0],
\qquad
B = [2,2,2,2].
\]

\textbf{NDCG comparison.}  
Using the simple form of DCG:
\[
\mathrm{DCG} = \sum_{i=1}^{4} \frac{\mathrm{rel}_i}{\log_2(i+1)}.
\]

\[
\mathrm{DCG}_A = \frac{3}{\log_2 2} = 3.
\]

\[
\mathrm{DCG}_B
= \frac{2}{\log_2 2}
+ \frac{2}{\log_2 3}
+ \frac{2}{\log_2 4}
+ \frac{2}{\log_2 5}
\approx 5.123.
\]

The ideal ranking places the grade 3 document first:
\[
\mathrm{DCG}_{\text{ideal}}
= \frac{3}{\log_2 2}
+ \frac{2}{\log_2 3}
+ \frac{2}{\log_2 4}
+ \frac{2}{\log_2 5}
\approx 6.123.
\]

\[
\mathrm{NDCG}_A = \frac{3}{6.123} \approx 0.49,
\qquad
\mathrm{NDCG}_B = \frac{5.123}{6.123} \approx 0.84.
\]

NDCG prefers B, because it rewards having several moderately relevant documents across the top ranks.

\textbf{ERR comparison.}  
Normalized satisfaction values:
\[
R(3)=\frac{3}{3}=1,\qquad
R(2)=\frac{2}{3},\qquad
R(0)=0.
\]

For A:
\[
\mathrm{ERR}_A = 1.
\]

For B (all \(R_i = 2/3\)):
\[
\mathrm{ERR}_B
= 1\cdot \frac{2}{3}
+ \frac{1}{2}(1-\tfrac{2}{3})\tfrac{2}{3}
+ \frac{1}{3}(1-\tfrac{2}{3})^2 \tfrac{2}{3}
+ \frac{1}{4}(1-\tfrac{2}{3})^3 \tfrac{2}{3}
\approx 0.81.
\]

Thus:
\[
\mathrm{ERR}_A = 1 \;>\; \mathrm{ERR}_B \approx 0.81.
\]

ERR prefers A, because the user is assumed to stop once a highly relevant document appears at the top.  
This fits tasks where a single excellent result early in the list is more valuable than several moderately relevant ones spread throughout the ranking.

\subsubsection*{4.2 When does MAP = MRR?}

MAP equals MRR when, for every query, the Average Precision reduces to the precision at the first relevant document. This happens in the following cases:

\begin{itemize}
\item Each query has exactly one relevant document in the entire collection.
\item The system retrieves at most one relevant document for each query (even if more exist).
\item All documents are relevant (or all are non-relevant). If all are relevant, AP = RR = 1 for every query. If none are relevant, both AP and RR are 0.
\item No query has more than one retrieved relevant document at different ranks (so the contribution to AP comes only from the first relevant match).
\end{itemize}

\subsubsection*{4.3 Stopword removal}

\paragraph*{(a) Lower recall}

\begin{itemize}
\item ``to be or not to be'': removing stopwords erases most of the query terms, making it impossible to retrieve documents that contain the full phrase, which lowers recall.

\item ``The Who discography'': removing \texttt{the} removes part of the band’s actual name ``The Who'', causing the system to miss relevant documents that contain the exact entity.
\end{itemize}

\paragraph*{(b) Lower precision}

\begin{itemize}
\item ``Bank of America'' \(\to\) ``bank america'': removing \texttt{of} breaks the specific organization name, leading the system to retrieve general documents about banks in America rather than the actual company, lowering precision.

\item ``vitamin A deficiency'' \(\to\) ``vitamin deficiency'': the reduced query matches deficiencies of any vitamin (B, C, D, etc.), producing many broader, non-relevant documents and reducing precision.
\end{itemize}


\subsubsection*{4.4 R-precision, F1 and AP bounds}

There are \(R = 3\) relevant documents in the collection.  
Given R-precision \(= \frac{2}{3}\), exactly 2 of the top 3 retrieved documents are relevant.

\paragraph*{4.4.1 F1@3}

At rank 3:
\[
P@3 = \frac{2}{3}, \qquad \text{Recall@3} = \frac{2}{3}.
\]

The F1 score is:
\[
F_1 = \frac{2PR}{P+R}
= \frac{2 \cdot \frac{2}{3} \cdot \frac{2}{3}}{\frac{2}{3} + \frac{2}{3}}
= \frac{2}{3}.
\]

\paragraph*{4.4.2 AP bounds at rank 3}

Since 2 of the top 3 positions are relevant, the possible patterns for the first three ranks are:

\[
\text{RRN},\quad \text{RNR},\quad \text{NRR}.
\]

AP at rank 3 is:
\[
\mathrm{AP} = \frac{1}{3} \sum_{k:\mathrm{rel}(k)=1} P@k.
\]

\begin{itemize}
\item \textbf{RRN} (ranks 1 and 2 relevant):  
      \[
      P@1 = 1,\quad P@2 = 1.
      \]
      \[
      \mathrm{AP} = \frac{1 + 1}{3} = \frac{2}{3}.
      \]

\item \textbf{RNR} (ranks 1 and 3 relevant):  
      \[
      P@1 = 1,\quad P@3 = \frac{2}{3}.
      \]
      \[
      \mathrm{AP} = \frac{1 + \frac{2}{3}}{3} = \frac{5}{9}.
      \]

\item \textbf{NRR} (ranks 2 and 3 relevant):  
      \[
      P@2 = \frac{1}{2},\quad P@3 = \frac{2}{3}.
      \]
      \[
      \mathrm{AP} = \frac{\frac{1}{2} + \frac{2}{3}}{3} = \frac{7}{18}.
      \]
\end{itemize}

Therefore:
\[
\mathrm{AP}_{\max} = \frac{2}{3}, \qquad
\mathrm{AP}_{\min} = \frac{7}{18}.
\]

\subsection*{5. True/False}

\subsubsection*{5.1 \(P@k\) is a monotonic non-decreasing function of \(k\).}

\textbf{False.}  
Precision@\(k\) is
\[
P@k = \frac{\text{\# relevant in top }k}{k}.
\]
If the next retrieved document is non-relevant, the numerator stays the same while the denominator increases, so precision can drop (e.g., \(P@1=1\), \(P@2=\frac{1}{2}\)).

\subsubsection*{5.2 Vector space retrieval is always more effective than Boolean retrieval.}

\textbf{False.}  
Effectiveness depends on the task and user needs. Boolean retrieval can be better when exact filtering is required (e.g., legal or patent search), while vector space retrieval is better for ranked, similarity-based searching. Neither is always superior.

\subsubsection*{5.3 In the vector space model, a higher normalization factor reduces a document's retrieval chances.}

\textbf{True.}  
Cosine similarity divides by the document norm:
\[
\cos(\vec{q},\vec{d}) = \frac{\vec{q}\cdot\vec{d}}{\|\vec{q}\|\,\|\vec{d}\|}.
\]
For a fixed dot product, a larger \(\|\vec{d}\|\) lowers the similarity score. Long documents with large norms are therefore penalized and less likely to appear high in the ranking.

\subsubsection*{5.4 Stemming increases the number of unique terms in the index.}

\textbf{False.}  
Stemming maps multiple surface forms to a single stem (e.g., \texttt{running}, \texttt{ran}, \texttt{runs} \(\to\) \texttt{run}). This reduces the vocabulary size and decreases the number of unique index terms.

\subsubsection*{5.5 Values of \(\beta>1\) in the \(F_\beta\) measure emphasize precision.}

\textbf{False.}  
The \(F_\beta\) formula is:
\[
F_\beta = \frac{(1+\beta^2)PR}{\beta^2 P + R}.
\]
When \(\beta>1\), the term \(\beta^2\) increases the weight of recall in the denominator. Thus \(\beta>1\) emphasizes recall, while \(\beta<1\) emphasizes precision.

\subsubsection*{5.6 In Rocchio's model, \(\vec{q}_0\) might be closer to the relevant centroid than \(\vec{q}_m\).}

\textbf{True.}  
Rocchio updates the query by pulling it toward the relevant centroid and pushing it away from the non-relevant centroid. With poorly chosen parameters \((\alpha,\beta,\gamma)\) or noisy relevance judgments, the negative push from non-relevant documents can move \(\vec{q}_m\) farther away from the true relevant centroid than the original \(\vec{q}_0\).  

\newpage

\section*{Wet Part}

\subsection*{Part A: Index docs.txt and Test Queries}

The index for Part A was built using Pyserini with the following configuration:
\begin{itemize}
\item Collection: TrecCollection
\item Stemmer: none
\item Stopword removal: disabled
\item BM25 parameters: \(k_1 = 0.9\), \(b = 0.4\)
\end{itemize}

\subsubsection*{Question 1: Query ``corporation''}

\paragraph*{1a. How many documents did you retrieve?}

\textbf{Retrieved:} 1 document.

\begin{center}
\begin{tabular}{ccc}
\toprule
\textbf{Rank} & \textbf{Document ID} & \textbf{Score} \\
\midrule
1 & D2 & 0.6747 \\
\bottomrule
\end{tabular}
\end{center}

\paragraph*{1b. How many documents did you expect to retrieve?}

\textbf{Expected:} 2 documents.

The query should match both D2 and D3:

\begin{itemize}
\item \textbf{D2:} contains the term \textbf{corporation}.
\item \textbf{D3:} contains the plural form \textbf{corporations}.
\end{itemize}

\textbf{Reason for missing D3.}  
Because the index was built without stemming, the terms ``corporation'' and ``corporations'' are treated as separate tokens. BM25 therefore retrieves only the document containing the exact singular form. With stemming (e.g., Krovetz or Porter), both forms would map to the same stem and D3 would be retrieved as well.

\subsubsection*{Question 2: Query to return D1 first (maximum 2 words)}

\textbf{Query:} ``Nobel Prize''

\begin{center}
\begin{tabular}{ccc}
\toprule
\textbf{Rank} & \textbf{Document ID} & \textbf{Score} \\
\midrule
1 & D1 & 1.9240 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Explanation.}  
The phrase ``Nobel Prize'' appears multiple times in D1 and does not appear in the other documents. BM25 ranks D1 at the top because:
\begin{itemize}
\item The term frequency in D1 is high.
\item The phrase is rare in the small collection, giving it a strong IDF value.
\end{itemize}

\subsubsection*{Question 3: Is D4 relevant to ``Michael Jackson''?}

\paragraph*{3a. Run query and analyze relevance}

\textbf{Query:} ``Michael Jackson''

\begin{center}
\begin{tabular}{ccc}
\toprule
\textbf{Rank} & \textbf{Document ID} & \textbf{Score} \\
\midrule
1 & D4 & 1.1867 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Excerpt from D4:}  
``...she remembers singing into a plastic tape recorder to the likes of Cyndi Lauper and \textbf{Michael Jackson}.''

\textbf{Answer: No.}  
D4 is not relevant to a query about Michael Jackson. The document is about Lady Gaga’s biography, and Michael Jackson is mentioned once in passing. This does not make the document topically relevant to the query.

\paragraph*{3b. Query for which D4 \emph{is} relevant (maximum 2 words)}

\textbf{Query:} ``Lady Gaga''

\begin{center}
\begin{tabular}{ccc}
\toprule
\textbf{Rank} & \textbf{Document ID} & \textbf{Score} \\
\midrule
1 & D4 & 1.5899 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Explanation.}  
D4 focuses on Lady Gaga and mentions her several times. Because the document is mainly about her, BM25 assigns a higher score than in the previous query, and the document is correctly ranked first.

\subsection*{Part B: Build 2 Indexes and Compare Retrieval Effectiveness}

Two indexes were built over the AP Collection and evaluated using 150 queries:

\begin{itemize}
\item \textbf{Index 1:} Krovetz stemming + stopword removal
\item \textbf{Index 2:} No stemming + stopword removal
\end{itemize}

Both indexes used:
\begin{itemize}
\item BM25 with \(k_1 = 0.9\), \(b = 0.4\)
\item Top 1000 documents per query
\item TREC format results
\item Evaluation via \texttt{pyserini.eval.trec\_eval}
\end{itemize}

\subsubsection*{Evaluation Results}

\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{MAP} & \textbf{P@5} & \textbf{P@10} \\
\midrule
Krovetz + Stopwords & 0.2144 & 0.4121 & 0.3913 \\
No Stemming + Stopwords & 0.1896 & 0.4094 & 0.3758 \\
\midrule
\textbf{Improvement (Krovetz)} & \textbf{+0.0248} & \textbf{+0.0027} & \textbf{+0.0155} \\
\bottomrule
\end{tabular}
\end{center}

\subsubsection*{Analysis and Conclusion}

\textbf{Best configuration by MAP:} Krovetz stemming.

\textbf{MAP improvement:} 0.0248 (about 13\% relative gain).

\textbf{Explanation.}  
Krovetz stemming reduces different word forms to a common root, which helps match documents and queries that use different morphological variants. Examples:

\begin{itemize}
\item ``running'', ``runs'', ``ran'' → ``run''
\item ``corporation'', ``corporations'' → ``corporation''
\item ``computing'', ``computer'', ``computers'' → ``compute''
\end{itemize}

This improves recall and makes the ranking more consistent across the full list of retrieved documents. The improvements in P@5 and P@10 are smaller, meaning that stemming mainly helps with recall across the whole ranking rather than only at the top ranks.

Overall, the results show that using Krovetz stemming produces better retrieval effectiveness for all evaluated metrics.

\end{document}
