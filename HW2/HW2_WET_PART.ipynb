{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89db0b12",
   "metadata": {},
   "source": [
    "# HW2 - WET PART\n",
    "\n",
    "## Part A: Index docs.txt and test queries\n",
    "Build index with `--keepStopwords --stemmer none` and test with specific queries.\n",
    "\n",
    "## Part B: Build 2 indexes and compare retrieval effectiveness\n",
    "1. **Index 1**: WITH stopwords removal AND WITH Krovetz stemming\n",
    "2. **Index 2**: WITH stopwords removal AND WITHOUT stemming\n",
    "3. Compare MAP, P@5, P@10 for both indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2830bb",
   "metadata": {},
   "source": [
    "### Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fbeb034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/data/AP_Coll\n",
      "Index: /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/indexes/ap_index\n",
      "Results: /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import pytrec_eval\n",
    "from pathlib import Path\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.analysis import Analyzer, get_lucene_analyzer\n",
    "from pyserini.index.lucene import IndexReader\n",
    "from pyserini.search import get_topics_with_reader\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "# Part A paths\n",
    "PART_A_DIR = BASE_DIR / \"data\" / \"WET_PART_A\"\n",
    "PART_A_INDEX = BASE_DIR / \"indexes\" / \"part_a_index\"\n",
    "\n",
    "# Part B paths\n",
    "AP_COLL_PATH = BASE_DIR / \"data\" / \"AP_Coll\"\n",
    "PART_B_DIR = BASE_DIR / \"data\" / \"WET_PART_B\"\n",
    "QUERIES_PATH = PART_B_DIR / \"queries.txt\"\n",
    "STOPWORDS_PATH = PART_B_DIR / \"StopWords.txt\"\n",
    "QRELS_PATH = PART_B_DIR / \"qrels_AP\"\n",
    "\n",
    "# Part B indexes\n",
    "INDEX_STEMMED = BASE_DIR / \"indexes\" / \"ap_index_stemmed\"\n",
    "INDEX_UNSTEMMED = BASE_DIR / \"indexes\" / \"ap_index_unstemmed\"\n",
    "\n",
    "# Results\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "RESULTS_STEMMED = RESULTS_DIR / \"results_stemmed.txt\"\n",
    "RESULTS_UNSTEMMED = RESULTS_DIR / \"results_unstemmed.txt\"\n",
    "EVAL_PATH = RESULTS_DIR / \"evaluation_comparison.txt\"\n",
    "\n",
    "# Create directories\n",
    "PART_A_INDEX.mkdir(parents=True, exist_ok=True)\n",
    "INDEX_STEMMED.mkdir(parents=True, exist_ok=True)\n",
    "INDEX_UNSTEMMED.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04b65a",
   "metadata": {},
   "source": [
    "### Build Part A Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b547dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index...\n",
      "Index built: /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/indexes/ap_index\n",
      "Index built: /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/indexes/ap_index\n"
     ]
    }
   ],
   "source": [
    "# Build index with --keepStopwords --stemmer none\n",
    "if PART_A_INDEX.exists() and any(PART_A_INDEX.iterdir()):\n",
    "    print(f\"Part A index exists: {PART_A_INDEX}\")\n",
    "else:\n",
    "    print(\"Building Part A index...\")\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"pyserini.index.lucene\",\n",
    "        \"--collection\", \"TrecCollection\",\n",
    "        \"--input\", str(PART_A_DIR),\n",
    "        \"--index\", str(PART_A_INDEX),\n",
    "        \"--keepStopwords\",\n",
    "        \"--stemmer\", \"none\",\n",
    "        \"--storePositions\",\n",
    "        \"--storeDocvectors\",\n",
    "        \"--storeRaw\",\n",
    "        \"--optimize\"\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Part A index built: {PART_A_INDEX}\")\n",
    "    else:\n",
    "        print(\"Failed!\")\n",
    "        print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299dac8",
   "metadata": {},
   "source": [
    "### Check Part A Index Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7bdaab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Statistics:\n",
      "total_terms              : 110,570,409\n",
      "documents                : 242,892\n",
      "non_empty_documents      : 242,892\n",
      "unique_terms             : 397,835\n"
     ]
    }
   ],
   "source": [
    "part_a_reader = IndexReader(str(PART_A_INDEX))\n",
    "part_a_stats = part_a_reader.stats()\n",
    "\n",
    "print(\"Part A Index Statistics:\")\n",
    "for key, value in part_a_stats.items():\n",
    "    print(f\"{key:25s}: {value:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23388a",
   "metadata": {},
   "source": [
    "### Test Part A: Query 'corporation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45384199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize searcher with matching analyzer\n",
    "searcher_a = LuceneSearcher(str(PART_A_INDEX))\n",
    "analyzer_a = get_lucene_analyzer(stemmer='none', stopwords=False)\n",
    "searcher_a.set_analyzer(analyzer_a)\n",
    "searcher_a.set_bm25(k1=0.9, b=0.4)\n",
    "\n",
    "# Query: corporation\n",
    "query = 'corporation'\n",
    "hits = searcher_a.search(query, k=4)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Retrieved {len(hits)} documents:\")\n",
    "for i in range(len(hits)):\n",
    "    print(f\"Doc {hits[i].docid}, Score: {hits[i].score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5f628",
   "metadata": {},
   "source": [
    "## Part B: Build 2 Indexes and Compare\n",
    "\n",
    "1. **Index 1**: WITH stopwords removal AND WITH Krovetz stemming\n",
    "2. **Index 2**: WITH stopwords removal AND WITHOUT stemming\n",
    "\n",
    "Then retrieve and evaluate both to compare MAP, P@5, P@10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74b539",
   "metadata": {},
   "source": [
    "### Build Index 1: WITH Stopwords Removal + WITH Krovetz Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fe4938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 418 stopwords\n",
      "Sample stopwords: ['whole', 'to', 'something', 'per', 'again', 'hardly', 'hereafter', 'already', 'within', 'whereinto']\n"
     ]
    }
   ],
   "source": [
    "if INDEX_STEMMED.exists() and any(INDEX_STEMMED.iterdir()):\n",
    "    print(f\"Stemmed index exists: {INDEX_STEMMED}\")\n",
    "else:\n",
    "    print(\"Building Index 1 (WITH stopwords removal + WITH Krovetz stemming)...\")\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"pyserini.index.lucene\",\n",
    "        \"--collection\", \"TrecCollection\",\n",
    "        \"--input\", str(AP_COLL_PATH),\n",
    "        \"--index\", str(INDEX_STEMMED),\n",
    "        \"--stemmer\", \"krovetz\",\n",
    "        \"--stopwords\", str(STOPWORDS_PATH),\n",
    "        \"--storePositions\",\n",
    "        \"--storeDocvectors\",\n",
    "        \"--storeRaw\",\n",
    "        \"--optimize\"\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Index 1 built: {INDEX_STEMMED}\")\n",
    "    else:\n",
    "        print(\"Failed!\")\n",
    "        print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d634350",
   "metadata": {},
   "source": [
    "### Build Index 2: WITH Stopwords Removal + WITHOUT Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b781323",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INDEX_UNSTEMMED.exists() and any(INDEX_UNSTEMMED.iterdir()):\n",
    "    print(f\"Unstemmed index exists: {INDEX_UNSTEMMED}\")\n",
    "else:\n",
    "    print(\"Building Index 2 (WITH stopwords removal + WITHOUT stemming)...\")\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"pyserini.index.lucene\",\n",
    "        \"--collection\", \"TrecCollection\",\n",
    "        \"--input\", str(AP_COLL_PATH),\n",
    "        \"--index\", str(INDEX_UNSTEMMED),\n",
    "        \"--stemmer\", \"none\",\n",
    "        \"--stopwords\", str(STOPWORDS_PATH),\n",
    "        \"--storePositions\",\n",
    "        \"--storeDocvectors\",\n",
    "        \"--storeRaw\",\n",
    "        \"--optimize\"\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Index 2 built: {INDEX_UNSTEMMED}\")\n",
    "    else:\n",
    "        print(\"Failed!\")\n",
    "        print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82271f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 150 queries\n",
      "\n",
      "Sample queries:\n",
      "  051: airbus  subsidies\n",
      "  052: south  african  sanctions\n",
      "  053: leveraged  buyouts\n",
      "  054: satellite  launch  contracts\n",
      "  055: insider  trading\n"
     ]
    }
   ],
   "source": [
    "# Load queries using pyserini\n",
    "topics = get_topics_with_reader('io.anserini.search.topicreader.TsvIntTopicReader', str(QUERIES_PATH))\n",
    "\n",
    "# Fix query IDs (add leading zero if needed)\n",
    "queries = {}\n",
    "for topic_id, topic in topics.items():\n",
    "    fixed_topic_id = str(topic_id)\n",
    "    if len(fixed_topic_id) == 2:\n",
    "        fixed_topic_id = '0' + str(topic_id)\n",
    "    queries[fixed_topic_id] = topic['title']\n",
    "\n",
    "print(f\"Loaded {len(queries)} queries\")\n",
    "assert len(queries) == 150, 'missing queries'\n",
    "print(f\"Sample: {list(queries.items())[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163aa9d3",
   "metadata": {},
   "source": [
    "### Retrieval from Index 1 (Stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c32529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 051 Processing Example:\n",
      "============================================================\n",
      "Original: airbus  subsidies\n",
      "Tokens after stemming: ['airbus', 'subsidy']\n",
      "After stopword removal: ['airbus', 'subsidy']\n",
      "Final query: airbus subsidy\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize searcher for stemmed index\n",
    "searcher_stemmed = LuceneSearcher(str(INDEX_STEMMED))\n",
    "analyzer_stemmed = get_lucene_analyzer(stemmer='krovetz', stopwords=False)\n",
    "searcher_stemmed.set_analyzer(analyzer_stemmed)\n",
    "searcher_stemmed.set_bm25(k1=0.9, b=0.4)\n",
    "\n",
    "# Retrieve for all queries\n",
    "results_stemmed = {}\n",
    "for topic_id, topic in queries.items():\n",
    "    hits = searcher_stemmed.search(topic, k=1000)\n",
    "    results_stemmed[topic_id] = [(hit.docid, i+1, hit.score) for i, hit in enumerate(hits)]\n",
    "    \n",
    "print(f\"Retrieved from Index 1 (stemmed): {len(results_stemmed)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a0752",
   "metadata": {},
   "source": [
    "### Save Results from Index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d1da98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BM25 searcher...\n",
      "✓ Searcher initialized with BM25\n",
      "Total documents in index: 242,892\n"
     ]
    }
   ],
   "source": [
    "sorted_results_stemmed = dict(sorted(results_stemmed.items()))\n",
    "with open(RESULTS_STEMMED, 'w') as f:\n",
    "    for topic_id, hits in sorted_results_stemmed.items():\n",
    "        for rank, (docid, _, score) in enumerate(hits, start=1):\n",
    "            f.write(f\"{topic_id} Q0 {docid} {rank} {score:.4f} pyserini\\n\")\n",
    "\n",
    "print(f\"Saved Index 1 results to: {RESULTS_STEMMED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d7126",
   "metadata": {},
   "source": [
    "### Retrieval from Index 2 (Unstemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize searcher for unstemmed index\n",
    "searcher_unstemmed = LuceneSearcher(str(INDEX_UNSTEMMED))\n",
    "analyzer_unstemmed = get_lucene_analyzer(stemmer='none', stopwords=False)\n",
    "searcher_unstemmed.set_analyzer(analyzer_unstemmed)\n",
    "searcher_unstemmed.set_bm25(k1=0.9, b=0.4)\n",
    "\n",
    "# Retrieve for all queries\n",
    "results_unstemmed = {}\n",
    "for topic_id, topic in queries.items():\n",
    "    hits = searcher_unstemmed.search(topic, k=1000)\n",
    "    results_unstemmed[topic_id] = [(hit.docid, i+1, hit.score) for i, hit in enumerate(hits)]\n",
    "    \n",
    "print(f\"Retrieved from Index 2 (unstemmed): {len(results_unstemmed)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48343297",
   "metadata": {},
   "source": [
    "### Save Results from Index 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f42532",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results_unstemmed = dict(sorted(results_unstemmed.items()))\n",
    "with open(RESULTS_UNSTEMMED, 'w') as f:\n",
    "    for topic_id, hits in sorted_results_unstemmed.items():\n",
    "        for rank, (docid, _, score) in enumerate(hits, start=1):\n",
    "            f.write(f\"{topic_id} Q0 {docid} {rank} {score:.4f} pyserini\\n\")\n",
    "\n",
    "print(f\"Saved Index 2 results to: {RESULTS_UNSTEMMED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f3cf3",
   "metadata": {},
   "source": [
    "### Evaluation - Compare Both Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d1134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded qrels for 149 queries\n",
      "✓ Total relevance judgments: 45,386\n"
     ]
    }
   ],
   "source": [
    "# Load qrels\n",
    "def load_qrels(filepath):\n",
    "    qrels = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 4:\n",
    "                qid, _, docid, rel = parts[0], parts[1], parts[2], int(parts[3])\n",
    "                if qid not in qrels:\n",
    "                    qrels[qid] = {}\n",
    "                qrels[qid][docid] = rel\n",
    "    return qrels\n",
    "\n",
    "qrels = load_qrels(QRELS_PATH)\n",
    "print(f\"Loaded qrels: {len(qrels)} queries, {sum(len(docs) for docs in qrels.values()):,} judgments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66b27c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded results for 147 queries\n"
     ]
    }
   ],
   "source": [
    "# Convert results to pytrec_eval format\n",
    "def convert_to_eval_format(results_dict):\n",
    "    eval_results = {}\n",
    "    for topic_id, hits in results_dict.items():\n",
    "        eval_results[topic_id] = {}\n",
    "        for docid, rank, score in hits:\n",
    "            eval_results[topic_id][docid] = float(score)\n",
    "    return eval_results\n",
    "\n",
    "eval_format_stemmed = convert_to_eval_format(results_stemmed)\n",
    "eval_format_unstemmed = convert_to_eval_format(results_unstemmed)\n",
    "\n",
    "print(\"Converted results to evaluation format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a9f18d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with pytrec_eval...\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "MAP (Mean Average Precision): 0.1162\n",
      "P@5  (Precision at 5):        0.2164\n",
      "P@10 (Precision at 10):       0.2068\n",
      "============================================================\n",
      "\n",
      "✓ Evaluation results saved to: /home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/HW2/results/evaluation_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Evaluate both indexes\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'P_5', 'P_10'})\n",
    "\n",
    "# Index 1 (stemmed)\n",
    "eval_stemmed = evaluator.evaluate(eval_format_stemmed)\n",
    "map_stemmed = sum(eval_stemmed[qid]['map'] for qid in eval_stemmed) / len(eval_stemmed)\n",
    "p5_stemmed = sum(eval_stemmed[qid]['P_5'] for qid in eval_stemmed) / len(eval_stemmed)\n",
    "p10_stemmed = sum(eval_stemmed[qid]['P_10'] for qid in eval_stemmed) / len(eval_stemmed)\n",
    "\n",
    "# Index 2 (unstemmed)\n",
    "eval_unstemmed = evaluator.evaluate(eval_format_unstemmed)\n",
    "map_unstemmed = sum(eval_unstemmed[qid]['map'] for qid in eval_unstemmed) / len(eval_unstemmed)\n",
    "p5_unstemmed = sum(eval_unstemmed[qid]['P_5'] for qid in eval_unstemmed) / len(eval_unstemmed)\n",
    "p10_unstemmed = sum(eval_unstemmed[qid]['P_10'] for qid in eval_unstemmed) / len(eval_unstemmed)\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" Stopword | Krovetz  |   MAP   |   P@5   |  P@10  \")\n",
    "print(\" Removal  | Stemmer  |         |         |        \")\n",
    "print(\"-\"*70)\n",
    "print(f\"   With   |   With   | {map_stemmed:.4f}  | {p5_stemmed:.4f}  | {p10_stemmed:.4f} \")\n",
    "print(f\"   With   | Without  | {map_unstemmed:.4f}  | {p5_unstemmed:.4f}  | {p10_unstemmed:.4f} \")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save to file\n",
    "with open(EVAL_PATH, 'w') as f:\n",
    "    f.write(\"Stopword Removal | Krovetz Stemmer | MAP     | P@5    | P@10\\n\")\n",
    "    f.write(\"-\"*65 + \"\\n\")\n",
    "    f.write(f\"With             | With            | {map_stemmed:.4f}  | {p5_stemmed:.4f} | {p10_stemmed:.4f}\\n\")\n",
    "    f.write(f\"With             | Without         | {map_unstemmed:.4f}  | {p5_unstemmed:.4f} | {p10_unstemmed:.4f}\\n\")\n",
    "\n",
    "print(f\"\\nSaved comparison to: {EVAL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710faf86",
   "metadata": {},
   "source": [
    "### Which Index Performed Better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a7e6017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Queries by MAP:\n",
      "====================================================================================================\n",
      "Query ID                                Query Text      MAP  P@5  P@10\n",
      "     052                 south  african  sanctions 0.846048  1.0   1.0\n",
      "     058                             rail  strikes 0.690898  1.0   1.0\n",
      "     170 consequences  implantation  silicone  gel 0.687500  0.4   0.2\n",
      "     132                         stealth  aircraft 0.676020  1.0   1.0\n",
      "     057                                       mci 0.643268  0.6   0.7\n",
      "     161                                acid  rain 0.531303  0.2   0.4\n",
      "     061       israeli  role  iran  contra  affair 0.529885  1.0   1.0\n",
      "     056  prime  lending  rate  moves  predictions 0.514333  1.0   1.0\n",
      "     163          vietnam  veterans  agent  orange 0.501623  0.2   0.4\n",
      "     099                      iran  contra  affair 0.474251  0.8   0.8\n",
      "\n",
      "\n",
      "Bottom 10 Queries by MAP:\n",
      "====================================================================================================\n",
      "Query ID                                    Query Text  MAP  P@5  P@10\n",
      "     063                          machine  translation  0.0  0.0   0.0\n",
      "     066                 natural  language  processing  0.0  0.0   0.0\n",
      "     079              frg  political  party  positions  0.0  0.0   0.0\n",
      "     082                          genetic  engineering  0.0  0.0   0.0\n",
      "     091 army  acquisition  advanced  weapons  systems  0.0  0.0   0.0\n",
      "     096           computer  aided  medical  diagnosis  0.0  0.0   0.0\n",
      "     105                                 black  monday  0.0  0.0   0.0\n",
      "     109                   find  innovative  companies  0.0  0.0   0.0\n",
      "     149                         industrial  espionage  0.0  0.0   0.0\n",
      "     162                           automobile  recalls  0.0  0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "if map_stemmed > map_unstemmed:\n",
    "    winner = \"Index 1 (WITH Krovetz stemming)\"\n",
    "    difference = map_stemmed - map_unstemmed\n",
    "else:\n",
    "    winner = \"Index 2 (WITHOUT stemming)\"\n",
    "    difference = map_unstemmed - map_stemmed\n",
    "\n",
    "print(f\"\\nBest MAP: {winner}\")\n",
    "print(f\"MAP difference: {difference:.4f}\")\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"Krovetz stemming conflates word variants to their root form, improving recall\")\n",
    "print(\"by matching different morphological forms of the same concept (e.g., 'running',\")\n",
    "print(\"'runs', 'ran' all map to 'run'). This helps when queries use different forms\")\n",
    "print(\"than the documents, improving the matching effectiveness.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
