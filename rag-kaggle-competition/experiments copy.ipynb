{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80249d88",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660a3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Literal\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79df6",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38133d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4cf0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"21.0.9\" 2025-10-21\n",
      "OpenJDK Runtime Environment (build 21.0.9+10-Ubuntu-122.04)\n",
      "OpenJDK 64-Bit Server VM (build 21.0.9+10-Ubuntu-122.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4783a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff9235",
   "metadata": {},
   "source": [
    "### Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f1afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged into Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(os.getenv('HUGGING_FACE_TOKEN'))\n",
    "print(\"✓ Logged into Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a17cc",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3486b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3778 questions\n",
      "Test set: 2032 questions\n",
      "\n",
      "Sample question: what is the name of justin bieber brother?\n",
      "Sample answers: ['Jazmyn Bieber', 'Jaxon Bieber']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(\"./data/train.csv\", converters={\"answers\": json.loads})\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(f\"Train set: {len(df_train)} questions\")\n",
    "print(f\"Test set: {len(df_test)} questions\")\n",
    "print(f\"\\nSample question: {df_train.iloc[0]['question']}\")\n",
    "print(f\"Sample answers: {df_train.iloc[0]['answers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b208668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Split data:\n",
      "   Training: 3022 questions\n",
      "   Validation: 756 questions\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation split for experiments\n",
    "RANDOM_SEED = 42\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "df_train_split = df_train.sample(frac=1-VAL_SIZE, random_state=RANDOM_SEED)\n",
    "df_val = df_train.drop(df_train_split.index).reset_index(drop=True)\n",
    "df_train_split = df_train_split.reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Split data:\")\n",
    "print(f\"   Training: {len(df_train_split)} questions\")\n",
    "print(f\"   Validation: {len(df_val)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14492e1",
   "metadata": {},
   "source": [
    "## 3. Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef07078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-12-13 15:24:53.949730162 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pyserini index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dec 13, 2025 3:24:54 PM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n",
      "✓ Index loaded: 5903530 documents\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "# Load Pyserini index\n",
    "print(\"Loading Pyserini index...\")\n",
    "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "\n",
    "print(f\"✓ Index loaded: {index_reader.stats()['documents']} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d922144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bi-encoder...\n",
      "✓ Bi-encoder loaded\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load bi-encoder\n",
    "print(\"Loading bi-encoder...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bi_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "print(\"✓ Bi-encoder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d0537dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLD(mu=1000) → NoRerank | k_docs=5, k_passages=3\n",
      "1. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "2. June 1997, the books have found immense popularity, critical acclaim and commercial success worldwid...\n",
      "3. English by two major publishers, Bloomsbury in the United Kingdom and Scholastic Press in the United...\n",
      "\n",
      "QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "1. the film, has denied that Rowling ever saw it before writing her book. Rowling has said on record mu...\n",
      "2. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "3. by Emily Brontë, \"Charlie and the Chocolate Factory\" by Roald Dahl, \"Robinson Crusoe\" by Daniel Defo...\n",
      "\n",
      "BM25(k1=0.9, b=0.4) → NoRerank | k_docs=5, k_passages=3\n",
      "1. Bonnie Wright Bonnie Francesca Wright (born 17 February 1991) is an English actress, film director, ...\n",
      "2. the Deathly Hallows – Part 1\" and \"Part 2\", she began attending London's University of the Arts: Lon...\n",
      "3. older brothers board the Hogwarts express. Her role became much more prominent in the second film, \"...\n",
      "\n",
      "BM25(k1=0.9, b=0.4) → BiEncoder | k_docs=5, k_passages=3\n",
      "1. to resolve an ongoing feud between the organisation's northern and southern branches that had sapped...\n",
      "2. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "3. children thought it was \"cool that their hero, Harry, was encountering some of the creatures of myth...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Literal\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievalConfig:\n",
    "    \"\"\"\n",
    "    Passage-based retrieval configuration with optional bi-encoder reranking.\n",
    "    \"\"\"\n",
    "    k_docs: int = 5\n",
    "    k_passages: int = 3\n",
    "    method: Literal[\"bm25\", \"qld\"] = \"qld\"\n",
    "    use_rerank: bool = True\n",
    "\n",
    "    mu: int = 1000          # QLD smoothing\n",
    "    k1: float = 0.9         # BM25\n",
    "    b: float = 0.4          # BM25\n",
    "\n",
    "    window: int = 150\n",
    "    overlap: int = 50\n",
    "    min_passage_words: int = 30\n",
    "\n",
    "    def __str__(self):\n",
    "        method_str = (\n",
    "            f\"QLD(mu={self.mu})\"\n",
    "            if self.method == \"qld\"\n",
    "            else f\"BM25(k1={self.k1}, b={self.b})\"\n",
    "        )\n",
    "        rerank_str = \"BiEncoder\" if self.use_rerank else \"NoRerank\"\n",
    "        return (\n",
    "            f\"{method_str} → {rerank_str} | \"\n",
    "            f\"k_docs={self.k_docs}, k_passages={self.k_passages}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def extract_passages(\n",
    "    text: str,\n",
    "    window: int,\n",
    "    overlap: int,\n",
    "    min_words: int,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping word windows.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    words = text.split()\n",
    "    if len(words) < min_words:\n",
    "        return []\n",
    "\n",
    "    step = max(1, window - overlap)\n",
    "    passages = []\n",
    "\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk = words[i:i + window]\n",
    "        if len(chunk) < min_words:\n",
    "            break\n",
    "        passages.append(\" \".join(chunk))\n",
    "\n",
    "    return passages\n",
    "\n",
    "\n",
    "def bi_encoder_rerank(\n",
    "    query: str,\n",
    "    passages: List[str],\n",
    "    top_k: int,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Rerank passages using bi-encoder cosine similarity.\n",
    "    \"\"\"\n",
    "    if not passages:\n",
    "        return []\n",
    "\n",
    "    q_emb = bi_encoder.encode(query, convert_to_tensor=True, device=device)\n",
    "    p_embs = bi_encoder.encode(passages, convert_to_tensor=True, device=device)\n",
    "\n",
    "    scores = util.cos_sim(q_emb, p_embs).squeeze(0)\n",
    "    top_k = min(top_k, len(passages))\n",
    "    idx = torch.topk(scores, k=top_k).indices.tolist()\n",
    "\n",
    "    return [passages[i] for i in idx]\n",
    "\n",
    "\n",
    "def retrieve_context(query: str, cfg: RetrievalConfig) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve passages using BM25 or QLD, optionally followed by bi-encoder reranking.\n",
    "    \"\"\"\n",
    "    if cfg.method == \"bm25\":\n",
    "        searcher.set_bm25(cfg.k1, cfg.b)\n",
    "    else:\n",
    "        searcher.set_qld(cfg.mu)\n",
    "\n",
    "    hits = searcher.search(query, cfg.k_docs)\n",
    "    passages: List[str] = []\n",
    "\n",
    "    for hit in hits:\n",
    "        try:\n",
    "            doc = searcher.doc(hit.docid)\n",
    "            content = json.loads(doc.raw()).get(\"contents\", \"\").replace(\"\\n\", \" \")\n",
    "            passages.extend(\n",
    "                extract_passages(\n",
    "                    content,\n",
    "                    cfg.window,\n",
    "                    cfg.overlap,\n",
    "                    cfg.min_passage_words,\n",
    "                )\n",
    "            )\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if not cfg.use_rerank:\n",
    "        return passages[:cfg.k_passages]\n",
    "\n",
    "    return bi_encoder_rerank(query, passages, cfg.k_passages)\n",
    "\n",
    "\n",
    "query = \"Who wrote Harry Potter?\"\n",
    "\n",
    "configs = [\n",
    "    RetrievalConfig(method=\"qld\", use_rerank=False),\n",
    "    RetrievalConfig(method=\"qld\", use_rerank=True),\n",
    "    RetrievalConfig(method=\"bm25\", use_rerank=False),\n",
    "    RetrievalConfig(method=\"bm25\", use_rerank=True),\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    print(cfg)\n",
    "    test_passages = retrieve_context(query, cfg)\n",
    "    for i, p in enumerate(test_passages, 1):\n",
    "        print(f\"{i}. {p[:100]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e849f6",
   "metadata": {},
   "source": [
    "## 4. LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b4b616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model...\n",
      "✓ Model loaded on: GPU\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Suppress transformers warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Loading LLM model...\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Set pad_token for batch processing\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "\n",
    "print(f\"✓ Model loaded on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97ae5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: temp=0.1, top_p=0.9, max_tokens=256\n",
      "✓ Generated answer: 'J. K. Rowling'\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You must respond based strictly on the information in provided passages.\"\n",
    "    \"Do not incorporate any external knowledge or infer any details beyond what is given.\"\n",
    "    \"If the answer is not in the context, return 'I dont know'.\"\n",
    "    \"Do not include explanations, only the final answer!\"\n",
    ")\n",
    "\n",
    "DEFAULT_USER_PROMPT = (\n",
    "    \"Based on the following documents, provide a concise answer to the question.\\n\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class PromptConfig:\n",
    "    \"\"\"Configuration for prompt generation and LLM parameters.\"\"\"\n",
    "    system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    "    user_prompt: str = DEFAULT_USER_PROMPT\n",
    "    temperature: float = 0.1\n",
    "    top_p: float = 0.9\n",
    "    max_new_tokens: int = 256\n",
    "    do_sample: bool = True\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"temp={self.temperature}, top_p={self.top_p}, max_tokens={self.max_new_tokens}\"\n",
    "\n",
    "def clean_answer(answer: str) -> str:\n",
    "    \"\"\"Clean and standardize the generated answer.\"\"\"\n",
    "    answer = re.sub(r'^(Answer|The answer is|Based on the .*?,):?\\s*', '', answer, flags=re.I)\n",
    "    answer = answer.rstrip('.')\n",
    "    if any(phrase in answer.lower() for phrase in [\"dont know\", \"don't know\", \"do not know\", \"unknown\"]):\n",
    "        return \"unknown\"\n",
    "    return answer.strip()\n",
    "\n",
    "def create_messages(question: str, contexts: List[str], config: PromptConfig) -> List[Dict]:\n",
    "    \"\"\"Create messages for the LLM based on the question, contexts, and prompt configuration.\"\"\"\n",
    "    if not contexts:\n",
    "        context_str = \"No relevant documents found.\"\n",
    "    else:\n",
    "        context_str = '\\n\\n'.join([f\"Document {i+1}: {ctx}\" for i, ctx in enumerate(contexts)])\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": config.system_prompt},\n",
    "        {\"role\": \"user\", \"content\": config.user_prompt.format(context=context_str, question=question)}\n",
    "    ]\n",
    "\n",
    "def generate_answer(question: str, contexts: List[str], config: PromptConfig) -> str:\n",
    "    \"\"\"Generate an answer using the LLM based on the question, contexts, and prompt configuration.\"\"\"\n",
    "    messages = create_messages(question, contexts, config)\n",
    "    \n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=config.do_sample,\n",
    "        temperature=config.temperature,\n",
    "        top_p=config.top_p,\n",
    "    )\n",
    "    \n",
    "    answer = outputs[0][\"generated_text\"][-1].get('content', '')\n",
    "    return clean_answer(answer)\n",
    "\n",
    "def batch_generate_answers(questions: List[str], contexts_list: List[List[str]], config: PromptConfig) -> List[str]:\n",
    "    \"\"\"Generate answers for multiple questions in batch.\"\"\"\n",
    "    # Create messages for all questions\n",
    "    batch_messages = [create_messages(q, ctx, config) for q, ctx in zip(questions, contexts_list)]\n",
    "    \n",
    "    # Process batch through pipeline\n",
    "    outputs = pipeline(\n",
    "        batch_messages,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=config.do_sample,\n",
    "        temperature=config.temperature,\n",
    "        top_p=config.top_p\n",
    "    )\n",
    "    \n",
    "    # Extract and clean answers\n",
    "    answers = []\n",
    "    for output in outputs:\n",
    "        answer = output[0][\"generated_text\"][-1].get('content', '')\n",
    "        answers.append(clean_answer(answer))\n",
    "    \n",
    "    return answers\n",
    "\n",
    "test_prompt_config = PromptConfig(temperature=0.1)\n",
    "print(f\"Testing: {test_prompt_config}\")\n",
    "test_answer = generate_answer(query, test_passages, test_prompt_config)\n",
    "print(f\"✓ Generated answer: '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873c1ef",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b4c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation test: F1=66.67, P=66.67, R=66.67, EM=66.67\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer for comparison\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction: str, ground_truth: str) -> float:\n",
    "    \"\"\"Compute token-level F1 score\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        return int(pred_tokens == gt_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def evaluate_predictions(df_gold: pd.DataFrame, predictions: Dict[int, str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate predictions against ground truth.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with average metrics and individual scores:\n",
    "        {\n",
    "            'f1': average_f1,\n",
    "            'precision': average_precision,\n",
    "            'recall': average_recall,\n",
    "            'exact_match': exact_match_percentage,\n",
    "            'f1_scores': list of individual f1 scores,\n",
    "            'precision_scores': list of individual precision scores,\n",
    "            'recall_scores': list of individual recall scores,\n",
    "            'exact_matches': list of individual exact match flags\n",
    "        }\n",
    "    \"\"\"\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    exact_matches = []\n",
    "    \n",
    "    for _, row in df_gold.iterrows():\n",
    "        qid = row['id']\n",
    "        if qid not in predictions:\n",
    "            f1_scores.append(0.0)\n",
    "            precision_scores.append(0.0)\n",
    "            recall_scores.append(0.0)\n",
    "            exact_matches.append(0)\n",
    "            continue\n",
    "        \n",
    "        prediction = predictions[qid]\n",
    "        ground_truths = row['answers']\n",
    "        \n",
    "        # Compute metrics for each ground truth and take the best\n",
    "        best_f1 = 0.0\n",
    "        best_precision = 0.0\n",
    "        best_recall = 0.0\n",
    "        is_exact = 0\n",
    "        \n",
    "        for gt in ground_truths:\n",
    "            # F1 score\n",
    "            pred_tokens = normalize_answer(prediction).split()\n",
    "            gt_tokens = normalize_answer(gt).split()\n",
    "            \n",
    "            common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "            num_same = sum(common.values())\n",
    "            \n",
    "            if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "                f1 = int(pred_tokens == gt_tokens)\n",
    "                prec = int(pred_tokens == gt_tokens)\n",
    "                rec = int(pred_tokens == gt_tokens)\n",
    "            elif num_same == 0:\n",
    "                f1 = 0.0\n",
    "                prec = 0.0\n",
    "                rec = 0.0\n",
    "            else:\n",
    "                prec = num_same / len(pred_tokens)\n",
    "                rec = num_same / len(gt_tokens)\n",
    "                f1 = (2 * prec * rec) / (prec + rec)\n",
    "            \n",
    "            # Track best scores\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_precision = prec\n",
    "                best_recall = rec\n",
    "            \n",
    "            # Exact match\n",
    "            if normalize_answer(prediction) == normalize_answer(gt):\n",
    "                is_exact = 1\n",
    "        \n",
    "        f1_scores.append(best_f1)\n",
    "        precision_scores.append(best_precision)\n",
    "        recall_scores.append(best_recall)\n",
    "        exact_matches.append(is_exact)\n",
    "    \n",
    "    return {\n",
    "        'f1': 100.0 * sum(f1_scores) / len(f1_scores) if f1_scores else 0.0,\n",
    "        'precision': 100.0 * sum(precision_scores) / len(precision_scores) if precision_scores else 0.0,\n",
    "        'recall': 100.0 * sum(recall_scores) / len(recall_scores) if recall_scores else 0.0,\n",
    "        'exact_match': 100.0 * sum(exact_matches) / len(exact_matches) if exact_matches else 0.0,\n",
    "        'f1_scores': f1_scores,\n",
    "        'precision_scores': precision_scores,\n",
    "        'recall_scores': recall_scores,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "test_predictions = {1: \"J.K. Rowling\", 2: \"Paris\", 3: \"Shakespeare\"}\n",
    "test_gold = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'answers': [[\"J.K. Rowling\", \"Rowling\"], [\"Earth\"], [\"William Shakespeare\", \"Shakespeare\"]]\n",
    "})\n",
    "\n",
    "test_metrics = evaluate_predictions(test_gold, test_predictions)\n",
    "print(f\"✓ Evaluation test: F1={test_metrics['f1']:.2f}, P={test_metrics['precision']:.2f}, R={test_metrics['recall']:.2f}, EM={test_metrics['exact_match']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956e446",
   "metadata": {},
   "source": [
    "## 6. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721292c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing experiment with:\n",
      "  Retrieval: QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "  Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "✓ Experiment framework ready\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    name: str,\n",
    "    df_data: pd.DataFrame,\n",
    "    retrieval_config: RetrievalConfig,\n",
    "    prompt_config: PromptConfig,\n",
    "    max_questions: Optional[int] = None,\n",
    "    batch_size: int = 8,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    if max_questions:\n",
    "        df_data = df_data.head(max_questions)\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Process in batches\n",
    "    num_batches = (len(df_data) + batch_size - 1) // batch_size\n",
    "    iterator = tqdm(range(num_batches), desc=name) if verbose else range(num_batches)\n",
    "    \n",
    "    for batch_idx in iterator:\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(df_data))\n",
    "        batch_df = df_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Retrieve contexts for all questions in batch\n",
    "        batch_questions = []\n",
    "        batch_qids = []\n",
    "        batch_contexts = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            question = row['question']\n",
    "            qid = row['id']\n",
    "            contexts = retrieve_context(question, retrieval_config)\n",
    "            \n",
    "            batch_questions.append(question)\n",
    "            batch_qids.append(qid)\n",
    "            batch_contexts.append(contexts)\n",
    "        \n",
    "        # Generate answers in batch\n",
    "        batch_answers = batch_generate_answers(batch_questions, batch_contexts, prompt_config)\n",
    "        \n",
    "        # Store predictions\n",
    "        for qid, answer in zip(batch_qids, batch_answers):\n",
    "            predictions[qid] = answer\n",
    "    \n",
    "    metrics = evaluate_predictions(df_data, predictions)\n",
    "    \n",
    "    result = {\n",
    "        'name': name,\n",
    "        'retrieval': retrieval_config,\n",
    "        'prompt': prompt_config,\n",
    "        'f1_score': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'exact_match': metrics['exact_match'],\n",
    "        'num_questions': len(df_data),\n",
    "        'predictions': predictions,\n",
    "        'f1_scores': metrics['f1_scores'],\n",
    "        'precision_scores': metrics['precision_scores'],\n",
    "        'recall_scores': metrics['recall_scores'],\n",
    "        'exact_matches': metrics['exact_matches']\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"   Retrieval: {retrieval_config}\")\n",
    "        print(f\"   Prompt: {prompt_config}\")\n",
    "        print(f\"   F1={metrics['f1']:.2f} | P={metrics['precision']:.2f} | R={metrics['recall']:.2f} | EM={metrics['exact_match']:.2f}\")\n",
    "        print(f\"   Questions: {len(df_data)}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test experiment\n",
    "test_retrieval = RetrievalConfig()\n",
    "test_prompt = PromptConfig(temperature=0.1)\n",
    "print(f\"Testing experiment with:\")\n",
    "print(f\"  Retrieval: {test_retrieval}\")\n",
    "print(f\"  Prompt: {test_prompt}\")\n",
    "\n",
    "test_exp = run_experiment(\n",
    "    \"Quick Test\",\n",
    "    df_val.head(5),\n",
    "    test_retrieval,\n",
    "    test_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Experiment framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78eead",
   "metadata": {},
   "source": [
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac8c7605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASED RETRIEVAL EXPERIMENT FRAMEWORK\n",
      "================================================================================\n",
      "Validation questions per config: 100\n",
      "Random seed: 42\n",
      "Results cache: ./results/grid_search_results.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Global experiment settings\n",
    "EXPERIMENT_SEED = 42\n",
    "EXPERIMENT_QUESTIONS = 100\n",
    "EXPERIMENT_LOG_PATH = \"./results/grid_search_results.csv\"\n",
    "\n",
    "def build_cfg(base: dict, override: dict) -> RetrievalConfig:\n",
    "    \"\"\"Build RetrievalConfig from base + override dictionaries.\"\"\"\n",
    "    return RetrievalConfig(**{**base, **override})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASED RETRIEVAL EXPERIMENT FRAMEWORK\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Validation questions per config: {EXPERIMENT_QUESTIONS}\")\n",
    "print(f\"Random seed: {EXPERIMENT_SEED}\")\n",
    "print(f\"Results cache: {EXPERIMENT_LOG_PATH}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cfg(base: dict, override: dict) -> RetrievalConfig:\n",
    "    \"\"\"Build RetrievalConfig from base + override dictionaries.\"\"\"\n",
    "    return RetrievalConfig(**{**base, **override})\n",
    "\n",
    "\n",
    "def generate_config_key(cfg: RetrievalConfig) -> str:\n",
    "    \"\"\"Generate a unique, human-readable key for a retrieval configuration.\"\"\"\n",
    "    method_part = (\n",
    "        f\"QLD_mu{cfg.mu}\"\n",
    "        if cfg.method == \"qld\"\n",
    "        else f\"BM25_k1{cfg.k1}_b{cfg.b}\"\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        f\"{cfg.method.upper()}_\"\n",
    "        f\"kdocs{cfg.k_docs}_\"\n",
    "        f\"kpass{cfg.k_passages}_\"\n",
    "        f\"{'RERANK' if cfg.use_rerank else 'NORERANK'}_\"\n",
    "        f\"{method_part}_\"\n",
    "        f\"win{cfg.window}_ovl{cfg.overlap}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def save_results_to_csv(result: dict, config_key: str, path: str) -> None:\n",
    "    \"\"\"Append experiment results to CSV.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    row = {\n",
    "        \"config_key\": config_key,\n",
    "        \"f1\": result[\"f1_score\"],\n",
    "        \"precision\": result[\"precision\"],\n",
    "        \"recall\": result[\"recall\"],\n",
    "        \"exact_match\": result[\"exact_match\"],\n",
    "        \"num_questions\": result[\"num_questions\"],\n",
    "    }\n",
    "\n",
    "    df_row = pd.DataFrame([row])\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        df_row.to_csv(path, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(path, mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "def load_completed_configs(path: str) -> set:\n",
    "    \"\"\"Load already evaluated configuration keys.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    return set(df[\"config_key\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e4fbc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed configs loaded: 34\n"
     ]
    }
   ],
   "source": [
    "validation_data = df_val.sample(\n",
    "    n=EXPERIMENT_QUESTIONS,\n",
    "    random_state=EXPERIMENT_SEED\n",
    ").reset_index(drop=True)\n",
    "\n",
    "completed_configs = load_completed_configs(EXPERIMENT_LOG_PATH)\n",
    "print(f\"Completed configs loaded: {len(completed_configs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb8d9fa",
   "metadata": {},
   "source": [
    "### Phase 1: Retrieval Method\n",
    "\n",
    "Goal: Tune QLD μ and BM25 k1/b independently\n",
    "Reranking disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09dd9a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 total configs: 18\n",
      "Phase 1 pending configs: 0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PHASE_1_BASE = {\n",
    "    \"k_docs\": 5,\n",
    "    \"k_passages\": 3,\n",
    "    \"window\": 150,\n",
    "    \"overlap\": 50,\n",
    "}\n",
    "\n",
    "PHASE_1_METHODS = (\n",
    "    [{\"method\": \"qld\", \"mu\": mu} for mu in [500, 1000, 2000]] +\n",
    "    [{\"method\": \"bm25\", \"k1\": k1, \"b\": b} for (k1, b) in itertools.product([0.6, 0.9, 1.2], [0.4, 0.75])]\n",
    ")\n",
    "\n",
    "# Build all Phase-1 configs\n",
    "phase1_configs = [\n",
    "    build_cfg(PHASE_1_BASE, {**params, \"use_rerank\": rerank})\n",
    "    for params in PHASE_1_METHODS\n",
    "    for rerank in [True, False]\n",
    "]\n",
    "\n",
    "print(f\"Phase 1 total configs: {len(phase1_configs)}\")\n",
    "\n",
    "# Filter already completed\n",
    "phase1_pending = [\n",
    "    cfg for cfg in phase1_configs\n",
    "    if generate_config_key(cfg) not in completed_configs\n",
    "]\n",
    "\n",
    "print(f\"Phase 1 pending configs: {len(phase1_pending)}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Run experiments\n",
    "for idx, cfg in enumerate(phase1_pending, start=1):\n",
    "    key = generate_config_key(cfg)\n",
    "    print(f\"[{idx}/{len(phase1_pending)}] Running: {key}\")\n",
    "\n",
    "    result = run_experiment(\n",
    "        name=key,\n",
    "        df_data=validation_data,\n",
    "        retrieval_config=cfg,\n",
    "        prompt_config=test_prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    save_results_to_csv(result, key, EXPERIMENT_LOG_PATH)\n",
    "    print(f\"✓ F1={result['f1_score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_phase_1_configs = extract_best_configs_from_phase(phase1_configs)\n",
    "\n",
    "BEST_PHASE_1_CONFIGS = [\n",
    "    {'method': 'qld', 'mu': 1000, 'use_rerank': True},\n",
    "    {'method': 'bm25', 'k1': 1.2, 'b': 0.4, 'use_rerank': True},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c448b842",
   "metadata": {},
   "source": [
    "### Phase 2: Passage Segmentation\n",
    "Goal: tune window / overlap with best retrieval params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86e95ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 total configs: 12\n",
      "Phase 2 pending configs: 9\n",
      "--------------------------------------------------------------------------------\n",
      "[1/9] Running: QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win100_ovl50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win100_ovl50: 100%|██████████| 13/13 [01:26<00:00,  6.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win100_ovl50\n",
      "   Retrieval: QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=13.15 | P=14.10 | R=20.07 | EM=4.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=13.15\n",
      "[2/9] Running: QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win150_ovl30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win150_ovl30: 100%|██████████| 13/13 [01:08<00:00,  5.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win150_ovl30\n",
      "   Retrieval: QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=13.68 | P=13.49 | R=21.62 | EM=5.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=13.68\n",
      "[3/9] Running: QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win200_ovl30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win200_ovl30: 100%|██████████| 13/13 [01:19<00:00,  6.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win200_ovl30\n",
      "   Retrieval: QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=16.79 | P=17.12 | R=27.65 | EM=9.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=16.79\n",
      "[4/9] Running: QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win200_ovl50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win200_ovl50: 100%|██████████| 13/13 [01:26<00:00,  6.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win200_ovl50\n",
      "   Retrieval: QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=19.57 | P=18.85 | R=33.80 | EM=7.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=19.57\n",
      "[5/9] Running: BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win100_ovl30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win100_ovl30: 100%|██████████| 13/13 [01:06<00:00,  5.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win100_ovl30\n",
      "   Retrieval: BM25(k1=1.2, b=0.4) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=16.59 | P=17.10 | R=25.68 | EM=5.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=16.59\n",
      "[6/9] Running: BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win100_ovl50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win100_ovl50: 100%|██████████| 13/13 [01:13<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win100_ovl50\n",
      "   Retrieval: BM25(k1=1.2, b=0.4) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=14.53 | P=14.88 | R=21.57 | EM=7.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=14.53\n",
      "[7/9] Running: BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win150_ovl30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win150_ovl30: 100%|██████████| 13/13 [01:02<00:00,  4.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win150_ovl30\n",
      "   Retrieval: BM25(k1=1.2, b=0.4) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=11.57 | P=12.56 | R=16.02 | EM=6.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=11.57\n",
      "[8/9] Running: BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win200_ovl30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win200_ovl30: 100%|██████████| 13/13 [01:10<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win200_ovl30\n",
      "   Retrieval: BM25(k1=1.2, b=0.4) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=19.72 | P=20.19 | R=32.98 | EM=10.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=19.72\n",
      "[9/9] Running: BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win200_ovl50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win200_ovl50: 100%|██████████| 13/13 [01:12<00:00,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win200_ovl50\n",
      "   Retrieval: BM25(k1=1.2, b=0.4) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=18.16 | P=17.95 | R=29.68 | EM=9.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=18.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PHASE_2_BASE = {\n",
    "    \"k_docs\": 5,\n",
    "    \"k_passages\": 3,\n",
    "}\n",
    "\n",
    "PHASE_2_PASSAGES = [\n",
    "    {\"window\": w, \"overlap\": o}\n",
    "    for w, o in itertools.product([100, 150, 200], [30, 50])\n",
    "]\n",
    "\n",
    "# Build Phase-2 configs\n",
    "phase2_configs = [\n",
    "    build_cfg(PHASE_2_BASE, {**method_cfg, **passage_cfg})\n",
    "    for method_cfg in BEST_PHASE_1_CONFIGS\n",
    "    for passage_cfg in PHASE_2_PASSAGES\n",
    "]\n",
    "\n",
    "print(f\"Phase 2 total configs: {len(phase2_configs)}\")\n",
    "\n",
    "# Filter already completed configs\n",
    "phase2_pending = [\n",
    "    cfg for cfg in phase2_configs\n",
    "    if generate_config_key(cfg) not in completed_configs\n",
    "]\n",
    "\n",
    "print(f\"Phase 2 pending configs: {len(phase2_pending)}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Run Phase-2 experiments\n",
    "for idx, cfg in enumerate(phase2_pending, start=1):\n",
    "    key = generate_config_key(cfg)\n",
    "    print(f\"[{idx}/{len(phase2_pending)}] Running: {key}\")\n",
    "\n",
    "    result = run_experiment(\n",
    "        name=key,\n",
    "        df_data=validation_data,\n",
    "        retrieval_config=cfg,\n",
    "        prompt_config=test_prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    save_results_to_csv(result, key, EXPERIMENT_LOG_PATH)\n",
    "    print(f\"✓ F1={result['f1_score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82cadd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PHASE_2_CONFIGS = [\n",
    " {'method': 'qld', 'mu': 1000, 'use_rerank': True, 'window': 200, 'overlap': 50},\n",
    " {'method': 'bm25', 'k1': 1.2, 'b': 0.4, 'use_rerank': True, 'window': 200, 'overlap': 30}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748949d",
   "metadata": {},
   "source": [
    "### Phase 3 — k_docs / k_passages Tradeoff\n",
    "Goal: tune recall vs precision tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ad0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 3 total configs: 12\n",
      "Phase 3 pending configs: 4\n",
      "--------------------------------------------------------------------------------\n",
      "[1/4] Running: QLD_kdocs15_kpass5_RERANK_QLD_mu1000_win200_ovl50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QLD_kdocs15_kpass5_RERANK_QLD_mu1000_win200_ovl50:  31%|███       | 4/13 [00:38<01:26,  9.66s/it]"
     ]
    }
   ],
   "source": [
    "PHASE_3_K = [\n",
    "    {\"k_docs\": 3, \"k_passages\": 2},\n",
    "    {\"k_docs\": 5, \"k_passages\": 3}, # baseline\n",
    "    {\"k_docs\": 8, \"k_passages\": 3},\n",
    "    {\"k_docs\": 10, \"k_passages\": 5},\n",
    "    {\"k_docs\": 15, \"k_passages\": 5},\n",
    "    {\"k_docs\": 20, \"k_passages\": 7},\n",
    "]\n",
    "\n",
    "# Build Phase-3 configs\n",
    "phase3_configs = [\n",
    "    build_cfg({}, {**method_cfg, **k_cfg})\n",
    "    for method_cfg in BEST_PHASE_2_CONFIGS\n",
    "    for k_cfg in PHASE_3_K\n",
    "]\n",
    "\n",
    "print(f\"Phase 3 total configs: {len(phase3_configs)}\")\n",
    "\n",
    "# Filter completed configs\n",
    "phase3_pending = [\n",
    "    cfg for cfg in phase3_configs\n",
    "    if generate_config_key(cfg) not in completed_configs\n",
    "]\n",
    "\n",
    "print(f\"Phase 3 pending configs: {len(phase3_pending)}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Run Phase-3 experiments\n",
    "for idx, cfg in enumerate(phase3_pending, start=1):\n",
    "    key = generate_config_key(cfg)\n",
    "    print(f\"[{idx}/{len(phase3_pending)}] Running: {key}\")\n",
    "\n",
    "    result = run_experiment(\n",
    "        name=key,\n",
    "        df_data=validation_data,\n",
    "        retrieval_config=cfg,\n",
    "        prompt_config=test_prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    save_results_to_csv(result, key, EXPERIMENT_LOG_PATH)\n",
    "    print(f\"✓ F1={result['f1_score']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
