{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80249d88",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "660a3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Literal\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79df6",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38133d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b4cf0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "544.09s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"21.0.9\" 2025-10-21\n",
      "OpenJDK Runtime Environment (build 21.0.9+10-Ubuntu-122.04)\n",
      "OpenJDK 64-Bit Server VM (build 21.0.9+10-Ubuntu-122.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4783a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff9235",
   "metadata": {},
   "source": [
    "### Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37f1afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged into Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(os.getenv('HUGGING_FACE_TOKEN'))\n",
    "print(\"✓ Logged into Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a17cc",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb3486b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3778 questions\n",
      "Test set: 2032 questions\n",
      "\n",
      "Sample question: what is the name of justin bieber brother?\n",
      "Sample answers: ['Jazmyn Bieber', 'Jaxon Bieber']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(\"./data/train.csv\", converters={\"answers\": json.loads})\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(f\"Train set: {len(df_train)} questions\")\n",
    "print(f\"Test set: {len(df_test)} questions\")\n",
    "print(f\"\\nSample question: {df_train.iloc[0]['question']}\")\n",
    "print(f\"Sample answers: {df_train.iloc[0]['answers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b208668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Split data:\n",
      "   Training: 3022 questions\n",
      "   Validation: 756 questions\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation split for experiments\n",
    "RANDOM_SEED = 42\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "df_train_split = df_train.sample(frac=1-VAL_SIZE, random_state=RANDOM_SEED)\n",
    "df_val = df_train.drop(df_train_split.index).reset_index(drop=True)\n",
    "df_train_split = df_train_split.reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Split data:\")\n",
    "print(f\"   Training: {len(df_train_split)} questions\")\n",
    "print(f\"   Validation: {len(df_val)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14492e1",
   "metadata": {},
   "source": [
    "## 3. Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ef07078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pyserini index...\n",
      "SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n",
      "✓ Index loaded: 5903530 documents\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "# Load Pyserini index\n",
    "print(\"Loading Pyserini index...\")\n",
    "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "\n",
    "print(f\"✓ Index loaded: {index_reader.stats()['documents']} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d922144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bi-encoder...\n",
      "✓ Bi-encoder loaded\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load bi-encoder\n",
    "print(\"Loading bi-encoder...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bi_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "print(\"✓ Bi-encoder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d0537dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLD(mu=1000) → NoRerank | k_docs=5, k_passages=3\n",
      "1. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "2. June 1997, the books have found immense popularity, critical acclaim and commercial success worldwid...\n",
      "3. English by two major publishers, Bloomsbury in the United Kingdom and Scholastic Press in the United...\n",
      "\n",
      "QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "1. the film, has denied that Rowling ever saw it before writing her book. Rowling has said on record mu...\n",
      "2. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "3. by Emily Brontë, \"Charlie and the Chocolate Factory\" by Roald Dahl, \"Robinson Crusoe\" by Daniel Defo...\n",
      "\n",
      "BM25(k1=0.9, b=0.4) → NoRerank | k_docs=5, k_passages=3\n",
      "1. Bonnie Wright Bonnie Francesca Wright (born 17 February 1991) is an English actress, film director, ...\n",
      "2. the Deathly Hallows – Part 1\" and \"Part 2\", she began attending London's University of the Arts: Lon...\n",
      "3. older brothers board the Hogwarts express. Her role became much more prominent in the second film, \"...\n",
      "\n",
      "BM25(k1=0.9, b=0.4) → BiEncoder | k_docs=5, k_passages=3\n",
      "1. to resolve an ongoing feud between the organisation's northern and southern branches that had sapped...\n",
      "2. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "3. children thought it was \"cool that their hero, Harry, was encountering some of the creatures of myth...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Literal\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievalConfig:\n",
    "    \"\"\"\n",
    "    Passage-based retrieval configuration with optional bi-encoder reranking.\n",
    "    \"\"\"\n",
    "    k_docs: int = 5\n",
    "    k_passages: int = 3\n",
    "    method: Literal[\"bm25\", \"qld\"] = \"qld\"\n",
    "    use_rerank: bool = True\n",
    "\n",
    "    mu: int = 1000          # QLD smoothing\n",
    "    k1: float = 0.9         # BM25\n",
    "    b: float = 0.4          # BM25\n",
    "\n",
    "    window: int = 150\n",
    "    overlap: int = 50\n",
    "    min_passage_words: int = 30\n",
    "\n",
    "    def __str__(self):\n",
    "        method_str = (\n",
    "            f\"QLD(mu={self.mu})\"\n",
    "            if self.method == \"qld\"\n",
    "            else f\"BM25(k1={self.k1}, b={self.b})\"\n",
    "        )\n",
    "        rerank_str = \"BiEncoder\" if self.use_rerank else \"NoRerank\"\n",
    "        return (\n",
    "            f\"{method_str} → {rerank_str} | \"\n",
    "            f\"k_docs={self.k_docs}, k_passages={self.k_passages}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def extract_passages(\n",
    "    text: str,\n",
    "    window: int,\n",
    "    overlap: int,\n",
    "    min_words: int,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping word windows.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    words = text.split()\n",
    "    if len(words) < min_words:\n",
    "        return []\n",
    "\n",
    "    step = max(1, window - overlap)\n",
    "    passages = []\n",
    "\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk = words[i:i + window]\n",
    "        if len(chunk) < min_words:\n",
    "            break\n",
    "        passages.append(\" \".join(chunk))\n",
    "\n",
    "    return passages\n",
    "\n",
    "\n",
    "def bi_encoder_rerank(\n",
    "    query: str,\n",
    "    passages: List[str],\n",
    "    top_k: int,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Rerank passages using bi-encoder cosine similarity.\n",
    "    \"\"\"\n",
    "    if not passages:\n",
    "        return []\n",
    "\n",
    "    q_emb = bi_encoder.encode(query, convert_to_tensor=True, device=device)\n",
    "    p_embs = bi_encoder.encode(passages, convert_to_tensor=True, device=device)\n",
    "\n",
    "    scores = util.cos_sim(q_emb, p_embs).squeeze(0)\n",
    "    top_k = min(top_k, len(passages))\n",
    "    idx = torch.topk(scores, k=top_k).indices.tolist()\n",
    "\n",
    "    return [passages[i] for i in idx]\n",
    "\n",
    "\n",
    "def retrieve_context(query: str, cfg: RetrievalConfig) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve passages using BM25 or QLD, optionally followed by bi-encoder reranking.\n",
    "    \"\"\"\n",
    "    if cfg.method == \"bm25\":\n",
    "        searcher.set_bm25(cfg.k1, cfg.b)\n",
    "    else:\n",
    "        searcher.set_qld(cfg.mu)\n",
    "\n",
    "    hits = searcher.search(query, cfg.k_docs)\n",
    "    passages: List[str] = []\n",
    "\n",
    "    for hit in hits:\n",
    "        try:\n",
    "            doc = searcher.doc(hit.docid)\n",
    "            content = json.loads(doc.raw()).get(\"contents\", \"\").replace(\"\\n\", \" \")\n",
    "            passages.extend(\n",
    "                extract_passages(\n",
    "                    content,\n",
    "                    cfg.window,\n",
    "                    cfg.overlap,\n",
    "                    cfg.min_passage_words,\n",
    "                )\n",
    "            )\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if not cfg.use_rerank:\n",
    "        return passages[:cfg.k_passages]\n",
    "\n",
    "    return bi_encoder_rerank(query, passages, cfg.k_passages)\n",
    "\n",
    "\n",
    "query = \"Who wrote Harry Potter?\"\n",
    "\n",
    "configs = [\n",
    "    RetrievalConfig(method=\"qld\", use_rerank=False),\n",
    "    RetrievalConfig(method=\"qld\", use_rerank=True),\n",
    "    RetrievalConfig(method=\"bm25\", use_rerank=False),\n",
    "    RetrievalConfig(method=\"bm25\", use_rerank=True),\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    print(cfg)\n",
    "    test_passages = retrieve_context(query, cfg)\n",
    "    for i, p in enumerate(test_passages, 1):\n",
    "        print(f\"{i}. {p[:100]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e849f6",
   "metadata": {},
   "source": [
    "## 4. LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b4b616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model...\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "device disk is invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading LLM model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m terminators \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m     pipeline\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     21\u001b[0m     pipeline\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m ]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Set pad_token for batch processing\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1027\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1026\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m-> 1027\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:293\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 293\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    295\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:5048\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5039\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   5041\u001b[0m     (\n\u001b[1;32m   5042\u001b[0m         model,\n\u001b[1;32m   5043\u001b[0m         missing_keys,\n\u001b[1;32m   5044\u001b[0m         unexpected_keys,\n\u001b[1;32m   5045\u001b[0m         mismatched_keys,\n\u001b[1;32m   5046\u001b[0m         offload_index,\n\u001b[1;32m   5047\u001b[0m         error_msgs,\n\u001b[0;32m-> 5048\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5054\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5063\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5064\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   5065\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:5468\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5465\u001b[0m         args_list \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mtqdm(args_list, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint shards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[0;32m-> 5468\u001b[0m         _error_msgs, disk_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5469\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _error_msgs\n\u001b[1;32m   5471\u001b[0m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:843\u001b[0m, in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[0;32m--> 843\u001b[0m     disk_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:698\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[0m\n\u001b[1;32m    696\u001b[0m is_safetensors \u001b[38;5;241m=\u001b[39m shard_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    697\u001b[0m is_meta_state_dict \u001b[38;5;241m=\u001b[39m is_safetensors\n\u001b[0;32m--> 698\u001b[0m file_pointer \u001b[38;5;241m=\u001b[39m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_device\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m is_meta_state_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    699\u001b[0m params_to_load \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name \u001b[38;5;129;01min\u001b[39;00m params_to_load:\n",
      "\u001b[0;31mSafetensorError\u001b[0m: device disk is invalid"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Suppress transformers warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Loading LLM model...\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Set pad_token for batch processing\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "\n",
    "print(f\"✓ Model loaded on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: temp=0.1, top_p=0.9, max_tokens=256\n",
      "✓ Generated answer: 'J. K. Rowling'\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You must respond based strictly on the information in provided passages.\"\n",
    "    \"Do not incorporate any external knowledge or infer any details beyond what is given.\"\n",
    "    \"If the answer is not in the context, return 'I dont know'.\"\n",
    "    \"Do not include explanations, only the final answer!\"\n",
    ")\n",
    "\n",
    "DEFAULT_USER_PROMPT = (\n",
    "    \"Based on the following documents, provide a concise answer to the question.\\n\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class PromptConfig:\n",
    "    \"\"\"Configuration for prompt generation and LLM parameters.\"\"\"\n",
    "    system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    "    user_prompt: str = DEFAULT_USER_PROMPT\n",
    "    temperature: float = 0.1\n",
    "    top_p: float = 0.9\n",
    "    max_new_tokens: int = 256\n",
    "    do_sample: bool = True\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"temp={self.temperature}, top_p={self.top_p}, max_tokens={self.max_new_tokens}\"\n",
    "\n",
    "def clean_answer(answer: str) -> str:\n",
    "    \"\"\"Clean and standardize the generated answer.\"\"\"\n",
    "    answer = re.sub(r'^(Answer|The answer is|Based on the .*?,):?\\s*', '', answer, flags=re.I)\n",
    "    answer = answer.rstrip('.')\n",
    "    if any(phrase in answer.lower() for phrase in [\"dont know\", \"don't know\", \"do not know\", \"unknown\"]):\n",
    "        return \"unknown\"\n",
    "    return answer.strip()\n",
    "\n",
    "def create_messages(question: str, contexts: List[str], config: PromptConfig) -> List[Dict]:\n",
    "    \"\"\"Create messages for the LLM based on the question, contexts, and prompt configuration.\"\"\"\n",
    "    if not contexts:\n",
    "        context_str = \"No relevant documents found.\"\n",
    "    else:\n",
    "        context_str = '\\n\\n'.join([f\"Document {i+1}: {ctx}\" for i, ctx in enumerate(contexts)])\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": config.system_prompt},\n",
    "        {\"role\": \"user\", \"content\": config.user_prompt.format(context=context_str, question=question)}\n",
    "    ]\n",
    "\n",
    "def generate_answer(question: str, contexts: List[str], config: PromptConfig) -> str:\n",
    "    \"\"\"Generate an answer using the LLM based on the question, contexts, and prompt configuration.\"\"\"\n",
    "    messages = create_messages(question, contexts, config)\n",
    "    \n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=config.do_sample,\n",
    "        temperature=config.temperature,\n",
    "        top_p=config.top_p,\n",
    "    )\n",
    "    \n",
    "    answer = outputs[0][\"generated_text\"][-1].get('content', '')\n",
    "    return clean_answer(answer)\n",
    "\n",
    "def batch_generate_answers(questions: List[str], contexts_list: List[List[str]], config: PromptConfig) -> List[str]:\n",
    "    \"\"\"Generate answers for multiple questions in batch.\"\"\"\n",
    "    # Create messages for all questions\n",
    "    batch_messages = [create_messages(q, ctx, config) for q, ctx in zip(questions, contexts_list)]\n",
    "    \n",
    "    # Process batch through pipeline\n",
    "    outputs = pipeline(\n",
    "        batch_messages,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=config.do_sample,\n",
    "        temperature=config.temperature,\n",
    "        top_p=config.top_p\n",
    "    )\n",
    "    \n",
    "    # Extract and clean answers\n",
    "    answers = []\n",
    "    for output in outputs:\n",
    "        answer = output[0][\"generated_text\"][-1].get('content', '')\n",
    "        answers.append(clean_answer(answer))\n",
    "    \n",
    "    return answers\n",
    "\n",
    "test_prompt_config = PromptConfig(temperature=0.1)\n",
    "print(f\"Testing: {test_prompt_config}\")\n",
    "test_answer = generate_answer(query, test_passages, test_prompt_config)\n",
    "print(f\"✓ Generated answer: '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873c1ef",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation test: F1=66.67, P=66.67, R=66.67, EM=66.67\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer for comparison\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction: str, ground_truth: str) -> float:\n",
    "    \"\"\"Compute token-level F1 score\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        return int(pred_tokens == gt_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def evaluate_predictions(df_gold: pd.DataFrame, predictions: Dict[int, str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate predictions against ground truth.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with average metrics and individual scores:\n",
    "        {\n",
    "            'f1': average_f1,\n",
    "            'precision': average_precision,\n",
    "            'recall': average_recall,\n",
    "            'exact_match': exact_match_percentage,\n",
    "            'f1_scores': list of individual f1 scores,\n",
    "            'precision_scores': list of individual precision scores,\n",
    "            'recall_scores': list of individual recall scores,\n",
    "            'exact_matches': list of individual exact match flags\n",
    "        }\n",
    "    \"\"\"\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    exact_matches = []\n",
    "    \n",
    "    for _, row in df_gold.iterrows():\n",
    "        qid = row['id']\n",
    "        if qid not in predictions:\n",
    "            f1_scores.append(0.0)\n",
    "            precision_scores.append(0.0)\n",
    "            recall_scores.append(0.0)\n",
    "            exact_matches.append(0)\n",
    "            continue\n",
    "        \n",
    "        prediction = predictions[qid]\n",
    "        ground_truths = row['answers']\n",
    "        \n",
    "        # Compute metrics for each ground truth and take the best\n",
    "        best_f1 = 0.0\n",
    "        best_precision = 0.0\n",
    "        best_recall = 0.0\n",
    "        is_exact = 0\n",
    "        \n",
    "        for gt in ground_truths:\n",
    "            # F1 score\n",
    "            pred_tokens = normalize_answer(prediction).split()\n",
    "            gt_tokens = normalize_answer(gt).split()\n",
    "            \n",
    "            common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "            num_same = sum(common.values())\n",
    "            \n",
    "            if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "                f1 = int(pred_tokens == gt_tokens)\n",
    "                prec = int(pred_tokens == gt_tokens)\n",
    "                rec = int(pred_tokens == gt_tokens)\n",
    "            elif num_same == 0:\n",
    "                f1 = 0.0\n",
    "                prec = 0.0\n",
    "                rec = 0.0\n",
    "            else:\n",
    "                prec = num_same / len(pred_tokens)\n",
    "                rec = num_same / len(gt_tokens)\n",
    "                f1 = (2 * prec * rec) / (prec + rec)\n",
    "            \n",
    "            # Track best scores\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_precision = prec\n",
    "                best_recall = rec\n",
    "            \n",
    "            # Exact match\n",
    "            if normalize_answer(prediction) == normalize_answer(gt):\n",
    "                is_exact = 1\n",
    "        \n",
    "        f1_scores.append(best_f1)\n",
    "        precision_scores.append(best_precision)\n",
    "        recall_scores.append(best_recall)\n",
    "        exact_matches.append(is_exact)\n",
    "    \n",
    "    return {\n",
    "        'f1': 100.0 * sum(f1_scores) / len(f1_scores) if f1_scores else 0.0,\n",
    "        'precision': 100.0 * sum(precision_scores) / len(precision_scores) if precision_scores else 0.0,\n",
    "        'recall': 100.0 * sum(recall_scores) / len(recall_scores) if recall_scores else 0.0,\n",
    "        'exact_match': 100.0 * sum(exact_matches) / len(exact_matches) if exact_matches else 0.0,\n",
    "        'f1_scores': f1_scores,\n",
    "        'precision_scores': precision_scores,\n",
    "        'recall_scores': recall_scores,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "test_predictions = {1: \"J.K. Rowling\", 2: \"Paris\", 3: \"Shakespeare\"}\n",
    "test_gold = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'answers': [[\"J.K. Rowling\", \"Rowling\"], [\"Earth\"], [\"William Shakespeare\", \"Shakespeare\"]]\n",
    "})\n",
    "\n",
    "test_metrics = evaluate_predictions(test_gold, test_predictions)\n",
    "print(f\"✓ Evaluation test: F1={test_metrics['f1']:.2f}, P={test_metrics['precision']:.2f}, R={test_metrics['recall']:.2f}, EM={test_metrics['exact_match']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956e446",
   "metadata": {},
   "source": [
    "## 6. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721292c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing experiment with:\n",
      "  Retrieval: QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "  Prompt: temp=0.1, top_p=0.9, max_tokens=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quick Test: 100%|██████████| 1/1 [00:04<00:00,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick Test\n",
      "   Retrieval: QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=6.67 | P=4.00 | R=20.00 | EM=0.00\n",
      "   Questions: 5\n",
      "\n",
      "✓ Experiment framework ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    name: str,\n",
    "    df_data: pd.DataFrame,\n",
    "    retrieval_config: RetrievalConfig,\n",
    "    prompt_config: PromptConfig,\n",
    "    max_questions: Optional[int] = None,\n",
    "    batch_size: int = 8,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    if max_questions:\n",
    "        df_data = df_data.head(max_questions)\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Process in batches\n",
    "    num_batches = (len(df_data) + batch_size - 1) // batch_size\n",
    "    iterator = tqdm(range(num_batches), desc=name) if verbose else range(num_batches)\n",
    "    \n",
    "    for batch_idx in iterator:\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(df_data))\n",
    "        batch_df = df_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Retrieve contexts for all questions in batch\n",
    "        batch_questions = []\n",
    "        batch_qids = []\n",
    "        batch_contexts = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            question = row['question']\n",
    "            qid = row['id']\n",
    "            contexts = retrieve_context(question, retrieval_config)\n",
    "            \n",
    "            batch_questions.append(question)\n",
    "            batch_qids.append(qid)\n",
    "            batch_contexts.append(contexts)\n",
    "        \n",
    "        # Generate answers in batch\n",
    "        batch_answers = batch_generate_answers(batch_questions, batch_contexts, prompt_config)\n",
    "        \n",
    "        # Store predictions\n",
    "        for qid, answer in zip(batch_qids, batch_answers):\n",
    "            predictions[qid] = answer\n",
    "    \n",
    "    metrics = evaluate_predictions(df_data, predictions)\n",
    "    \n",
    "    result = {\n",
    "        'name': name,\n",
    "        'retrieval': retrieval_config,\n",
    "        'prompt': prompt_config,\n",
    "        'f1_score': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'exact_match': metrics['exact_match'],\n",
    "        'num_questions': len(df_data),\n",
    "        'predictions': predictions,\n",
    "        'f1_scores': metrics['f1_scores'],\n",
    "        'precision_scores': metrics['precision_scores'],\n",
    "        'recall_scores': metrics['recall_scores'],\n",
    "        'exact_matches': metrics['exact_matches']\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"   Retrieval: {retrieval_config}\")\n",
    "        print(f\"   Prompt: {prompt_config}\")\n",
    "        print(f\"   F1={metrics['f1']:.2f} | P={metrics['precision']:.2f} | R={metrics['recall']:.2f} | EM={metrics['exact_match']:.2f}\")\n",
    "        print(f\"   Questions: {len(df_data)}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test experiment\n",
    "test_retrieval = RetrievalConfig()\n",
    "test_prompt = PromptConfig(temperature=0.1)\n",
    "print(f\"Testing experiment with:\")\n",
    "print(f\"  Retrieval: {test_retrieval}\")\n",
    "print(f\"  Prompt: {test_prompt}\")\n",
    "\n",
    "test_exp = run_experiment(\n",
    "    \"Quick Test\",\n",
    "    df_val.head(5),\n",
    "    test_retrieval,\n",
    "    test_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Experiment framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78eead",
   "metadata": {},
   "source": [
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c7605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASED RETRIEVAL EXPERIMENT FRAMEWORK\n",
      "================================================================================\n",
      "Validation questions per config: 100\n",
      "Random seed: 42\n",
      "Results cache: ./results/grid_search_results.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_SEED = 42\n",
    "EXPERIMENT_QUESTIONS = 100\n",
    "EXPERIMENT_LOG_PATH = \"./results/grid_search_results.csv\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASED RETRIEVAL EXPERIMENT FRAMEWORK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Validation questions per config: {EXPERIMENT_QUESTIONS}\")\n",
    "print(f\"Random seed: {EXPERIMENT_SEED}\")\n",
    "print(f\"Results cache: {EXPERIMENT_LOG_PATH}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cfg(base: dict, override: dict) -> RetrievalConfig:\n",
    "    \"\"\"Build RetrievalConfig from base + override dictionaries.\"\"\"\n",
    "    return RetrievalConfig(**{**base, **override})\n",
    "\n",
    "\n",
    "def generate_config_key(cfg: RetrievalConfig) -> str:\n",
    "    \"\"\"Generate a unique, human-readable key for a retrieval configuration.\"\"\"\n",
    "    method_part = (\n",
    "        f\"QLD_mu{cfg.mu}\"\n",
    "        if cfg.method == \"qld\"\n",
    "        else f\"BM25_k1{cfg.k1}_b{cfg.b}\"\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        f\"{cfg.method.upper()}_\"\n",
    "        f\"kdocs{cfg.k_docs}_\"\n",
    "        f\"kpass{cfg.k_passages}_\"\n",
    "        f\"{'RERANK' if cfg.use_rerank else 'NORERANK'}_\"\n",
    "        f\"{method_part}_\"\n",
    "        f\"win{cfg.window}_ovl{cfg.overlap}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def save_results_to_csv(result: dict, config_key: str, path: str) -> None:\n",
    "    \"\"\"Append experiment results to CSV.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    row = {\n",
    "        \"config_key\": config_key,\n",
    "        \"f1\": result[\"f1_score\"],\n",
    "        \"precision\": result[\"precision\"],\n",
    "        \"recall\": result[\"recall\"],\n",
    "        \"exact_match\": result[\"exact_match\"],\n",
    "        \"num_questions\": result[\"num_questions\"],\n",
    "    }\n",
    "\n",
    "    df_row = pd.DataFrame([row])\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        df_row.to_csv(path, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(path, mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "def load_completed_configs(path: str) -> set:\n",
    "    \"\"\"Load already evaluated configuration keys.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    return set(df[\"config_key\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4fbc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed configs loaded: 38\n"
     ]
    }
   ],
   "source": [
    "validation_data = df_val.sample(\n",
    "    n=EXPERIMENT_QUESTIONS,\n",
    "    random_state=EXPERIMENT_SEED\n",
    ").reset_index(drop=True)\n",
    "\n",
    "completed_configs = load_completed_configs(EXPERIMENT_LOG_PATH)\n",
    "print(f\"Completed configs loaded: {len(completed_configs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987015b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_per_algorithm(configs):\n",
    "    \"\"\"Return best (highest-F1) config per algorithm (QLD, BM25).\"\"\"\n",
    "    df = pd.read_csv(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    best = {\n",
    "        \"qld\": {\"f1\": -1.0, \"cfg\": None},\n",
    "        \"bm25\": {\"f1\": -1.0, \"cfg\": None},\n",
    "    }\n",
    "\n",
    "    for cfg in configs:\n",
    "        key = generate_config_key(cfg)\n",
    "        row = df[df[\"config_key\"] == key]\n",
    "        if row.empty:\n",
    "            continue\n",
    "\n",
    "        f1 = row.iloc[0][\"f1\"]\n",
    "        if f1 > best[cfg.method][\"f1\"]:\n",
    "            best[cfg.method] = {\"f1\": f1, \"cfg\": cfg}\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def run_phase(\n",
    "    *,\n",
    "    phase_name: str,\n",
    "    base_cfg: dict,\n",
    "    grid: list[dict],\n",
    "    validation_data,\n",
    "    completed_configs: set,\n",
    "    select_best_fn=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run one experiment phase:\n",
    "    - Build configs from base + grid combinations\n",
    "    - Run missing configurations\n",
    "    - Save results to CSV\n",
    "    - Optionally select best config per algorithm\n",
    "    \n",
    "    Returns:\n",
    "        List of configs (all if no selection, or best per algorithm)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(phase_name)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Build all configurations for this phase\n",
    "    configs = [build_cfg(base_cfg, g) for g in grid]\n",
    "\n",
    "    # Filter to pending configurations only\n",
    "    pending = [\n",
    "        cfg for cfg in configs\n",
    "        if generate_config_key(cfg) not in completed_configs\n",
    "    ]\n",
    "\n",
    "    print(f\"Total configs: {len(configs)}\")\n",
    "    print(f\"Pending configs: {len(pending)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Run pending experiments\n",
    "    for i, cfg in enumerate(pending, 1):\n",
    "        key = generate_config_key(cfg)\n",
    "        print(f\"\\n[{i}/{len(pending)}] Running: {key}\")\n",
    "\n",
    "        result = run_experiment(\n",
    "            name=key,\n",
    "            df_data=validation_data,\n",
    "            retrieval_config=cfg,\n",
    "            prompt_config=test_prompt,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        save_results_to_csv(result, key, EXPERIMENT_LOG_PATH)\n",
    "        print(f\"✓ F1={result['f1_score']:.2f}\")\n",
    "\n",
    "    # Return all configs if no selection function provided\n",
    "    if not select_best_fn:\n",
    "        return configs\n",
    "\n",
    "    # Select best configurations\n",
    "    best = select_best_fn(configs)\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Best configs selected:\")\n",
    "    for method, entry in best.items():\n",
    "        if entry[\"cfg\"] is not None:\n",
    "            print(\n",
    "                f\"  ✓ {method.upper()}: \"\n",
    "                f\"{generate_config_key(entry['cfg'])} | \"\n",
    "                f\"F1={entry['f1']:.2f}\"\n",
    "            )\n",
    "\n",
    "    return [entry[\"cfg\"] for entry in best.values() if entry[\"cfg\"] is not None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb8d9fa",
   "metadata": {},
   "source": [
    "### Phase 1: Retrieval Method\n",
    "\n",
    "Goal: Tune QLD μ and BM25 k1/b independently\n",
    "Reranking disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd9a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1 — RETRIEVAL METHOD & RERANKING\n",
      "================================================================================\n",
      "Total configs: 18\n",
      "Pending configs: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Best configs selected:\n",
      "  ✓ QLD: QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win150_ovl50 | F1=13.81\n",
      "  ✓ BM25: BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win150_ovl50 | F1=12.34\n"
     ]
    }
   ],
   "source": [
    "PHASE_1_BASE = {\n",
    "    \"k_docs\": 5,\n",
    "    \"k_passages\": 3,\n",
    "    \"window\": 150,\n",
    "    \"overlap\": 50,\n",
    "}\n",
    "\n",
    "PHASE_1_GRID = (\n",
    "    [{\"method\": \"qld\", \"mu\": mu, \"use_rerank\": r}\n",
    "     for mu in [500, 1000, 2000]\n",
    "     for r in [True, False]]\n",
    "    +\n",
    "    [{\"method\": \"bm25\", \"k1\": k1, \"b\": b, \"use_rerank\": r}\n",
    "     for k1, b in itertools.product([0.6, 0.9, 1.2], [0.4, 0.75])\n",
    "     for r in [True, False]]\n",
    ")\n",
    "\n",
    "BEST_PHASE_1_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 1 — RETRIEVAL METHOD & RERANKING\",\n",
    "    base_cfg=PHASE_1_BASE,\n",
    "    grid=PHASE_1_GRID,\n",
    "    validation_data=validation_data,\n",
    "    completed_configs=completed_configs,\n",
    "    select_best_fn=select_best_per_algorithm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c448b842",
   "metadata": {},
   "source": [
    "### Phase 2: Passage Segmentation\n",
    "Goal: tune window / overlap with best retrieval params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e95ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2 — PASSAGE WINDOW & OVERLAP\n",
      "================================================================================\n",
      "Total configs: 12\n",
      "Pending configs: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Best configs selected:\n",
      "  ✓ QLD: QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win200_ovl50 | F1=19.57\n",
      "  ✓ BM25: BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win200_ovl30 | F1=19.72\n"
     ]
    }
   ],
   "source": [
    "PHASE_2_BASE = {\n",
    "    \"k_docs\": 5,\n",
    "    \"k_passages\": 3,\n",
    "}\n",
    "\n",
    "PHASE_2_PASSAGES = [\n",
    "    {\"window\": w, \"overlap\": o}\n",
    "    for w, o in itertools.product([100, 150, 200], [30, 50])\n",
    "]\n",
    "\n",
    "PHASE_2_GRID = [\n",
    "    {**cfg.__dict__, **p}\n",
    "    for cfg in BEST_PHASE_1_CONFIGS\n",
    "    for p in PHASE_2_PASSAGES\n",
    "]\n",
    "\n",
    "BEST_PHASE_2_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 2 — PASSAGE WINDOW & OVERLAP\",\n",
    "    base_cfg={},\n",
    "    grid=PHASE_2_GRID,\n",
    "    validation_data=validation_data,\n",
    "    completed_configs=completed_configs,\n",
    "    select_best_fn=select_best_per_algorithm,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748949d",
   "metadata": {},
   "source": [
    "### Phase 3 — k_docs / k_passages Tradeoff\n",
    "Goal: tune recall vs precision tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f47ad0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3 — RETRIEVAL DEPTH\n",
      "================================================================================\n",
      "Total configs: 12\n",
      "Pending configs: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Best configs selected:\n",
      "  ✓ QLD: QLD_kdocs20_kpass7_RERANK_QLD_mu1000_win200_ovl50 | F1=22.83\n",
      "  ✓ BM25: BM25_kdocs20_kpass7_RERANK_BM25_k11.2_b0.4_win200_ovl30 | F1=22.41\n"
     ]
    }
   ],
   "source": [
    "PHASE_3_K = [\n",
    "    {\"k_docs\": 3, \"k_passages\": 2},\n",
    "    {\"k_docs\": 5, \"k_passages\": 3}, # baseline\n",
    "    {\"k_docs\": 8, \"k_passages\": 3},\n",
    "    {\"k_docs\": 10, \"k_passages\": 5},\n",
    "    {\"k_docs\": 15, \"k_passages\": 5},\n",
    "    {\"k_docs\": 20, \"k_passages\": 7},\n",
    "]\n",
    "\n",
    "PHASE_3_GRID = [\n",
    "    {**cfg.__dict__, **k}\n",
    "    for cfg in BEST_PHASE_2_CONFIGS\n",
    "    for k in PHASE_3_K\n",
    "]\n",
    "\n",
    "BEST_PHASE_3_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 3 — RETRIEVAL DEPTH\",\n",
    "    base_cfg={},\n",
    "    grid=PHASE_3_GRID,\n",
    "    validation_data=validation_data,\n",
    "    completed_configs=completed_configs,\n",
    "    select_best_fn=select_best_per_algorithm\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
