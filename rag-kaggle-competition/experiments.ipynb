{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80249d88",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660a3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dependencies imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Literal\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79df6",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38133d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install semantic-text-splitter\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff9235",
   "metadata": {},
   "source": [
    "### Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f1afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Logged into Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(os.getenv('HUGGING_FACE_TOKEN'))\n",
    "print(\"‚úì Logged into Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a17cc",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb3486b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3778 questions\n",
      "Test set: 2032 questions\n",
      "\n",
      "Sample question: what is the name of justin bieber brother?\n",
      "Sample answers: ['Jazmyn Bieber', 'Jaxon Bieber']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(\"./data/train.csv\", converters={\"answers\": json.loads})\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(f\"Train set: {len(df_train)} questions\")\n",
    "print(f\"Test set: {len(df_test)} questions\")\n",
    "print(f\"\\nSample question: {df_train.iloc[0]['question']}\")\n",
    "print(f\"Sample answers: {df_train.iloc[0]['answers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b208668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Split data:\n",
      "   Training: 3022 questions\n",
      "   Validation: 756 questions\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation split for experiments\n",
    "RANDOM_SEED = 42\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "df_train_split = df_train.sample(frac=1-VAL_SIZE, random_state=RANDOM_SEED)\n",
    "df_val = df_train.drop(df_train_split.index).reset_index(drop=True)\n",
    "df_train_split = df_train_split.reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úì Split data:\")\n",
    "print(f\"   Training: {len(df_train_split)} questions\")\n",
    "print(f\"   Validation: {len(df_val)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14492e1",
   "metadata": {},
   "source": [
    "## 3. Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef07078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-12-12 10:55:57.460178756 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pyserini index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dec 12, 2025 10:55:58 AM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n",
      "‚úì Index loaded: 5903530 documents\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "# Load Pyserini index\n",
    "print(\"Loading Pyserini index...\")\n",
    "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "\n",
    "print(f\"‚úì Index loaded: {index_reader.stats()['documents']} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d0537dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: QLD(k=3, mu=1000) | docs: 300chars\n",
      "‚úì Retrieved 3 documents\n",
      "  - Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "  - Bonnie Wright Bonnie Francesca Wright (born 17 February 1991) is an English actress, film director, ...\n",
      "  - Politics of Harry Potter There are many published theories about the politics of the Harry Potter bo...\n",
      "\n",
      "Testing: QLD(k=3, mu=1000) | passages: window=150, overlap=50, max=8\n",
      "‚úì Retrieved 8 passages\n",
      "  - Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "  - June 1997, the books have found immense popularity, critical acclaim and commercial success worldwid...\n",
      "  - English by two major publishers, Bloomsbury in the United Kingdom and Scholastic Press in the United...\n",
      "  - media franchises of all time. A series of many genres, including fantasy, drama, coming of age, and ...\n",
      "  - in 2012, a digital platform on which J.K. Rowling updates the series with new information and insigh...\n",
      "  - at the age of eleven that he is a wizard, though he lives in the ordinary world of non-magical peopl...\n",
      "  - adolescence, he learns to overcome the problems that face him: magical, social, and emotional, inclu...\n",
      "  - magical community of the Harry Potter books is inspired by 1990s British culture, European folklore,...\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class RetrievalConfig:\n",
    "    \"\"\"Configuration for document/passage retrieval.\"\"\"\n",
    "    k: int = 5\n",
    "    method: Literal['bm25', 'rm3', 'qld'] = 'qld'\n",
    "    mu: int = 1000\n",
    "    k1: float = 0.9\n",
    "    b: float = 0.4\n",
    "    rm3_terms: int = 10\n",
    "    rm3_docs: int = 10\n",
    "    rm3_weight: float = 0.5\n",
    "    use_passages: bool = False\n",
    "    window: int = 150\n",
    "    overlap: int = 50\n",
    "    min_passage_words: int = 30\n",
    "    max_passages: int = 10\n",
    "    max_chars: int = 300\n",
    "    \n",
    "    def __str__(self):\n",
    "        method_params = \"\"\n",
    "        if self.method == 'qld':\n",
    "            method_params = f\"mu={self.mu}\"\n",
    "        elif self.method == 'bm25':\n",
    "            method_params = f\"k1={self.k1}, b={self.b}\"\n",
    "        elif self.method == 'rm3':\n",
    "            method_params = f\"terms={self.rm3_terms}, docs={self.rm3_docs}\"\n",
    "        \n",
    "        if self.use_passages:\n",
    "            mode_info = f\"passages: window={self.window}, overlap={self.overlap}, max={self.max_passages}\"\n",
    "        else:\n",
    "            mode_info = f\"docs: {self.max_chars}chars\"\n",
    "        \n",
    "        return f\"{self.method.upper()}(k={self.k}, {method_params}) | {mode_info}\"\n",
    "\n",
    "def extract_passages(text, window=150, overlap=50, min_words=30):\n",
    "    \"\"\"Extract overlapping passages from text.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    words = text.split()\n",
    "    if len(words) < min_words:\n",
    "        return []\n",
    "    \n",
    "    passages = []\n",
    "    step = max(1, window - overlap)\n",
    "    \n",
    "    for start in range(0, len(words), step):\n",
    "        chunk = words[start:start + window]\n",
    "        \n",
    "        if len(chunk) < min_words:\n",
    "            if passages:\n",
    "                passages[-1] += \" \" + \" \".join(chunk)\n",
    "            else:\n",
    "                passages.append(\" \".join(chunk))\n",
    "            break\n",
    "        \n",
    "        passages.append(\" \".join(chunk))\n",
    "    \n",
    "    return passages\n",
    "\n",
    "def retrieve_context(query: str, config: RetrievalConfig) -> List[str]:\n",
    "    \"\"\"Retrieve documents or passages for a given query based on the retrieval configuration.\"\"\"\n",
    "    if config.method == 'bm25':\n",
    "        searcher.set_bm25(config.k1, config.b)\n",
    "    elif config.method == 'rm3':\n",
    "        searcher.set_rm3(config.rm3_terms, config.rm3_docs, config.rm3_weight)\n",
    "    else:\n",
    "        searcher.set_qld(config.mu)\n",
    "    \n",
    "    hits = searcher.search(query, config.k)\n",
    "    \n",
    "    contexts = []\n",
    "    for hit in hits:\n",
    "        try:\n",
    "            doc = searcher.doc(hit.docid)\n",
    "            data = json.loads(doc.raw())\n",
    "            content = data['contents'].replace('\\n', ' ')\n",
    "            \n",
    "            if config.use_passages:\n",
    "                passages = extract_passages(content, config.window, config.overlap, config.min_passage_words)\n",
    "                contexts.extend(passages)\n",
    "            else:\n",
    "                contexts.append(content[:config.max_chars])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return contexts[:config.max_passages] if config.use_passages else contexts\n",
    "\n",
    "query = \"Who wrote Harry Potter?\"\n",
    "\n",
    "test_config_docs = RetrievalConfig(k=3, method='qld', mu=1000)\n",
    "print(f\"Testing: {test_config_docs}\")\n",
    "test_docs = retrieve_context(query, test_config_docs)\n",
    "print(f\"‚úì Retrieved {len(test_docs)} documents\")\n",
    "for doc in test_docs:\n",
    "    print(f\"  - {doc[:100]}...\")\n",
    "print()\n",
    "\n",
    "test_config_passages = RetrievalConfig(k=3, method='qld', mu=1000,\n",
    "                                       use_passages=True, window=150, overlap=50, max_passages=8)\n",
    "print(f\"Testing: {test_config_passages}\")\n",
    "test_passages = retrieve_context(query, test_config_passages)\n",
    "print(f\"‚úì Retrieved {len(test_passages)} passages\")\n",
    "for passage in test_passages:\n",
    "    print(f\"  - {passage[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e849f6",
   "metadata": {},
   "source": [
    "## 4. LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b4b616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model...\n",
      "‚úì Model loaded on: GPU\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Suppress transformers warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Loading LLM model...\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "print(f\"‚úì Model loaded on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97ae5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: temp=0.1, top_p=0.9, max_tokens=256\n",
      "‚úì Generated answer: 'J. K. Rowling'\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You must respond based strictly on the information in provided passages.\"\n",
    "    \"Do not incorporate any external knowledge or infer any details beyond what is given.\"\n",
    "    \"If the answer is not in the context, return 'I dont know'.\"\n",
    "    \"Do not include explanations, only the final answer!\"\n",
    ")\n",
    "\n",
    "DEFAULT_USER_PROMPT = (\n",
    "    \"Based on the following documents, provide a concise answer to the question.\\n\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class PromptConfig:\n",
    "    \"\"\"Configuration for prompt generation and LLM parameters.\"\"\"\n",
    "    system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    "    user_prompt: str = DEFAULT_USER_PROMPT\n",
    "    temperature: float = 0.6\n",
    "    top_p: float = 0.9\n",
    "    max_new_tokens: int = 256\n",
    "    do_sample: bool = True\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"temp={self.temperature}, top_p={self.top_p}, max_tokens={self.max_new_tokens}\"\n",
    "\n",
    "def clean_answer(answer: str) -> str:\n",
    "    \"\"\"Clean and standardize the generated answer.\"\"\"\n",
    "    answer = re.sub(r'^(Answer|The answer is|Based on the .*?,):?\\s*', '', answer, flags=re.I)\n",
    "    answer = answer.rstrip('.')\n",
    "    if any(phrase in answer.lower() for phrase in [\"dont know\", \"don't know\", \"do not know\", \"unknown\"]):\n",
    "        return \"unknown\"\n",
    "    return answer.strip()\n",
    "\n",
    "def create_messages(question: str, contexts: List[str], config: PromptConfig) -> List[Dict]:\n",
    "    \"\"\"Create messages for the LLM based on the question, contexts, and prompt configuration.\"\"\"\n",
    "    if not contexts:\n",
    "        context_str = \"No relevant documents found.\"\n",
    "    else:\n",
    "        context_str = '\\n\\n'.join([f\"Document {i+1}: {ctx}\" for i, ctx in enumerate(contexts)])\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": config.system_prompt},\n",
    "        {\"role\": \"user\", \"content\": config.user_prompt.format(context=context_str, question=question)}\n",
    "    ]\n",
    "\n",
    "def generate_answer(question: str, contexts: List[str], config: PromptConfig) -> str:\n",
    "    \"\"\"Generate an answer using the LLM based on the question, contexts, and prompt configuration.\"\"\"\n",
    "    messages = create_messages(question, contexts, config)\n",
    "    \n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=config.do_sample,\n",
    "        temperature=config.temperature,\n",
    "        top_p=config.top_p,\n",
    "    )\n",
    "    \n",
    "    answer = outputs[0][\"generated_text\"][-1].get('content', '')\n",
    "    return clean_answer(answer)\n",
    "\n",
    "test_prompt_config = PromptConfig(temperature=0.1)\n",
    "print(f\"Testing: {test_prompt_config}\")\n",
    "test_answer = generate_answer(query, test_passages, test_prompt_config)\n",
    "print(f\"‚úì Generated answer: '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873c1ef",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b4c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Evaluation test: F1 = 66.67\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer for comparison\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction: str, ground_truth: str) -> float:\n",
    "    \"\"\"Compute token-level F1 score\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        return int(pred_tokens == gt_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def evaluate_predictions(df_gold: pd.DataFrame, predictions: Dict[int, str]) -> Tuple[float, List[float]]:\n",
    "    \"\"\"\n",
    "    Evaluate predictions against ground truth.\n",
    "    \n",
    "    Returns:\n",
    "        (average_f1_score, list_of_individual_f1_scores)\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for _, row in df_gold.iterrows():\n",
    "        qid = row['id']\n",
    "        if qid not in predictions:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        prediction = predictions[qid]\n",
    "        ground_truths = row['answers']\n",
    "        \n",
    "        # Get max F1 over all ground truths\n",
    "        max_f1 = max(f1_score(prediction, gt) for gt in ground_truths)\n",
    "        scores.append(max_f1)\n",
    "    \n",
    "    avg_score = 100.0 * sum(scores) / len(scores) if scores else 0.0\n",
    "    return avg_score, scores\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "test_predictions = {1: \"J.K. Rowling\", 2: \"Paris\", 3: \"Shakespeare\"}\n",
    "test_gold = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'answers': [[\"J.K. Rowling\", \"Rowling\"], [\"Earth\"], [\"William Shakespeare\", \"Shakespeare\"]]\n",
    "})\n",
    "test_score, _ = evaluate_predictions(test_gold, test_predictions)\n",
    "print(f\"‚úì Evaluation test: F1 = {test_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956e446",
   "metadata": {},
   "source": [
    "## 6. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "721292c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing experiment with:\n",
      "  Retrieval: QLD(k=3, mu=1000) | docs: 300chars\n",
      "  Prompt: temp=0.1, top_p=0.9, max_tokens=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quick Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:14<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick Test\n",
      "   Retrieval: QLD(k=3, mu=1000) | docs: 300chars\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1 Score: 14.67\n",
      "   Questions: 5\n",
      "\n",
      "‚úì Experiment framework ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    name: str,\n",
    "    df_data: pd.DataFrame,\n",
    "    retrieval_config: RetrievalConfig,\n",
    "    prompt_config: PromptConfig,\n",
    "    max_questions: Optional[int] = None,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    if max_questions:\n",
    "        df_data = df_data.head(max_questions)\n",
    "    \n",
    "    predictions = {}\n",
    "    iterator = tqdm(df_data.iterrows(), total=len(df_data), desc=name) if verbose else df_data.iterrows()\n",
    "    \n",
    "    for _, row in iterator:\n",
    "        question = row['question']\n",
    "        qid = row['id']\n",
    "        \n",
    "        contexts = retrieve_context(question, retrieval_config)\n",
    "        answer = generate_answer(question, contexts, prompt_config)\n",
    "        predictions[qid] = answer\n",
    "    \n",
    "    f1, individual_scores = evaluate_predictions(df_data, predictions)\n",
    "    \n",
    "    result = {\n",
    "        'name': name,\n",
    "        'retrieval': retrieval_config,\n",
    "        'prompt': prompt_config,\n",
    "        'f1_score': f1,\n",
    "        'num_questions': len(df_data),\n",
    "        'predictions': predictions,\n",
    "        'individual_scores': individual_scores\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"   Retrieval: {retrieval_config}\")\n",
    "        print(f\"   Prompt: {prompt_config}\")\n",
    "        print(f\"   F1 Score: {f1:.2f}\")\n",
    "        print(f\"   Questions: {len(df_data)}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "test_retrieval = RetrievalConfig(k=3, method='qld')\n",
    "test_prompt = PromptConfig(temperature=0.1)\n",
    "print(f\"Testing experiment with:\")\n",
    "print(f\"  Retrieval: {test_retrieval}\")\n",
    "print(f\"  Prompt: {test_prompt}\")\n",
    "\n",
    "test_exp = run_experiment(\n",
    "    \"Quick Test\",\n",
    "    df_val.head(5),\n",
    "    test_retrieval,\n",
    "    test_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"‚úì Experiment framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dbefa",
   "metadata": {},
   "source": [
    "## 7. Run Experiments\n",
    "\n",
    "Now we can quickly test different configurations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b73a4bc",
   "metadata": {},
   "source": [
    "### Quick Test (10 questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89ba21",
   "metadata": {},
   "source": [
    "### Experiment 1: Baseline (QL Dirichlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55cb0da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline - QLD Docs:   6%|‚ñå         | 6/100 [00:24<06:18,  4.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m exp1_baseline \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBaseline - QLD Docs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrieval_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRetrievalConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqld\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPromptConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_questions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(name, df_data, retrieval_config, prompt_config, max_questions, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m     qid \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m     contexts \u001b[38;5;241m=\u001b[39m retrieve_context(question, retrieval_config)\n\u001b[0;32m---> 20\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     predictions[qid] \u001b[38;5;241m=\u001b[39m answer\n\u001b[1;32m     23\u001b[0m f1, individual_scores \u001b[38;5;241m=\u001b[39m evaluate_predictions(df_data, predictions)\n",
      "Cell \u001b[0;32mIn[13], line 52\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(question, contexts, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate an answer using the LLM based on the question, contexts, and prompt configuration.\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m messages \u001b[38;5;241m=\u001b[39m create_messages(question, contexts, config)\n\u001b[0;32m---> 52\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m answer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_answer(answer)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:325\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m         chats \u001b[38;5;241m=\u001b[39m (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# üêà üêà üêà\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1467\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1461\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1464\u001b[0m         )\n\u001b[1;32m   1465\u001b[0m     )\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1474\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1473\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1474\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1374\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1373\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1374\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:432\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    430\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 432\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    435\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2779\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2777\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 2779\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2780\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   2781\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   2595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2596\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp1_baseline = run_experiment(\n",
    "    name=\"Baseline - QLD Docs\",\n",
    "    df_data=df_val,\n",
    "    retrieval_config=RetrievalConfig(k=5, method='qld', mu=1000, max_chars=300),\n",
    "    prompt_config=PromptConfig(temperature=0.6, top_p=0.9),\n",
    "    max_questions=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b09a252",
   "metadata": {},
   "source": [
    "### Experiment 2: Try BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17915275",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_bm25 = run_experiment(\n",
    "    name=\"BM25 Docs\",\n",
    "    df_data=df_val,\n",
    "    retrieval_config=RetrievalConfig(k=5, method='bm25', k1=0.9, b=0.4, max_chars=300),\n",
    "    prompt_config=PromptConfig(temperature=0.6, top_p=0.9),\n",
    "    max_questions=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35dba74",
   "metadata": {},
   "source": [
    "### Experiment 3: RM3 Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_rm3 = run_experiment(\n",
    "    name=\"RM3 Query Expansion Docs\",\n",
    "    df_data=df_val,\n",
    "    retrieval_config=RetrievalConfig(\n",
    "        k=5, \n",
    "        method='rm3', \n",
    "        rm3_terms=10, \n",
    "        rm3_docs=10,\n",
    "        rm3_weight=0.5,\n",
    "        max_chars=300\n",
    "    ),\n",
    "    prompt_config=PromptConfig(temperature=0.6, top_p=0.9),\n",
    "    max_questions=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329f4d8",
   "metadata": {},
   "source": [
    "### Experiment 4: QLD with Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efacc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp4_qld_passages = run_experiment(\n",
    "    name=\"QLD Passages\",\n",
    "    df_data=df_val,\n",
    "    retrieval_config=RetrievalConfig(\n",
    "        k=5, \n",
    "        method='qld', \n",
    "        mu=1000,\n",
    "\n",
    "        use_passages=True,)\n",
    "\n",
    "        window=150,    max_questions=100\n",
    "\n",
    "        overlap=50,    prompt_config=PromptConfig(temperature=0.6, top_p=0.9),\n",
    "\n",
    "        max_passages=8    ),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda2ad2",
   "metadata": {},
   "source": [
    "### Experiment 5: BM25 with Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e10e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp5_bm25_passages = run_experiment(\n",
    "    name=\"BM25 Passages\",\n",
    "    df_data=df_val,\n",
    "    retrieval_config=RetrievalConfig(\n",
    "        k=5,\n",
    "        method='bm25',\n",
    "        k1=0.9,\n",
    "\n",
    "        b=0.4,)\n",
    "\n",
    "        use_passages=True,    max_questions=100\n",
    "\n",
    "        window=150,    prompt_config=PromptConfig(temperature=0.6, top_p=0.9),\n",
    "\n",
    "        overlap=50,    ),\n",
    "        max_passages=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2457cec",
   "metadata": {},
   "source": [
    "### Experiment 6: More Documents (k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp6_more_docs = run_experiment(\n",
    "    name=\"More Docs (k=10)\",\n",
    "    df_data=df_val,\n",
    "    retrieval_config=RetrievalConfig(k=10, method='qld', mu=1000, max_chars=300),\n",
    "    prompt_config=PromptConfig(temperature=0.6, top_p=0.9),\n",
    "    max_questions=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32931d",
   "metadata": {},
   "source": [
    "## 8. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c676f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all experiments\n",
    "experiments = [\n",
    "    exp1_baseline,\n",
    "    exp2_bm25,\n",
    "    exp3_rm3,\n",
    "    exp4_qld_passages,\n",
    "    exp5_bm25_passages,\n",
    "    exp6_more_docs\n",
    "]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': exp['name'],\n",
    "        'F1 Score': exp['f1_score'],\n",
    "        'Retrieval': str(exp['retrieval']),\n",
    "        'Prompt': f\"temp={exp['prompt'].temperature}\"\n",
    "    }\n",
    "    for exp in experiments\n",
    "])\n",
    "\n",
    "results_df = results_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä EXPERIMENT RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best configuration\n",
    "best_exp = experiments[results_df.index[0]]\n",
    "print(f\"\\nüèÜ BEST CONFIGURATION:\")\n",
    "\n",
    "print(f\"   Name: {best_exp['name']}\")print(f\"   F1 Score: {best_exp['f1_score']:.2f}\")\n",
    "\n",
    "print(f\"   Retrieval: {best_exp['retrieval']}\")print(f\"   Prompt: {best_exp['prompt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261142d",
   "metadata": {},
   "source": [
    "## 9. Generate Submission with Best Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76825b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best configuration to generate predictions for test set\n",
    "print(\"üöÄ Generating predictions for test set with best configuration...\")\n",
    "\n",
    "test_predictions = run_experiment(\n",
    "    name=\"Final Test Submission\",\n",
    "    df_data=df_test,\n",
    "    retrieval_config=best_exp['retrieval'],\n",
    "    prompt_config=best_exp['prompt'],\n",
    "    max_questions=None,  # Use all test questions\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_df = pd.DataFrame([\n",
    "    {'id': qid, 'prediction': json.dumps([pred], ensure_ascii=False)}\n",
    "    for qid, pred in test_predictions['predictions'].items()\n",
    "])\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"../data/submission_best_config.csv\"\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úì Submission saved to: {output_path}\")\n",
    "print(f\"üìä Total predictions: {len(submission_df)}\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fddd8",
   "metadata": {},
   "source": [
    "## 10. Error Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23acfe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors from validation set\n",
    "def analyze_errors(result: Dict, df_gold: pd.DataFrame, top_n: int = 10):\n",
    "    \"\"\"Show worst performing questions\"\"\"\n",
    "    error_analysis = []\n",
    "    \n",
    "    for idx, row in df_gold.iterrows():\n",
    "        qid = row['id']\n",
    "        if qid in result['predictions']:\n",
    "            score = result['individual_scores'][idx]\n",
    "            error_analysis.append({\n",
    "                'id': qid,\n",
    "                'question': row['question'],\n",
    "                'prediction': result['predictions'][qid],\n",
    "                'ground_truth': row['answers'],\n",
    "                'f1_score': score\n",
    "            })\n",
    "    \n",
    "    error_df = pd.DataFrame(error_analysis).sort_values('f1_score')\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚ùå WORST {top_n} PREDICTIONS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for idx, row in error_df.head(top_n).iterrows():\n",
    "        print(f\"\\nQ: {row['question']}\")\n",
    "        print(f\"Predicted: {row['prediction']}\")\n",
    "        print(f\"Expected: {row['ground_truth']}\")\n",
    "        print(f\"F1: {row['f1_score']:.2f}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return error_df\n",
    "\n",
    "# Analyze best experiment\n",
    "error_df = analyze_errors(best_exp, df_val, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c8c32",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Next Steps\n",
    "\n",
    "1. **Review error analysis** to understand failure modes\n",
    "2. **Try custom configurations** by creating new RetrievalConfig/PromptConfig objects\n",
    "3. **Combine best strategies** (e.g., RM3 + lower temperature + longer context)\n",
    "4. **Submit to Kaggle** and compare with leaderboard scores\n",
    "5. **Iterate based on results**\n",
    "\n",
    "Happy experimenting! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
