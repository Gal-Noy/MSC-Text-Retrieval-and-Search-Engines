{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e61fc84",
   "metadata": {},
   "source": [
    "# Text Retrieval and Search Engines - Assignment 3\n",
    "**Gal Noy** · 209346486"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80249d88",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660a3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Literal\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79df6",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48dd5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install -y openjdk-21-jdk\n",
    "# !update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-21-openjdk-amd64/bin/java 1\n",
    "# !update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac 1\n",
    "# !update-alternatives --set java /usr/lib/jvm/java-21-openjdk-amd64/bin/java\n",
    "# !update-alternatives --set javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4cf0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"21.0.9\" 2025-10-21\n",
      "OpenJDK Runtime Environment (build 21.0.9+10-Ubuntu-122.04)\n",
      "OpenJDK 64-Bit Server VM (build 21.0.9+10-Ubuntu-122.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6308b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff9235",
   "metadata": {},
   "source": [
    "### Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f1afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged into Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(os.getenv('HUGGING_FACE_TOKEN'))\n",
    "print(\"✓ Logged into Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a17cc",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3486b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3778 questions\n",
      "Test set: 2032 questions\n",
      "\n",
      "Sample question: what is the name of justin bieber brother?\n",
      "Sample answers: ['Jazmyn Bieber', 'Jaxon Bieber']\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./data/train.csv\", converters={\"answers\": json.loads})\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(f\"Train set: {len(df_train)} questions\")\n",
    "print(f\"Test set: {len(df_test)} questions\")\n",
    "print(f\"\\nSample question: {df_train.iloc[0]['question']}\")\n",
    "print(f\"Sample answers: {df_train.iloc[0]['answers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14492e1",
   "metadata": {},
   "source": [
    "## 3. Retrieval Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51843bc1",
   "metadata": {},
   "source": [
    "### Pyserini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef07078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-12-15 21:14:02.439824503 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pyserini index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dec 15, 2025 9:14:03 PM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n",
      "✓ Index loaded: 5903530 documents\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "print(\"Loading Pyserini index...\")\n",
    "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "\n",
    "print(f\"✓ Index loaded: {index_reader.stats()['documents']} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286c2fc",
   "metadata": {},
   "source": [
    "### Cross-encoder reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d922144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cross-encoder reranker...\n",
      "✓ Cross-encoder loaded\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Loading cross-encoder reranker...\")\n",
    "reranker = CrossEncoder(\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    device=device,\n",
    "    max_length=512,\n",
    ")\n",
    "print(\"✓ Cross-encoder loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7a1c3",
   "metadata": {},
   "source": [
    "### Retrieval manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0537dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalManager(k_docs=100, k_passages=7, RRF_k=60, window=150, overlap=50)\n",
      "1. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The novels chronicle th...\n",
      "2. J. K. Rowling Joanne Rowling ( \"rolling\"; born 31 July 1965), better known by her pen name J. K. Rowling, is a British n...\n",
      "3. The Magical Worlds of Harry Potter The Magical Worlds of Harry Potter: A Treasury of Myths, Legends, and Fascinating Fac...\n",
      "4. English by two major publishers, Bloomsbury in the United Kingdom and Scholastic Press in the United States. A play, \"Ha...\n",
      "5. the lives of the surviving characters and the effects of Voldemort's death on the Wizarding World. In the epilogue, Harr...\n",
      "6. Fry reading the UK editions and Jim Dale voicing the series for the American editions. Section::::Adaptations.:Stage pro...\n",
      "7. Harry Potter influences and analogues Writer J. K. Rowling cites several writers as influences in her creation of her be...\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_doc_content(docid: str) -> str:\n",
    "    \"\"\"Return cached raw document text.\"\"\"\n",
    "    try:\n",
    "        doc = searcher.doc(docid)\n",
    "        return json.loads(doc.raw()).get(\"contents\", \"\").replace(\"\\n\", \" \")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class RetrievalManager:\n",
    "    \"\"\"\n",
    "    Hybrid retrieval with:\n",
    "      - BM25 document retrieval\n",
    "      - QLD document retrieval\n",
    "      - RRF on documents (recall stage)\n",
    "      - Passage segmentation\n",
    "      - Cross-encoder passage reranking (precision stage)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieval\n",
    "    k_docs: int = 100\n",
    "    k_passages: int = 7\n",
    "    rrf_k: int = 60\n",
    "\n",
    "    # BM25 / QLD\n",
    "    mu: int = 1000\n",
    "    k1: float = 0.9\n",
    "    b: float = 0.4\n",
    "\n",
    "    # Passage extraction\n",
    "    window: int = 150\n",
    "    overlap: int = 50\n",
    "    min_passage_words: int = 30\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"RetrievalManager(\"\n",
    "            f\"k_docs={self.k_docs}, k_passages={self.k_passages}, \"\n",
    "            f\"RRF_k={self.rrf_k}, \"\n",
    "            f\"window={self.window}, overlap={self.overlap})\"\n",
    "        )\n",
    "\n",
    "    def extract_passages(self, text: str) -> List[str]:\n",
    "        \"\"\"Split document text into overlapping word windows.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        words = text.split()\n",
    "        if len(words) < self.min_passage_words:\n",
    "            return []\n",
    "\n",
    "        step = max(1, self.window - self.overlap)\n",
    "        passages = []\n",
    "\n",
    "        for i in range(0, len(words), step):\n",
    "            chunk = words[i:i + self.window]\n",
    "            if len(chunk) < self.min_passage_words:\n",
    "                break\n",
    "            passages.append(\" \".join(chunk))\n",
    "\n",
    "        return passages\n",
    "\n",
    "    def rerank_passages(self, query: str, passages: List[str]) -> List[str]:\n",
    "        \"\"\"Rerank passages using cross-encoder (ordering only).\"\"\"\n",
    "        if not passages:\n",
    "            return []\n",
    "\n",
    "        pairs = [(query, p) for p in passages]\n",
    "        scores = reranker.predict(\n",
    "            pairs,\n",
    "            batch_size=16,\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "\n",
    "        ranked = sorted(\n",
    "            zip(passages, scores),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        return [p for p, _ in ranked[:self.k_passages]]\n",
    "    \n",
    "    def retrieve_context(self, query: str) -> List[str]:\n",
    "        \"\"\"Return top answer-bearing passages.\"\"\"\n",
    "\n",
    "        # Lexical retrieval\n",
    "        searcher.set_bm25(self.k1, self.b)\n",
    "        bm25_docids = [h.docid for h in searcher.search(query, self.k_docs)]\n",
    "\n",
    "        searcher.set_qld(self.mu)\n",
    "        qld_docids = [h.docid for h in searcher.search(query, self.k_docs)]\n",
    "\n",
    "        # RRF on documents\n",
    "        doc_scores: Dict[str, float] = {}\n",
    "\n",
    "        for rank, docid in enumerate(bm25_docids):\n",
    "            doc_scores[docid] = doc_scores.get(docid, 0.0) + 1.0 / (self.rrf_k + rank + 1)\n",
    "\n",
    "        for rank, docid in enumerate(qld_docids):\n",
    "            doc_scores[docid] = doc_scores.get(docid, 0.0) + 1.0 / (self.rrf_k + rank + 1)\n",
    "\n",
    "        ranked_docids = sorted(\n",
    "            doc_scores,\n",
    "            key=doc_scores.get,\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        # Passage extraction\n",
    "        passages: List[str] = []\n",
    "\n",
    "        for docid in ranked_docids:\n",
    "            content = get_doc_content(docid)\n",
    "            if not content:\n",
    "                continue\n",
    "\n",
    "            passages.extend(self.extract_passages(content))\n",
    "\n",
    "            # implicit cap: enough passages for reranking\n",
    "            if len(passages) >= self.k_docs * 5:\n",
    "                break\n",
    "\n",
    "        # Cross-encoder reranking\n",
    "        return self.rerank_passages(query, passages)\n",
    "    \n",
    "\n",
    "query = \"Who wrote Harry Potter?\"\n",
    "\n",
    "rm = RetrievalManager()\n",
    "print(rm)\n",
    "\n",
    "passages = rm.retrieve_context(query)\n",
    "for i, p in enumerate(passages, 1):\n",
    "    print(f\"{i}. {p[:120]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e849f6",
   "metadata": {},
   "source": [
    "## 4. LLM Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9df06",
   "metadata": {},
   "source": [
    "### Load LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b4b616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model...\n",
      "✓ Model loaded on: GPU\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Suppress transformers warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Loading LLM model...\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    device=0\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Set pad_token for batch processing\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "\n",
    "print(f\"✓ Model loaded on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1fa0d",
   "metadata": {},
   "source": [
    "### Prompt manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ae5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: temp=0.0, top_p=1.0, max_tokens=64\n",
      "✓ Generated answer: 'J. K. Rowling'\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a strict, grounded Question Answering system.\\n\"\n",
    "    \"You are given documents and a question.\\n\"\n",
    "    \"Answer ONLY using information that appears in the documents.\\n\"\n",
    "    \"Your answer must be ONLY the entity or value that answers the question.\\n\"\n",
    "    \"Do NOT return sentences, clauses, or descriptions.\\n\"\n",
    "    \"If the answer cannot be verified in the documents, return: unknown.\"\n",
    ")\n",
    "\n",
    "\n",
    "USER_PROMPT = (\n",
    "    \"Documents:\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Task:\\n\"\n",
    "    \"- Answer the question using only the documents.\\n\"\n",
    "    \"- Return ONE single answer only (not a list or multiple items).\\n\"\n",
    "    \"- The answer must match the question type (person, place, date, number).\\n\"\n",
    "    \"- Return the shortest complete answer that answers the question.\\n\"\n",
    "    \"- The answer must appear verbatim or as a clear entity in the documents.\\n\"\n",
    "    \"- Do NOT return explanations, relations, or full sentences.\\n\"\n",
    "    \"- If you cannot verify the answer in the documents, output: unknown.\\n\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PromptManager:\n",
    "    \"\"\"Manages prompt generation and LLM answer generation.\"\"\"\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = USER_PROMPT\n",
    "    temperature: float = 0.0\n",
    "    top_p: float = 1.0\n",
    "    max_new_tokens: int = 64\n",
    "    do_sample: bool = False\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"temp={self.temperature}, top_p={self.top_p}, max_tokens={self.max_new_tokens}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_answer(answer: str) -> str:\n",
    "        \"\"\"Clean and standardize the generated answer.\"\"\"\n",
    "        answer = re.sub(r'^(Answer|The answer is|Based on the .*?,):?\\s*', '', answer, flags=re.I)\n",
    "        answer = answer.rstrip('.')\n",
    "        if any(phrase in answer.lower() for phrase in [\"dont know\", \"don't know\", \"do not know\", \"unknown\"]):\n",
    "            return \"unknown\"\n",
    "        return answer.strip()\n",
    "\n",
    "    def create_messages(self, question: str, contexts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Create messages for the LLM based on the question and contexts.\"\"\"\n",
    "        if not contexts:\n",
    "            context_str = \"No relevant documents found.\"\n",
    "        else:\n",
    "            context_str = '\\n\\n'.join([f\"Document {i+1}: {ctx}\" for i, ctx in enumerate(contexts)])\n",
    "        \n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": self.user_prompt.format(context=context_str, question=question)}\n",
    "        ]\n",
    "\n",
    "    def generate_answer(self, question: str, contexts: List[str]) -> str:\n",
    "        \"\"\"Generate an answer using the LLM based on the question and contexts.\"\"\"\n",
    "        messages = self.create_messages(question, contexts)\n",
    "        \n",
    "        outputs = pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=self.do_sample,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p\n",
    "        )\n",
    "        \n",
    "        answer = outputs[0][\"generated_text\"][-1].get('content', '')\n",
    "        return self.clean_answer(answer)\n",
    "\n",
    "\n",
    "test_prompt_manager = PromptManager()\n",
    "print(f\"Testing: {test_prompt_manager}\")\n",
    "test_answer = test_prompt_manager.generate_answer(query, passages)\n",
    "print(f\"✓ Generated answer: '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873c1ef",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9b4c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation test: F1=66.67, P=66.67, R=66.67, EM=66.67\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer for comparison\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_token_metrics(prediction: str, ground_truth: str) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score for token-level comparison.\n",
    "    Returns: (precision, recall, f1)\n",
    "    \"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    # Handle empty cases\n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        match = int(pred_tokens == gt_tokens)\n",
    "        return match, match, match\n",
    "    \n",
    "    # Compute overlap\n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate_predictions(df_gold: pd.DataFrame, predictions: Dict[int, str]) -> Dict:\n",
    "    \"\"\"Evaluate predictions against ground truth.\"\"\"\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    exact_matches = []\n",
    "    \n",
    "    for _, row in df_gold.iterrows():\n",
    "        qid = row['id']\n",
    "        \n",
    "        # Handle missing predictions\n",
    "        if qid not in predictions:\n",
    "            f1_scores.append(0.0)\n",
    "            precision_scores.append(0.0)\n",
    "            recall_scores.append(0.0)\n",
    "            exact_matches.append(0)\n",
    "            continue\n",
    "        \n",
    "        prediction = predictions[qid]\n",
    "        ground_truths = row['answers']\n",
    "        \n",
    "        # Normalize once\n",
    "        norm_prediction = normalize_answer(prediction)\n",
    "        \n",
    "        # Find best match across all ground truths\n",
    "        best_f1 = 0.0\n",
    "        best_precision = 0.0\n",
    "        best_recall = 0.0\n",
    "        is_exact = 0\n",
    "        \n",
    "        for gt in ground_truths:\n",
    "            norm_gt = normalize_answer(gt)\n",
    "            \n",
    "            # Compute metrics\n",
    "            prec, rec, f1 = compute_token_metrics(prediction, gt)\n",
    "            \n",
    "            # Track best scores\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_precision = prec\n",
    "                best_recall = rec\n",
    "            \n",
    "            # Check exact match\n",
    "            if norm_prediction == norm_gt:\n",
    "                is_exact = 1\n",
    "        \n",
    "        f1_scores.append(best_f1)\n",
    "        precision_scores.append(best_precision)\n",
    "        recall_scores.append(best_recall)\n",
    "        exact_matches.append(is_exact)\n",
    "    \n",
    "    return {\n",
    "        'f1': 100.0 * sum(f1_scores) / len(f1_scores) if f1_scores else 0.0,\n",
    "        'precision': 100.0 * sum(precision_scores) / len(precision_scores) if precision_scores else 0.0,\n",
    "        'recall': 100.0 * sum(recall_scores) / len(recall_scores) if recall_scores else 0.0,\n",
    "        'exact_match': 100.0 * sum(exact_matches) / len(exact_matches) if exact_matches else 0.0,\n",
    "        'f1_scores': f1_scores,\n",
    "        'precision_scores': precision_scores,\n",
    "        'recall_scores': recall_scores,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "test_predictions = {1: \"J.K. Rowling\", 2: \"Paris\", 3: \"Shakespeare\"}\n",
    "test_gold = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'answers': [[\"J.K. Rowling\", \"Rowling\"], [\"Earth\"], [\"William Shakespeare\", \"Shakespeare\"]]\n",
    "})\n",
    "\n",
    "test_metrics = evaluate_predictions(test_gold, test_predictions)\n",
    "print(f\"✓ Evaluation test: F1={test_metrics['f1']:.2f}, P={test_metrics['precision']:.2f}, R={test_metrics['recall']:.2f}, EM={test_metrics['exact_match']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956e446",
   "metadata": {},
   "source": [
    "## 6. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "721292c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing experiment with:\n",
      "  Retrieval: RetrievalManager(k_docs=100, k_passages=7, RRF_k=60, window=150, overlap=50)\n",
      "  Prompt: temp=0.0, top_p=1.0, max_tokens=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quick Test: 100%|██████████| 25/25 [00:46<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick Test\n",
      "   Retrieval: RetrievalManager(k_docs=100, k_passages=7, RRF_k=60, window=150, overlap=50)\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=64\n",
      "   F1=30.33 | P=36.00 | R=27.73 | EM=20.00\n",
      "   Questions: 25\n",
      "\n",
      "✓ Experiment framework ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    name: str,\n",
    "    df_data: pd.DataFrame,\n",
    "    retrieval_manager: RetrievalManager,\n",
    "    prompt_manager: PromptManager,\n",
    "    max_questions: Optional[int] = None,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run a full experiment: retrieval + prompting + evaluation.\n",
    "    Args:\n",
    "        name: Name of the experiment.\n",
    "        df_data: DataFrame with questions and answers.\n",
    "        retrieval_manager: RetrievalManager instance.\n",
    "        prompt_manager: PromptManager instance.\n",
    "        max_questions: Optional limit on number of questions to process.\n",
    "        verbose: Whether to print progress and results.\n",
    "    \"\"\"\n",
    "    if max_questions:\n",
    "        df_data = df_data.head(max_questions)\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    iterator = tqdm(df_data.iterrows(), total=len(df_data), desc=name) if verbose else df_data.iterrows()\n",
    "\n",
    "    for _, row in iterator:\n",
    "        question = row['question']\n",
    "        qid = row['id']\n",
    "\n",
    "        contexts = retrieval_manager.retrieve_context(question)\n",
    "        answer = prompt_manager.generate_answer(question, contexts)\n",
    "\n",
    "        predictions[qid] = answer\n",
    "\n",
    "    metrics = evaluate_predictions(df_data, predictions)\n",
    "\n",
    "    result = {\n",
    "        'name': name,\n",
    "        'retrieval': retrieval_manager,\n",
    "        'prompt': prompt_manager,\n",
    "        'f1_score': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'exact_match': metrics['exact_match'],\n",
    "        'num_questions': len(df_data),\n",
    "        'predictions': predictions,\n",
    "        'f1_scores': metrics['f1_scores'],\n",
    "        'precision_scores': metrics['precision_scores'],\n",
    "        'recall_scores': metrics['recall_scores'],\n",
    "        'exact_matches': metrics['exact_matches']\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"   Retrieval: {retrieval_manager}\")\n",
    "        print(f\"   Prompt: {prompt_manager}\")\n",
    "        print(\n",
    "            f\"   F1={metrics['f1']:.2f} | \"\n",
    "            f\"P={metrics['precision']:.2f} | \"\n",
    "            f\"R={metrics['recall']:.2f} | \"\n",
    "            f\"EM={metrics['exact_match']:.2f}\"\n",
    "        )\n",
    "        print(f\"   Questions: {len(df_data)}\\n\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "test_retrieval = RetrievalManager()\n",
    "test_prompt = PromptManager()\n",
    "print(f\"Testing experiment with:\")\n",
    "print(f\"  Retrieval: {test_retrieval}\")\n",
    "print(f\"  Prompt: {test_prompt}\")\n",
    "\n",
    "test_exp = run_experiment(\n",
    "    \"Quick Test\",\n",
    "    df_train.head(25),\n",
    "    test_retrieval,\n",
    "    test_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Experiment framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78eead",
   "metadata": {},
   "source": [
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54f379",
   "metadata": {},
   "source": [
    "### Experiments global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac8c7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_SEED = 42\n",
    "EXPERIMENTS_NUM_QUESTIONS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9a89e",
   "metadata": {},
   "source": [
    "### Experiments utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4d8b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_log_path(num_questions: int) -> str:\n",
    "    \"\"\"Get path for experiment log CSV based on number of questions.\"\"\"\n",
    "    return f\"./results/grid_search_results_q{num_questions}.csv\"\n",
    "\n",
    "\n",
    "def generate_config_key(\n",
    "    retrieval_mgr: RetrievalManager,\n",
    "    prompt_mgr: PromptManager,\n",
    ") -> str:\n",
    "    \"\"\"Generate unique config key for RRF-based retrieval.\"\"\"\n",
    "    return (\n",
    "        f\"RRF_k{retrieval_mgr.rrf_k}_\"\n",
    "        f\"mu{retrieval_mgr.mu}_\"\n",
    "        f\"k1{retrieval_mgr.k1}_b{retrieval_mgr.b}_\"\n",
    "        f\"kdocs{retrieval_mgr.k_docs}_\"\n",
    "        f\"kpass{retrieval_mgr.k_passages}_\"\n",
    "        f\"win{retrieval_mgr.window}_ovl{retrieval_mgr.overlap}\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "def build_retrieval_manager(base: dict, override: dict) -> RetrievalManager:\n",
    "    \"\"\"Build RetrievalManager safely.\"\"\"\n",
    "    return RetrievalManager(**{**base, **override})\n",
    "\n",
    "\n",
    "def save_results_to_csv(result: dict, key: str, path: str):\n",
    "    \"\"\"Save experiment results to a CSV file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    row = {\n",
    "        \"config_key\": key,\n",
    "        \"f1\": result[\"f1_score\"],\n",
    "        \"precision\": result[\"precision\"],\n",
    "        \"recall\": result[\"recall\"],\n",
    "        \"exact_match\": result[\"exact_match\"],\n",
    "        \"num_questions\": result[\"num_questions\"],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([row])\n",
    "    if not os.path.exists(path):\n",
    "        df.to_csv(path, index=False)\n",
    "    else:\n",
    "        df.to_csv(path, mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "def load_completed_configs(path: str) -> set[str]:\n",
    "    \"\"\"Load set of completed experiment config keys from CSV.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    return set(pd.read_csv(path)[\"config_key\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc59d56",
   "metadata": {},
   "source": [
    "### Print utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03274a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grid_results_table(grid: list[dict], *, num_questions: int):\n",
    "    \"\"\"Print results table for a given experiment grid.\"\"\"\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "    path = get_experiment_log_path(num_questions)\n",
    "    if not os.path.exists(path):\n",
    "        print(\"No results file found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    keys = [\n",
    "        generate_config_key(g[\"retrieval_mgr\"], g[\"prompt_mgr\"])\n",
    "        for g in grid\n",
    "    ]\n",
    "\n",
    "    grid_df = (\n",
    "        df[df[\"config_key\"].isin(keys)]\n",
    "        .sort_values(\"f1\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if grid_df.empty:\n",
    "        print(\"No completed configs found for this grid.\")\n",
    "        return\n",
    "\n",
    "    display_cols = [\n",
    "        \"config_key\",\n",
    "        \"f1\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"exact_match\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\nGrid results (sorted by F1):\")\n",
    "    display(grid_df[display_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb074e07",
   "metadata": {},
   "source": [
    "### Best-config selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "987015b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k_configs(\n",
    "    retrieval_managers: list[RetrievalManager],\n",
    "    prompt_managers: list[PromptManager],\n",
    "    *,\n",
    "    num_questions: int,\n",
    "    top_k: int = 5,\n",
    "):\n",
    "    \"\"\"Select top-k configurations from experiment results.\"\"\"\n",
    "    path = get_experiment_log_path(num_questions)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    scored_entries = []\n",
    "\n",
    "    for r_mgr, p_mgr in zip(retrieval_managers, prompt_managers):\n",
    "        key = generate_config_key(r_mgr, p_mgr)\n",
    "        row = df[df[\"config_key\"] == key]\n",
    "        if row.empty:\n",
    "            continue\n",
    "\n",
    "        scored_entries.append({\n",
    "            \"retrieval_mgr\": r_mgr,\n",
    "            \"prompt_mgr\": p_mgr,\n",
    "            \"f1\": float(row.iloc[0][\"f1\"]),\n",
    "            \"config_key\": key,\n",
    "        })\n",
    "\n",
    "    scored_entries.sort(\n",
    "        key=lambda x: (x[\"f1\"], x[\"config_key\"]),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    return scored_entries[:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f416f",
   "metadata": {},
   "source": [
    "### Phase runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "827db996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phase(\n",
    "    *,\n",
    "    phase_name: str,\n",
    "    grid: list[dict],\n",
    "    df_train: pd.DataFrame,\n",
    "    num_questions: int,\n",
    "    top_k: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a phase of experiments over a grid of configurations.\n",
    "    Args:\n",
    "        phase_name: Name of the experiment phase.\n",
    "        grid: List of configurations (dicts with 'retrieval_mgr' and 'prompt_mgr').\n",
    "        df_train: DataFrame with training questions and answers.\n",
    "        num_questions: Number of questions to use per configuration.\n",
    "        top_k: If specified, return only the top-k configurations after running.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(phase_name)\n",
    "    print(f\"Questions per config: {num_questions}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    path = get_experiment_log_path(num_questions)\n",
    "\n",
    "    validation_data = df_train.sample(\n",
    "        n=num_questions,\n",
    "        random_state=EXPERIMENT_SEED,\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    completed = load_completed_configs(path)\n",
    "\n",
    "    pending = [\n",
    "        g for g in grid\n",
    "        if generate_config_key(g[\"retrieval_mgr\"], g[\"prompt_mgr\"]) not in completed\n",
    "    ]\n",
    "\n",
    "    print(f\"Total configs: {len(grid)}\")\n",
    "    print(f\"Completed: {len(grid) - len(pending)}\")\n",
    "    print(f\"Pending: {len(pending)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, entry in enumerate(pending, start=1):\n",
    "        r_mgr = entry[\"retrieval_mgr\"]\n",
    "        p_mgr = entry[\"prompt_mgr\"]\n",
    "        key = generate_config_key(r_mgr, p_mgr)\n",
    "\n",
    "        print(f\"[{i}/{len(pending)}] Running: {key}\")\n",
    "\n",
    "        result = run_experiment(\n",
    "            name=key,\n",
    "            df_data=validation_data,\n",
    "            retrieval_manager=r_mgr,\n",
    "            prompt_manager=p_mgr,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        save_results_to_csv(result, key, path)\n",
    "        print(f\"✓ F1={result['f1_score']:.4f}\")\n",
    "\n",
    "    print_grid_results_table(grid, num_questions=num_questions)\n",
    "\n",
    "    if top_k is None:\n",
    "        return grid\n",
    "\n",
    "    return select_top_k_configs(\n",
    "        [g[\"retrieval_mgr\"] for g in grid],\n",
    "        [g[\"prompt_mgr\"] for g in grid],\n",
    "        num_questions=num_questions,\n",
    "        top_k=top_k,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205277ae",
   "metadata": {},
   "source": [
    "### Phase 1 - Capacity (k_docs / k_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03cdf5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1 — Retrieval Capacity\n",
      "Questions per config: 100\n",
      "================================================================================\n",
      "Total configs: 11\n",
      "Completed: 11\n",
      "Pending: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Grid results (sorted by F1):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs500_kpass10_win150_ovl50</td>\n",
       "      <td>33.289076</td>\n",
       "      <td>36.833333</td>\n",
       "      <td>32.933333</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50</td>\n",
       "      <td>30.414461</td>\n",
       "      <td>34.841782</td>\n",
       "      <td>30.433333</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass10_win150_ovl50</td>\n",
       "      <td>27.426702</td>\n",
       "      <td>31.623462</td>\n",
       "      <td>28.850000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs250_kpass10_win150_ovl50</td>\n",
       "      <td>26.462324</td>\n",
       "      <td>30.593452</td>\n",
       "      <td>26.742857</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass7_win150_ovl50</td>\n",
       "      <td>26.452391</td>\n",
       "      <td>28.940591</td>\n",
       "      <td>28.350000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass5_win150_ovl50</td>\n",
       "      <td>26.335129</td>\n",
       "      <td>29.825000</td>\n",
       "      <td>26.850000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass5_win150_ovl50</td>\n",
       "      <td>26.032440</td>\n",
       "      <td>29.429798</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win150_ovl50</td>\n",
       "      <td>25.007879</td>\n",
       "      <td>27.016667</td>\n",
       "      <td>26.492857</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass7_win150_ovl50</td>\n",
       "      <td>24.125595</td>\n",
       "      <td>27.998925</td>\n",
       "      <td>25.433333</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass7_win150_ovl50</td>\n",
       "      <td>22.550469</td>\n",
       "      <td>24.627778</td>\n",
       "      <td>24.266667</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass10_win150_ovl50</td>\n",
       "      <td>22.338095</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>22.350000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 config_key         f1  \\\n",
       "0   RRF_k60_mu1000_k10.9_b0.4_kdocs500_kpass10_win150_ovl50  33.289076   \n",
       "1    RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50  30.414461   \n",
       "2   RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass10_win150_ovl50  27.426702   \n",
       "3   RRF_k60_mu1000_k10.9_b0.4_kdocs250_kpass10_win150_ovl50  26.462324   \n",
       "4    RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass7_win150_ovl50  26.452391   \n",
       "5    RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass5_win150_ovl50  26.335129   \n",
       "6     RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass5_win150_ovl50  26.032440   \n",
       "7     RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win150_ovl50  25.007879   \n",
       "8     RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass7_win150_ovl50  24.125595   \n",
       "9     RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass7_win150_ovl50  22.550469   \n",
       "10   RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass10_win150_ovl50  22.338095   \n",
       "\n",
       "    precision     recall  exact_match  \n",
       "0   36.833333  32.933333         21.0  \n",
       "1   34.841782  30.433333         17.0  \n",
       "2   31.623462  28.850000         14.0  \n",
       "3   30.593452  26.742857         14.0  \n",
       "4   28.940591  28.350000         14.0  \n",
       "5   29.825000  26.850000         14.0  \n",
       "6   29.429798  27.600000         13.0  \n",
       "7   27.016667  26.492857         13.0  \n",
       "8   27.998925  25.433333         11.0  \n",
       "9   24.627778  24.266667         12.0  \n",
       "10  24.250000  22.350000         14.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PHASE_1_GRID = []\n",
    "\n",
    "BASE_RETRIEVAL_PARAMS = {\n",
    "    \"window\": 150,\n",
    "    \"overlap\": 50,\n",
    "    \"mu\": 1000,\n",
    "    \"k1\": 0.9,\n",
    "    \"b\": 0.4,\n",
    "}\n",
    "\n",
    "CAPACITY_PAIRS = [\n",
    "    (10, 5), (10, 7), (10, 10),\n",
    "    (50, 5), (50, 7), (50, 10),\n",
    "    (100, 5), (100, 7), (100, 10),\n",
    "    (250, 10),\n",
    "    (500, 10),\n",
    "]\n",
    "\n",
    "for k_docs, k_passages in CAPACITY_PAIRS:\n",
    "    PHASE_1_GRID.append({\n",
    "        \"retrieval_mgr\": RetrievalManager(\n",
    "            k_docs=k_docs,\n",
    "            k_passages=k_passages,\n",
    "            **BASE_RETRIEVAL_PARAMS,\n",
    "        ),\n",
    "        \"prompt_mgr\": PromptManager(),\n",
    "    })\n",
    "\n",
    "PHASE_1_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 1 — Retrieval Capacity\",\n",
    "    grid=PHASE_1_GRID,\n",
    "    df_train=df_train,\n",
    "    num_questions=EXPERIMENTS_NUM_QUESTIONS,\n",
    "    top_k=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30605bd4",
   "metadata": {},
   "source": [
    "### Phase 2 - Passage segmentation (window / overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e95ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2 — Passage Segmentation\n",
      "Questions per config: 100\n",
      "================================================================================\n",
      "Total configs: 12\n",
      "Completed: 9\n",
      "Pending: 3\n",
      "--------------------------------------------------------------------------------\n",
      "[1/3] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs500_kpass10_win100_ovl30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs500_kpass10_win100_ovl30:   6%|▌         | 6/100 [00:29<07:49,  5.00s/it]"
     ]
    }
   ],
   "source": [
    "PHASE_2_GRID = []\n",
    "\n",
    "WINDOW_OVERLAP_PAIRS = [\n",
    "    (100, 30),\n",
    "    (150, 50),  # baseline\n",
    "    (200, 50),\n",
    "    (250, 60),\n",
    "]\n",
    "\n",
    "for entry in PHASE_1_TOP_CONFIGS:\n",
    "    base = entry[\"retrieval_mgr\"]\n",
    "    for w, o in WINDOW_OVERLAP_PAIRS:\n",
    "        PHASE_2_GRID.append({\n",
    "            \"retrieval_mgr\": RetrievalManager(\n",
    "                k_docs=base.k_docs,\n",
    "                k_passages=base.k_passages,\n",
    "                window=w,\n",
    "                overlap=o,\n",
    "                mu=base.mu,\n",
    "                k1=base.k1,\n",
    "                b=base.b,\n",
    "            ),\n",
    "            \"prompt_mgr\": PromptManager(),\n",
    "        })\n",
    "\n",
    "PHASE_2_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 2 — Passage Segmentation\",\n",
    "    grid=PHASE_2_GRID,\n",
    "    df_train=df_train,\n",
    "    num_questions=EXPERIMENTS_NUM_QUESTIONS,\n",
    "    top_k=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc13a845",
   "metadata": {},
   "source": [
    "### Phase 3 - BM25 / QLD parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ad0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3 — Lexical Hyperparameters\n",
      "Questions per config: 100\n",
      "================================================================================\n",
      "Total configs: 12\n",
      "Completed: 9\n",
      "Pending: 3\n",
      "--------------------------------------------------------------------------------\n",
      "[1/3] Running: RRF_k60_mu2000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu2000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptdefault: 100%|██████████| 100/100 [02:21<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu2000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptdefault\n",
      "   Retrieval: RetrievalManager(k_docs=50, k_passages=10, RRF_k=60, window=150, overlap=50)\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=29.07 | P=33.00 | R=29.10 | EM=14.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=29.0681\n",
      "[2/3] Running: RRF_k60_mu1000_k11.2_b0.6_kdocs50_kpass10_win150_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k11.2_b0.6_kdocs50_kpass10_win150_ovl50_promptdefault: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k11.2_b0.6_kdocs50_kpass10_win150_ovl50_promptdefault\n",
      "   Retrieval: RetrievalManager(k_docs=50, k_passages=10, RRF_k=60, window=150, overlap=50)\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=29.05 | P=32.74 | R=29.85 | EM=16.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=29.0526\n",
      "[3/3] Running: RRF_k60_mu2000_k11.2_b0.6_kdocs50_kpass10_win150_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu2000_k11.2_b0.6_kdocs50_kpass10_win150_ovl50_promptdefault: 100%|██████████| 100/100 [02:19<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu2000_k11.2_b0.6_kdocs50_kpass10_win150_ovl50_promptdefault\n",
      "   Retrieval: RetrievalManager(k_docs=50, k_passages=10, RRF_k=60, window=150, overlap=50)\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=28.47 | P=31.14 | R=29.27 | EM=16.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=28.4700\n",
      "\n",
      "Grid results (sorted by F1):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptdefault</td>\n",
       "      <td>32.588889</td>\n",
       "      <td>36.333333</td>\n",
       "      <td>31.200000</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RRF_k60_mu2000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptdefault</td>\n",
       "      <td>31.879365</td>\n",
       "      <td>35.450000</td>\n",
       "      <td>31.033333</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RRF_k60_mu1000_k10.6_b0.3_kdocs100_kpass7_win200_ovl50_promptdefault</td>\n",
       "      <td>31.077778</td>\n",
       "      <td>34.466129</td>\n",
       "      <td>31.366667</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptdefault</td>\n",
       "      <td>30.414461</td>\n",
       "      <td>34.841782</td>\n",
       "      <td>30.433333</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RRF_k60_mu1000_k11.2_b0.6_kdocs100_kpass7_win200_ovl50_promptdefault</td>\n",
       "      <td>29.172222</td>\n",
       "      <td>32.666667</td>\n",
       "      <td>27.766667</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RRF_k60_mu2000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptdefault</td>\n",
       "      <td>29.068056</td>\n",
       "      <td>32.998925</td>\n",
       "      <td>29.100000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RRF_k60_mu1000_k11.2_b0.6_kdocs50_kpass10_win150_ovl50_promptdefault</td>\n",
       "      <td>29.052556</td>\n",
       "      <td>32.741782</td>\n",
       "      <td>29.850000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RRF_k60_mu2000_k11.2_b0.6_kdocs100_kpass7_win200_ovl50_promptdefault</td>\n",
       "      <td>28.855556</td>\n",
       "      <td>32.366667</td>\n",
       "      <td>28.200000</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RRF_k60_mu2000_k11.2_b0.6_kdocs50_kpass10_win150_ovl50_promptdefault</td>\n",
       "      <td>28.470016</td>\n",
       "      <td>31.141782</td>\n",
       "      <td>29.266667</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RRF_k60_mu2000_k10.6_b0.3_kdocs50_kpass10_win150_ovl50_promptdefault</td>\n",
       "      <td>28.198958</td>\n",
       "      <td>32.007874</td>\n",
       "      <td>28.183333</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RRF_k60_mu1000_k10.6_b0.3_kdocs50_kpass10_win150_ovl50_promptdefault</td>\n",
       "      <td>28.100919</td>\n",
       "      <td>31.350731</td>\n",
       "      <td>29.100000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RRF_k60_mu2000_k10.6_b0.3_kdocs100_kpass7_win200_ovl50_promptdefault</td>\n",
       "      <td>26.911111</td>\n",
       "      <td>29.466129</td>\n",
       "      <td>27.533333</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              config_key  \\\n",
       "0   RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptdefault   \n",
       "1   RRF_k60_mu2000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptdefault   \n",
       "2   RRF_k60_mu1000_k10.6_b0.3_kdocs100_kpass7_win200_ovl50_promptdefault   \n",
       "3   RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptdefault   \n",
       "4   RRF_k60_mu1000_k11.2_b0.6_kdocs100_kpass7_win200_ovl50_promptdefault   \n",
       "5   RRF_k60_mu2000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptdefault   \n",
       "6   RRF_k60_mu1000_k11.2_b0.6_kdocs50_kpass10_win150_ovl50_promptdefault   \n",
       "7   RRF_k60_mu2000_k11.2_b0.6_kdocs100_kpass7_win200_ovl50_promptdefault   \n",
       "8   RRF_k60_mu2000_k11.2_b0.6_kdocs50_kpass10_win150_ovl50_promptdefault   \n",
       "9   RRF_k60_mu2000_k10.6_b0.3_kdocs50_kpass10_win150_ovl50_promptdefault   \n",
       "10  RRF_k60_mu1000_k10.6_b0.3_kdocs50_kpass10_win150_ovl50_promptdefault   \n",
       "11  RRF_k60_mu2000_k10.6_b0.3_kdocs100_kpass7_win200_ovl50_promptdefault   \n",
       "\n",
       "           f1  precision     recall  exact_match  \n",
       "0   32.588889  36.333333  31.200000         23.0  \n",
       "1   31.879365  35.450000  31.033333         22.0  \n",
       "2   31.077778  34.466129  31.366667         22.0  \n",
       "3   30.414461  34.841782  30.433333         17.0  \n",
       "4   29.172222  32.666667  27.766667         20.0  \n",
       "5   29.068056  32.998925  29.100000         14.0  \n",
       "6   29.052556  32.741782  29.850000         16.0  \n",
       "7   28.855556  32.366667  28.200000         20.0  \n",
       "8   28.470016  31.141782  29.266667         16.0  \n",
       "9   28.198958  32.007874  28.183333         15.0  \n",
       "10  28.100919  31.350731  29.100000         16.0  \n",
       "11  26.911111  29.466129  27.533333         19.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PHASE_3_GRID = []\n",
    "\n",
    "BM25_PARAMS = [\n",
    "    {\"k1\": 0.6, \"b\": 0.3},\n",
    "    {\"k1\": 0.9, \"b\": 0.4},  # baseline\n",
    "    {\"k1\": 1.2, \"b\": 0.6},\n",
    "]\n",
    "\n",
    "QLD_PARAMS = [\n",
    "    {\"mu\": 1000},   # baseline\n",
    "    {\"mu\": 2000},\n",
    "]\n",
    "\n",
    "for entry in PHASE_2_TOP_CONFIGS:\n",
    "    base = entry[\"retrieval_mgr\"]\n",
    "    for bm25 in BM25_PARAMS:\n",
    "        for qld in QLD_PARAMS:\n",
    "            PHASE_3_GRID.append({\n",
    "                \"retrieval_mgr\": RetrievalManager(\n",
    "                    k_docs=base.k_docs,\n",
    "                    k_passages=base.k_passages,\n",
    "                    window=base.window,\n",
    "                    overlap=base.overlap,\n",
    "                    k1=bm25[\"k1\"],\n",
    "                    b=bm25[\"b\"],\n",
    "                    mu=qld[\"mu\"],\n",
    "                ),\n",
    "                \"prompt_mgr\": PromptManager(),\n",
    "            })\n",
    "\n",
    "PHASE_3_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 3 — Lexical Hyperparameters\",\n",
    "    grid=PHASE_3_GRID,\n",
    "    df_train=df_train,\n",
    "    num_questions=EXPERIMENTS_NUM_QUESTIONS,\n",
    "    top_k=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544d5ca",
   "metadata": {},
   "source": [
    "### Phase 4 - Best configs comparison over more questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a904e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 (100q): 32.5889\n",
      "Promotion threshold: 30.9594 (95% of best)\n",
      "\n",
      "Promoted configs: 3\n",
      "1. RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptdefault | F1=32.5889\n",
      "2. RRF_k60_mu2000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptdefault | F1=31.8794\n",
      "3. RRF_k60_mu1000_k10.6_b0.3_kdocs100_kpass7_win200_ovl50_promptdefault | F1=31.0778\n"
     ]
    }
   ],
   "source": [
    "PHASE_4_F1_RATIO = 0.95          # promote configs within 95% of best\n",
    "PHASE_4_MAX = 10                 # safety cap\n",
    "PHASE_4_NUM_QUESTIONS = 300\n",
    "\n",
    "experiments_path = get_experiment_log_path(\n",
    "    num_questions=EXPERIMENTS_NUM_QUESTIONS\n",
    ")\n",
    "experiments_df = pd.read_csv(experiments_path)\n",
    "\n",
    "assert not experiments_df.empty, \"No results found for experiments\"\n",
    "\n",
    "# Sort globally by F1\n",
    "experiments_df = experiments_df.sort_values(\n",
    "    \"f1\", ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "best_f1 = experiments_df.iloc[0][\"f1\"]\n",
    "threshold_f1 = best_f1 * PHASE_4_F1_RATIO\n",
    "\n",
    "print(f\"Best F1 ({EXPERIMENTS_NUM_QUESTIONS}q): {best_f1:.4f}\")\n",
    "print(f\"Promotion threshold: {threshold_f1:.4f} \"\n",
    "      f\"({PHASE_4_F1_RATIO:.0%} of best)\")\n",
    "\n",
    "# Promote configs close by ratio\n",
    "promoted_df = (\n",
    "    experiments_df[experiments_df[\"f1\"] >= threshold_f1]\n",
    "    .head(PHASE_4_MAX)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"\\nPromoted configs: {len(promoted_df)}\")\n",
    "for i, row in promoted_df.iterrows():\n",
    "    print(\n",
    "        f\"{i+1}. {row['config_key']} | \"\n",
    "        f\"F1={row['f1']:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad7b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Phase 4 grid size: 3\n",
      "\n",
      "================================================================================\n",
      "PHASE 4 — Stabilized Comparison\n",
      "Questions per config: 300\n",
      "================================================================================\n",
      "Total configs: 3\n",
      "Completed: 0\n",
      "Pending: 3\n",
      "--------------------------------------------------------------------------------\n",
      "[1/3] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptphase4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptphase4: 100%|██████████| 300/300 [13:33<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptphase4\n",
      "   Retrieval: RetrievalManager(k_docs=100, k_passages=7, RRF_k=60, window=200, overlap=50)\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=29.59 | P=32.21 | R=30.07 | EM=19.33\n",
      "   Questions: 300\n",
      "\n",
      "✓ F1=29.5872\n",
      "[2/3] Running: RRF_k60_mu2000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptphase4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu2000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptphase4: 100%|██████████| 300/300 [13:36<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu2000_k10.9_b0.4_kdocs100_kpass7_win200_ovl50_promptphase4\n",
      "   Retrieval: RetrievalManager(k_docs=100, k_passages=7, RRF_k=60, window=200, overlap=50)\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=28.61 | P=31.13 | R=29.07 | EM=19.00\n",
      "   Questions: 300\n",
      "\n",
      "✓ F1=28.6111\n",
      "[3/3] Running: RRF_k60_mu1000_k10.6_b0.3_kdocs100_kpass7_win200_ovl50_promptphase4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.6_b0.3_kdocs100_kpass7_win200_ovl50_promptphase4:   1%|▏         | 4/300 [00:12<15:12,  3.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m PHASE_4_GRID \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     {\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieval_mgr\u001b[39m\u001b[38;5;124m\"\u001b[39m: ALL_CONFIGS[key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieval_mgr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m ALL_CONFIGS\n\u001b[1;32m     19\u001b[0m ]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Phase 4 grid size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(PHASE_4_GRID)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m PHASE_4_RESULTS \u001b[38;5;241m=\u001b[39m \u001b[43mrun_phase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPHASE 4 — Stabilized Comparison\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPHASE_4_GRID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPHASE_4_NUM_QUESTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 49\u001b[0m, in \u001b[0;36mrun_phase\u001b[0;34m(phase_name, grid, df_train, num_questions, top_k)\u001b[0m\n\u001b[1;32m     45\u001b[0m key \u001b[38;5;241m=\u001b[39m generate_config_key(r_mgr, p_mgr)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pending)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrieval_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr_mgr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_mgr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m save_results_to_csv(result, key, path)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 31\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(name, df_data, retrieval_manager, prompt_manager, max_questions, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m     qid \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m     contexts \u001b[38;5;241m=\u001b[39m retrieval_manager\u001b[38;5;241m.\u001b[39mretrieve_context(question)\n\u001b[0;32m---> 31\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     predictions[qid] \u001b[38;5;241m=\u001b[39m answer\n\u001b[1;32m     35\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_predictions(df_data, predictions)\n",
      "Cell \u001b[0;32mIn[17], line 66\u001b[0m, in \u001b[0;36mPromptManager.generate_answer\u001b[0;34m(self, question, contexts)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate an answer using the LLM based on the question and contexts.\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_messages(question, contexts)\n\u001b[0;32m---> 66\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m answer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_answer(answer)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:325\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m         chats \u001b[38;5;241m=\u001b[39m (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# 🐈 🐈 🐈\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1467\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1461\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1464\u001b[0m         )\n\u001b[1;32m   1465\u001b[0m     )\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1474\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1473\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1474\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1374\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1373\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1374\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:432\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    430\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 432\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    435\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2779\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2777\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 2779\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2780\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   2781\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   2595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2596\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Collect all unique configs by config_key\n",
    "ALL_CONFIGS = {}\n",
    "\n",
    "for entry in PHASE_1_GRID + PHASE_2_GRID + PHASE_3_GRID:\n",
    "    key = generate_config_key(\n",
    "        entry[\"retrieval_mgr\"],\n",
    "        entry[\"prompt_mgr\"],\n",
    "    )\n",
    "    ALL_CONFIGS[key] = entry   # dedupe by key\n",
    "\n",
    "# Build Phase 4 grid directly from promoted config_keys\n",
    "PHASE_4_GRID = [\n",
    "    {\n",
    "        \"retrieval_mgr\": ALL_CONFIGS[key][\"retrieval_mgr\"],\n",
    "        \"prompt_mgr\": PromptManager(),\n",
    "    }\n",
    "    for key in promoted_df[\"config_key\"].values\n",
    "    if key in ALL_CONFIGS\n",
    "]\n",
    "\n",
    "print(f\"\\n✓ Phase 4 grid size: {len(PHASE_4_GRID)}\")\n",
    "\n",
    "PHASE_4_RESULTS = run_phase(\n",
    "    phase_name=\"PHASE 4 — Stabilized Comparison\",\n",
    "    grid=PHASE_4_GRID,\n",
    "    df_train=df_train,\n",
    "    num_questions=PHASE_4_NUM_QUESTIONS,\n",
    "    top_k=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd538c4b",
   "metadata": {},
   "source": [
    "### Phase 5 - Final comparison between 2 best configs, over 1000 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5075fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 configs from Phase 4:\n",
      "1. RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass10_win150_ovl50_promptphase4 | F1=24.0822\n",
      "2. RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptphase4 | F1=24.0822\n",
      "\n",
      "✓ Phase 5 grid size: 2\n",
      "\n",
      "================================================================================\n",
      "PHASE 5 — Final Tie-Breaker\n",
      "Questions per config: 1000\n",
      "================================================================================\n",
      "Total configs: 2\n",
      "Completed: 0\n",
      "Pending: 2\n",
      "--------------------------------------------------------------------------------\n",
      "[1/2] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass10_win150_ovl50_promptphase5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass10_win150_ovl50_promptphase5:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass10_win150_ovl50_promptphase5: 100%|██████████| 1000/1000 [14:40<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass10_win150_ovl50_promptphase5\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=100, k_passages=10 | window=150, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=25.91 | P=27.89 | R=27.07 | EM=16.30\n",
      "   Questions: 1000\n",
      "\n",
      "✓ F1=25.9145\n",
      "[2/2] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptphase5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptphase5: 100%|██████████| 1000/1000 [13:55<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptphase5\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=50, k_passages=10 | window=150, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=25.96 | P=27.94 | R=27.12 | EM=16.40\n",
      "   Questions: 1000\n",
      "\n",
      "✓ F1=25.9645\n",
      "\n",
      "Grid results (sorted by F1):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptphase5</td>\n",
       "      <td>25.964506</td>\n",
       "      <td>27.944222</td>\n",
       "      <td>27.124597</td>\n",
       "      <td>16.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass10_win150_ovl50_promptphase5</td>\n",
       "      <td>25.914506</td>\n",
       "      <td>27.894222</td>\n",
       "      <td>27.074597</td>\n",
       "      <td>16.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             config_key  \\\n",
       "0   RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptphase5   \n",
       "1  RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass10_win150_ovl50_promptphase5   \n",
       "\n",
       "          f1  precision     recall  exact_match  \n",
       "0  25.964506  27.944222  27.124597         16.4  \n",
       "1  25.914506  27.894222  27.074597         16.3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PHASE_5_NUM_QUESTIONS = 1000\n",
    "PHASE_5_TOP_K = 2\n",
    "\n",
    "phase_4_path = get_experiment_log_path(num_questions=PHASE_4_NUM_QUESTIONS)\n",
    "df_phase_4 = pd.read_csv(phase_4_path)\n",
    "\n",
    "assert not df_phase_4.empty, \"No Phase 4 results found\"\n",
    "\n",
    "df_phase_4 = df_phase_4.sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "top_3_keys = df_phase_4.head(PHASE_5_TOP_K)[\"config_key\"].tolist()\n",
    "\n",
    "print(\"Top 3 configs from Phase 4:\")\n",
    "for i, row in df_phase_4.head(PHASE_5_TOP_K).iterrows():\n",
    "    print(f\"{i+1}. {row['config_key']} | F1={row['f1']:.4f}\")\n",
    "\n",
    "\n",
    "PHASE_5_GRID = [\n",
    "    {\n",
    "        \"retrieval_mgr\": e[\"retrieval_mgr\"],\n",
    "        \"prompt_mgr\": PromptManager(),\n",
    "    }\n",
    "    for e in PHASE_4_GRID\n",
    "    if generate_config_key(e[\"retrieval_mgr\"], e[\"prompt_mgr\"]) in top_3_keys\n",
    "]\n",
    "\n",
    "print(f\"\\n✓ Phase 5 grid size: {len(PHASE_5_GRID)}\")\n",
    "\n",
    "PHASE_5_RESULTS = run_phase(\n",
    "    phase_name=\"PHASE 5 — Final Tie-Breaker\",\n",
    "    grid=PHASE_5_GRID,\n",
    "    df_train=df_train,\n",
    "    num_questions=PHASE_5_NUM_QUESTIONS,\n",
    "    top_k=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efafd3a",
   "metadata": {},
   "source": [
    "## Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43a237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KAGGLE SUBMISSION — FINAL SYSTEM SELECTION (PHASE 5)\n",
      "================================================================================\n",
      "Selected best config from Phase 5:\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs50_kpass10_win150_ovl50_promptphase5 | F1=25.9645\n",
      "\n",
      "Generating answers for test set...\n",
      "Processed 50/2032 questions\n",
      "Processed 100/2032 questions\n",
      "Processed 150/2032 questions\n",
      "Processed 200/2032 questions\n",
      "Processed 250/2032 questions\n"
     ]
    }
   ],
   "source": [
    "phase_5_path = get_experiment_log_path(num_questions=PHASE_5_NUM_QUESTIONS)\n",
    "df_phase_5 = pd.read_csv(phase_5_path)\n",
    "\n",
    "assert not df_phase_5.empty, \"No Phase 5 results found\"\n",
    "\n",
    "df_phase_5 = df_phase_5.sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "best_row = df_phase_5.iloc[0]\n",
    "BEST_CONFIG_KEY = best_row[\"config_key\"]\n",
    "\n",
    "print(\"Selected best config from Phase 5:\")\n",
    "print(f\"{BEST_CONFIG_KEY} | F1={best_row['f1']:.4f}\")\n",
    "\n",
    "best_entry = next(\n",
    "    e for e in PHASE_5_GRID\n",
    "    if generate_config_key(e[\"retrieval_mgr\"], e[\"prompt_mgr\"]) == BEST_CONFIG_KEY\n",
    ")\n",
    "\n",
    "BEST_RETRIEVAL_MGR = best_entry[\"retrieval_mgr\"]\n",
    "BEST_PROMPT_MGR = best_entry[\"prompt_mgr\"]\n",
    "\n",
    "print(\"\\nGenerating answers for test set...\")\n",
    "\n",
    "test_questions = df_test[\"question\"].tolist()\n",
    "test_ids = df_test[\"id\"].tolist()\n",
    "\n",
    "answers = []\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processed {i}/{len(test_questions)} questions\")\n",
    "\n",
    "    contexts = BEST_RETRIEVAL_MGR.retrieve_context(question)\n",
    "    answer = BEST_PROMPT_MGR.generate_answer(question, contexts)\n",
    "    answers.append(answer)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"answer\": answers,\n",
    "})\n",
    "\n",
    "SUBMISSION_PATH = \"./results/kaggle_submission.csv\"\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Kaggle submission saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"✓ Total rows: {len(submission_df)}\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
