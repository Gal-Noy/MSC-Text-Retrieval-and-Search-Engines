{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80249d88",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "660a3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Literal\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79df6",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38133d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b4cf0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"21.0.9\" 2025-10-21\n",
      "OpenJDK Runtime Environment (build 21.0.9+10-Ubuntu-122.04)\n",
      "OpenJDK 64-Bit Server VM (build 21.0.9+10-Ubuntu-122.04, mixed mode, sharing)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4783a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff9235",
   "metadata": {},
   "source": [
    "### Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37f1afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged into Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(os.getenv('HUGGING_FACE_TOKEN'))\n",
    "print(\"✓ Logged into Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a17cc",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb3486b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3778 questions\n",
      "Test set: 2032 questions\n",
      "\n",
      "Sample question: what is the name of justin bieber brother?\n",
      "Sample answers: ['Jazmyn Bieber', 'Jaxon Bieber']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(\"./data/train.csv\", converters={\"answers\": json.loads})\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(f\"Train set: {len(df_train)} questions\")\n",
    "print(f\"Test set: {len(df_test)} questions\")\n",
    "print(f\"\\nSample question: {df_train.iloc[0]['question']}\")\n",
    "print(f\"Sample answers: {df_train.iloc[0]['answers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14492e1",
   "metadata": {},
   "source": [
    "## 3. Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ef07078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pyserini index...\n",
      "SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n",
      "✓ Index loaded: 5903530 documents\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "# Load Pyserini index\n",
    "print(\"Loading Pyserini index...\")\n",
    "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "\n",
    "print(f\"✓ Index loaded: {index_reader.stats()['documents']} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d922144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bi-encoder...\n",
      "✓ Bi-encoder loaded\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load bi-encoder\n",
    "print(\"Loading bi-encoder...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bi_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "print(\"✓ Bi-encoder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0537dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLD(mu=1000) → NoRerank | k_docs=5, k_passages=3\n",
      "1. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "2. June 1997, the books have found immense popularity, critical acclaim and commercial success worldwid...\n",
      "3. English by two major publishers, Bloomsbury in the United Kingdom and Scholastic Press in the United...\n",
      "\n",
      "QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "1. the film, has denied that Rowling ever saw it before writing her book. Rowling has said on record mu...\n",
      "2. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "3. by Emily Brontë, \"Charlie and the Chocolate Factory\" by Roald Dahl, \"Robinson Crusoe\" by Daniel Defo...\n",
      "\n",
      "BM25(k1=0.9, b=0.4) → NoRerank | k_docs=5, k_passages=3\n",
      "1. Bonnie Wright Bonnie Francesca Wright (born 17 February 1991) is an English actress, film director, ...\n",
      "2. the Deathly Hallows – Part 1\" and \"Part 2\", she began attending London's University of the Arts: Lon...\n",
      "3. older brothers board the Hogwarts express. Her role became much more prominent in the second film, \"...\n",
      "\n",
      "BM25(k1=0.9, b=0.4) → BiEncoder | k_docs=5, k_passages=3\n",
      "1. to resolve an ongoing feud between the organisation's northern and southern branches that had sapped...\n",
      "2. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "3. children thought it was \"cool that their hero, Harry, was encountering some of the creatures of myth...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Literal\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievalManager:\n",
    "    \"\"\"\n",
    "    Manages passage-based retrieval with optional bi-encoder reranking.\n",
    "    \"\"\"\n",
    "    # Configuration parameters\n",
    "    k_docs: int = 5\n",
    "    k_passages: int = 3\n",
    "    method: Literal[\"bm25\", \"qld\"] = \"qld\"\n",
    "    use_rerank: bool = True\n",
    "\n",
    "    mu: int = 1000          # QLD smoothing\n",
    "    k1: float = 0.9         # BM25\n",
    "    b: float = 0.4          # BM25\n",
    "\n",
    "    window: int = 150\n",
    "    overlap: int = 50\n",
    "    min_passage_words: int = 30\n",
    "\n",
    "    def __str__(self):\n",
    "        method_str = (\n",
    "            f\"QLD(mu={self.mu})\"\n",
    "            if self.method == \"qld\"\n",
    "            else f\"BM25(k1={self.k1}, b={self.b})\"\n",
    "        )\n",
    "        rerank_str = \"BiEncoder\" if self.use_rerank else \"NoRerank\"\n",
    "        return (\n",
    "            f\"{method_str} → {rerank_str} | \"\n",
    "            f\"k_docs={self.k_docs}, k_passages={self.k_passages}\"\n",
    "        )\n",
    "\n",
    "    def extract_passages(\n",
    "        self,\n",
    "        text: str,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into overlapping word windows.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        words = text.split()\n",
    "        if len(words) < self.min_passage_words:\n",
    "            return []\n",
    "\n",
    "        step = max(1, self.window - self.overlap)\n",
    "        passages = []\n",
    "\n",
    "        for i in range(0, len(words), step):\n",
    "            chunk = words[i:i + self.window]\n",
    "            if len(chunk) < self.min_passage_words:\n",
    "                break\n",
    "            passages.append(\" \".join(chunk))\n",
    "\n",
    "        return passages\n",
    "\n",
    "    def rerank(\n",
    "        self,\n",
    "        query: str,\n",
    "        passages: List[str],\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Rerank passages using bi-encoder cosine similarity.\n",
    "        \"\"\"\n",
    "        if not passages:\n",
    "            return []\n",
    "\n",
    "        q_emb = bi_encoder.encode(query, convert_to_tensor=True, device=device)\n",
    "        p_embs = bi_encoder.encode(passages, convert_to_tensor=True, device=device)\n",
    "\n",
    "        scores = util.cos_sim(q_emb, p_embs).squeeze(0)\n",
    "        top_k = min(self.k_passages, len(passages))\n",
    "        idx = torch.topk(scores, k=top_k).indices.tolist()\n",
    "\n",
    "        return [passages[i] for i in idx]\n",
    "\n",
    "    def retrieve_context(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieve passages using BM25 or QLD, optionally followed by bi-encoder reranking.\n",
    "        \"\"\"\n",
    "        if self.method == \"bm25\":\n",
    "            searcher.set_bm25(self.k1, self.b)\n",
    "        else:\n",
    "            searcher.set_qld(self.mu)\n",
    "\n",
    "        hits = searcher.search(query, self.k_docs)\n",
    "        passages: List[str] = []\n",
    "\n",
    "        for hit in hits:\n",
    "            try:\n",
    "                doc = searcher.doc(hit.docid)\n",
    "                content = json.loads(doc.raw()).get(\"contents\", \"\").replace(\"\\n\", \" \")\n",
    "                passages.extend(self.extract_passages(content))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not self.use_rerank:\n",
    "            return passages[:self.k_passages]\n",
    "\n",
    "        return self.rerank(query, passages)\n",
    "\n",
    "\n",
    "# Test the RetrievalManager\n",
    "query = \"Who wrote Harry Potter?\"\n",
    "\n",
    "configs = [\n",
    "    RetrievalManager(method=\"qld\", use_rerank=False),\n",
    "    RetrievalManager(method=\"qld\", use_rerank=True),\n",
    "    RetrievalManager(method=\"bm25\", use_rerank=False),\n",
    "    RetrievalManager(method=\"bm25\", use_rerank=True),\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    print(cfg)\n",
    "    test_passages = cfg.retrieve_context(query)\n",
    "    for i, p in enumerate(test_passages, 1):\n",
    "        print(f\"{i}. {p[:100]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e849f6",
   "metadata": {},
   "source": [
    "## 4. LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b4b616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-12-13 21:19:54,020 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded on: GPU\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Suppress transformers warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Loading LLM model...\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Set pad_token for batch processing\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "\n",
    "print(f\"✓ Model loaded on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97ae5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: temp=0.1, top_p=0.9, max_tokens=256\n",
      "✓ Generated answer: 'J. K. Rowling'\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You must respond based strictly on the information in provided passages.\"\n",
    "    \"Do not incorporate any external knowledge or infer any details beyond what is given.\"\n",
    "    \"If the answer is not in the context, return 'I dont know'.\"\n",
    "    \"Do not include explanations, only the final answer!\"\n",
    ")\n",
    "\n",
    "DEFAULT_USER_PROMPT = (\n",
    "    \"Based on the following documents, provide a concise answer to the question.\\n\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class PromptManager:\n",
    "    \"\"\"Manages prompt generation and LLM answer generation.\"\"\"\n",
    "    system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    "    user_prompt: str = DEFAULT_USER_PROMPT\n",
    "    temperature: float = 0.1\n",
    "    top_p: float = 0.9\n",
    "    max_new_tokens: int = 256\n",
    "    do_sample: bool = True\n",
    "    prompt_id: str = \"default\"  # For later use in prompt tuning\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"temp={self.temperature}, top_p={self.top_p}, max_tokens={self.max_new_tokens}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_answer(answer: str) -> str:\n",
    "        \"\"\"Clean and standardize the generated answer.\"\"\"\n",
    "        answer = re.sub(r'^(Answer|The answer is|Based on the .*?,):?\\s*', '', answer, flags=re.I)\n",
    "        answer = answer.rstrip('.')\n",
    "        if any(phrase in answer.lower() for phrase in [\"dont know\", \"don't know\", \"do not know\", \"unknown\"]):\n",
    "            return \"unknown\"\n",
    "        return answer.strip()\n",
    "\n",
    "    def create_messages(self, question: str, contexts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Create messages for the LLM based on the question and contexts.\"\"\"\n",
    "        if not contexts:\n",
    "            context_str = \"No relevant documents found.\"\n",
    "        else:\n",
    "            context_str = '\\n\\n'.join([f\"Document {i+1}: {ctx}\" for i, ctx in enumerate(contexts)])\n",
    "        \n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": self.user_prompt.format(context=context_str, question=question)}\n",
    "        ]\n",
    "\n",
    "    def generate_answer(self, question: str, contexts: List[str]) -> str:\n",
    "        \"\"\"Generate an answer using the LLM based on the question and contexts.\"\"\"\n",
    "        messages = self.create_messages(question, contexts)\n",
    "        \n",
    "        outputs = pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=self.do_sample,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "        )\n",
    "        \n",
    "        answer = outputs[0][\"generated_text\"][-1].get('content', '')\n",
    "        return self.clean_answer(answer)\n",
    "\n",
    "    def batch_generate_answers(self, questions: List[str], contexts_list: List[List[str]]) -> List[str]:\n",
    "        \"\"\"Generate answers for multiple questions in batch.\"\"\"\n",
    "        # Create messages for all questions\n",
    "        batch_messages = [self.create_messages(q, ctx) for q, ctx in zip(questions, contexts_list)]\n",
    "        \n",
    "        # Process batch through pipeline\n",
    "        outputs = pipeline(\n",
    "            batch_messages,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=self.do_sample,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p\n",
    "        )\n",
    "        \n",
    "        # Extract and clean answers\n",
    "        answers = []\n",
    "        for output in outputs:\n",
    "            answer = output[0][\"generated_text\"][-1].get('content', '')\n",
    "            answers.append(self.clean_answer(answer))\n",
    "        \n",
    "        return answers\n",
    "\n",
    "\n",
    "# Test the PromptManager\n",
    "test_prompt_manager = PromptManager()\n",
    "print(f\"Testing: {test_prompt_manager}\")\n",
    "test_answer = test_prompt_manager.generate_answer(query, test_passages)\n",
    "print(f\"✓ Generated answer: '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873c1ef",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9b4c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation test: F1=66.67, P=66.67, R=66.67, EM=66.67\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer for comparison\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_token_metrics(prediction: str, ground_truth: str) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score for token-level comparison.\n",
    "    Returns: (precision, recall, f1)\n",
    "    \"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    # Handle empty cases\n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        match = int(pred_tokens == gt_tokens)\n",
    "        return match, match, match\n",
    "    \n",
    "    # Compute overlap\n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate_predictions(df_gold: pd.DataFrame, predictions: Dict[int, str]) -> Dict:\n",
    "    \"\"\"Evaluate predictions against ground truth.\"\"\"\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    exact_matches = []\n",
    "    \n",
    "    for _, row in df_gold.iterrows():\n",
    "        qid = row['id']\n",
    "        \n",
    "        # Handle missing predictions\n",
    "        if qid not in predictions:\n",
    "            f1_scores.append(0.0)\n",
    "            precision_scores.append(0.0)\n",
    "            recall_scores.append(0.0)\n",
    "            exact_matches.append(0)\n",
    "            continue\n",
    "        \n",
    "        prediction = predictions[qid]\n",
    "        ground_truths = row['answers']\n",
    "        \n",
    "        # Normalize once\n",
    "        norm_prediction = normalize_answer(prediction)\n",
    "        \n",
    "        # Find best match across all ground truths\n",
    "        best_f1 = 0.0\n",
    "        best_precision = 0.0\n",
    "        best_recall = 0.0\n",
    "        is_exact = 0\n",
    "        \n",
    "        for gt in ground_truths:\n",
    "            norm_gt = normalize_answer(gt)\n",
    "            \n",
    "            # Compute metrics\n",
    "            prec, rec, f1 = compute_token_metrics(prediction, gt)\n",
    "            \n",
    "            # Track best scores\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_precision = prec\n",
    "                best_recall = rec\n",
    "            \n",
    "            # Check exact match\n",
    "            if norm_prediction == norm_gt:\n",
    "                is_exact = 1\n",
    "        \n",
    "        f1_scores.append(best_f1)\n",
    "        precision_scores.append(best_precision)\n",
    "        recall_scores.append(best_recall)\n",
    "        exact_matches.append(is_exact)\n",
    "    \n",
    "    return {\n",
    "        'f1': 100.0 * sum(f1_scores) / len(f1_scores) if f1_scores else 0.0,\n",
    "        'precision': 100.0 * sum(precision_scores) / len(precision_scores) if precision_scores else 0.0,\n",
    "        'recall': 100.0 * sum(recall_scores) / len(recall_scores) if recall_scores else 0.0,\n",
    "        'exact_match': 100.0 * sum(exact_matches) / len(exact_matches) if exact_matches else 0.0,\n",
    "        'f1_scores': f1_scores,\n",
    "        'precision_scores': precision_scores,\n",
    "        'recall_scores': recall_scores,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "test_predictions = {1: \"J.K. Rowling\", 2: \"Paris\", 3: \"Shakespeare\"}\n",
    "test_gold = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'answers': [[\"J.K. Rowling\", \"Rowling\"], [\"Earth\"], [\"William Shakespeare\", \"Shakespeare\"]]\n",
    "})\n",
    "\n",
    "test_metrics = evaluate_predictions(test_gold, test_predictions)\n",
    "print(f\"✓ Evaluation test: F1={test_metrics['f1']:.2f}, P={test_metrics['precision']:.2f}, R={test_metrics['recall']:.2f}, EM={test_metrics['exact_match']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956e446",
   "metadata": {},
   "source": [
    "## 6. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "721292c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing experiment with:\n",
      "  Retrieval: QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "  Prompt: temp=0.1, top_p=0.9, max_tokens=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quick Test: 100%|██████████| 2/2 [00:08<00:00,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick Test\n",
      "   Retrieval: QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=20.00 | P=24.00 | R=30.00 | EM=0.00\n",
      "   Questions: 5\n",
      "\n",
      "✓ Experiment framework ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    name: str,\n",
    "    df_data: pd.DataFrame,\n",
    "    retrieval_manager: RetrievalManager,\n",
    "    prompt_manager: PromptManager,\n",
    "    max_questions: Optional[int] = None,\n",
    "    batch_size: int = 4,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    if max_questions:\n",
    "        df_data = df_data.head(max_questions)\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Process in batches\n",
    "    num_batches = (len(df_data) + batch_size - 1) // batch_size\n",
    "    iterator = tqdm(range(num_batches), desc=name) if verbose else range(num_batches)\n",
    "    \n",
    "    for batch_idx in iterator:\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(df_data))\n",
    "        batch_df = df_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Retrieve contexts for all questions in batch\n",
    "        batch_questions = []\n",
    "        batch_qids = []\n",
    "        batch_contexts = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            question = row['question']\n",
    "            qid = row['id']\n",
    "            contexts = retrieval_manager.retrieve_context(question)\n",
    "            \n",
    "            batch_questions.append(question)\n",
    "            batch_qids.append(qid)\n",
    "            batch_contexts.append(contexts)\n",
    "        \n",
    "        # Generate answers in batch\n",
    "        batch_answers = prompt_manager.batch_generate_answers(batch_questions, batch_contexts)\n",
    "        \n",
    "        # Store predictions\n",
    "        for qid, answer in zip(batch_qids, batch_answers):\n",
    "            predictions[qid] = answer\n",
    "    \n",
    "    metrics = evaluate_predictions(df_data, predictions)\n",
    "    \n",
    "    result = {\n",
    "        'name': name,\n",
    "        'retrieval': retrieval_manager,\n",
    "        'prompt': prompt_manager,\n",
    "        'f1_score': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'exact_match': metrics['exact_match'],\n",
    "        'num_questions': len(df_data),\n",
    "        'predictions': predictions,\n",
    "        'f1_scores': metrics['f1_scores'],\n",
    "        'precision_scores': metrics['precision_scores'],\n",
    "        'recall_scores': metrics['recall_scores'],\n",
    "        'exact_matches': metrics['exact_matches']\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"   Retrieval: {retrieval_manager}\")\n",
    "        print(f\"   Prompt: {prompt_manager}\")\n",
    "        print(f\"   F1={metrics['f1']:.2f} | P={metrics['precision']:.2f} | R={metrics['recall']:.2f} | EM={metrics['exact_match']:.2f}\")\n",
    "        print(f\"   Questions: {len(df_data)}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test experiment\n",
    "test_retrieval = RetrievalManager()\n",
    "test_prompt = PromptManager()\n",
    "print(f\"Testing experiment with:\")\n",
    "print(f\"  Retrieval: {test_retrieval}\")\n",
    "print(f\"  Prompt: {test_prompt}\")\n",
    "\n",
    "test_exp = run_experiment(\n",
    "    \"Quick Test\",\n",
    "    df_val.head(5),\n",
    "    test_retrieval,\n",
    "    test_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Experiment framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78eead",
   "metadata": {},
   "source": [
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54f379",
   "metadata": {},
   "source": [
    "### Experiments global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac8c7605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASED RETRIEVAL + GENERATION EXPERIMENT FRAMEWORK\n",
      "================================================================================\n",
      "Validation questions per config: 100\n",
      "Random seed: 42\n",
      "Results cache: ./results/grid_search_results_q100.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_SEED = 42\n",
    "EXPERIMENT_QUESTIONS = 100\n",
    "\n",
    "EXPERIMENT_LOG_PATH = (\n",
    "    f\"./results/grid_search_results_q{EXPERIMENT_QUESTIONS}.csv\"\n",
    ")\n",
    "\n",
    "validation_data = df_val.sample(\n",
    "    n=EXPERIMENT_QUESTIONS,\n",
    "    random_state=EXPERIMENT_SEED\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASED RETRIEVAL + GENERATION EXPERIMENT FRAMEWORK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Validation questions per config: {EXPERIMENT_QUESTIONS}\")\n",
    "print(f\"Random seed: {EXPERIMENT_SEED}\")\n",
    "print(f\"Results cache: {EXPERIMENT_LOG_PATH}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9a89e",
   "metadata": {},
   "source": [
    "### Experiments utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_retrieval_manager(base: dict, override: dict) -> RetrievalManager:\n",
    "    \"\"\"Build RetrievalManager safely.\"\"\"\n",
    "    return RetrievalManager(**{**base, **override})\n",
    "\n",
    "\n",
    "def generate_config_key(\n",
    "    retrieval_mgr: RetrievalManager,\n",
    "    prompt_mgr: PromptManager,\n",
    ") -> str:\n",
    "    \"\"\"Unique experiment key: retrieval + prompt + decoding + dataset size.\"\"\"\n",
    "    method_part = f\"alpha{retrieval_mgr.alpha}_mu{retrieval_mgr.mu}_k1{retrieval_mgr.k1}_b{retrieval_mgr.b}\"\n",
    "\n",
    "    return (\n",
    "        f\"HYBRID_\"\n",
    "        f\"kdocs{retrieval_mgr.k_docs}_\"\n",
    "        f\"kpass{retrieval_mgr.k_passages}_\"\n",
    "        f\"{method_part}_\"\n",
    "        f\"win{retrieval_mgr.window}_ovl{retrieval_mgr.overlap}_\"\n",
    "        f\"prompt{prompt_mgr.prompt_id}_\"\n",
    "        f\"sample{int(prompt_mgr.do_sample)}_\"\n",
    "        f\"temp{prompt_mgr.temperature}_\"\n",
    "        f\"q{EXPERIMENT_QUESTIONS}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def save_results_to_csv(result: dict, key: str, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    row = {\n",
    "        \"config_key\": key,\n",
    "        \"f1\": result[\"f1_score\"],\n",
    "        \"precision\": result[\"precision\"],\n",
    "        \"recall\": result[\"recall\"],\n",
    "        \"exact_match\": result[\"exact_match\"],\n",
    "        \"num_questions\": result[\"num_questions\"],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([row])\n",
    "    if not os.path.exists(path):\n",
    "        df.to_csv(path, index=False)\n",
    "    else:\n",
    "        df.to_csv(path, mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "def load_completed_configs(path: str) -> set[str]:\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    return set(pd.read_csv(path)[\"config_key\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb074e07",
   "metadata": {},
   "source": [
    "### Best-config selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987015b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_per_alpha_range(\n",
    "    retrieval_managers: list[RetrievalManager],\n",
    "    prompt_managers: list[PromptManager],\n",
    "):\n",
    "    \"\"\"Select the best (highest-F1) config for low, mid, and high alpha values.\"\"\"\n",
    "    df = pd.read_csv(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    # Define alpha ranges\n",
    "    best = {\n",
    "        \"low_alpha\": {\"f1\": -1.0, \"entry\": None},    # alpha < 0.4 (BM25-dominant)\n",
    "        \"mid_alpha\": {\"f1\": -1.0, \"entry\": None},    # 0.4 <= alpha <= 0.6 (balanced)\n",
    "        \"high_alpha\": {\"f1\": -1.0, \"entry\": None},   # alpha > 0.6 (QLD-dominant)\n",
    "    }\n",
    "\n",
    "    for r_mgr, p_mgr in zip(retrieval_managers, prompt_managers):\n",
    "        key = generate_config_key(r_mgr, p_mgr)\n",
    "        row = df[df[\"config_key\"] == key]\n",
    "        if row.empty:\n",
    "            continue\n",
    "\n",
    "        f1 = row.iloc[0][\"f1\"]\n",
    "        alpha = r_mgr.alpha\n",
    "\n",
    "        # Categorize by alpha value\n",
    "        if alpha < 0.4:\n",
    "            category = \"low_alpha\"\n",
    "        elif alpha <= 0.6:\n",
    "            category = \"mid_alpha\"\n",
    "        else:\n",
    "            category = \"high_alpha\"\n",
    "\n",
    "        if f1 > best[category][\"f1\"]:\n",
    "            best[category] = {\n",
    "                \"f1\": f1,\n",
    "                \"entry\": {\n",
    "                    \"retrieval_mgr\": r_mgr,\n",
    "                    \"prompt_mgr\": p_mgr,\n",
    "                    \"f1\": f1,\n",
    "                },\n",
    "            }\n",
    "\n",
    "    return [\n",
    "        v[\"entry\"]\n",
    "        for v in best.values()\n",
    "        if v[\"entry\"] is not None\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f416f",
   "metadata": {},
   "source": [
    "### Phase runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827db996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phase(\n",
    "    *,\n",
    "    phase_name: str,\n",
    "    grid: list[dict],\n",
    "    validation_data,\n",
    "    select_best=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a single experiment phase.\n",
    "    Each grid item must contain:\n",
    "      - retrieval_mgr: RetrievalManager\n",
    "      - prompt_mgr: PromptManager\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(phase_name)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    completed = load_completed_configs(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    pending = [\n",
    "        g for g in grid\n",
    "        if generate_config_key(g[\"retrieval_mgr\"], g[\"prompt_mgr\"]) not in completed\n",
    "    ]\n",
    "\n",
    "    print(f\"Total configs: {len(grid)}\")\n",
    "    print(f\"Pending configs: {len(pending)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, entry in enumerate(pending, start=1):\n",
    "        retrieval_mgr = entry[\"retrieval_mgr\"]\n",
    "        prompt_mgr = entry[\"prompt_mgr\"]\n",
    "\n",
    "        key = generate_config_key(retrieval_mgr, prompt_mgr)\n",
    "        print(f\"[{i}/{len(pending)}] Running: {key}\")\n",
    "\n",
    "        result = run_experiment(\n",
    "            name=key,\n",
    "            df_data=validation_data,\n",
    "            retrieval_manager=retrieval_mgr,\n",
    "            prompt_manager=prompt_mgr,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        save_results_to_csv(result, key, EXPERIMENT_LOG_PATH)\n",
    "        print(f\"✓ F1={result['f1_score']:.2f}\")\n",
    "\n",
    "    if not select_best:\n",
    "        return grid\n",
    "\n",
    "    best = select_best_per_alpha_range(\n",
    "        [g[\"retrieval_mgr\"] for g in grid],\n",
    "        [g[\"prompt_mgr\"] for g in grid],\n",
    "    )\n",
    "\n",
    "    print(\"\\nBest configs selected:\")\n",
    "    for entry in best:\n",
    "        print(\n",
    "            f\"✓ {generate_config_key(entry['retrieval_mgr'], entry['prompt_mgr'])} | \"\n",
    "            f\"F1={entry['f1']:.2f}\"\n",
    "        )\n",
    "\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb8d9fa",
   "metadata": {},
   "source": [
    "### Phase 1: Retrieval Method\n",
    "\n",
    "Goal: Tune QLD μ and BM25 k1/b independently\n",
    "Reranking disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09dd9a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1 — RETRIEVAL METHOD & RERANKING\n",
      "================================================================================\n",
      "Total configs: 18\n",
      "Pending configs: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Best configs selected:\n",
      "✓ QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win150_ovl50_promptdefault_sample1_temp0.1_q100 | F1=13.81\n",
      "✓ BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win150_ovl50_promptdefault_sample1_temp0.1_q100 | F1=12.34\n"
     ]
    }
   ],
   "source": [
    "PHASE_1_GRID = []\n",
    "\n",
    "BASE_RETRIEVAL = {\n",
    "    \"k_docs\": 5,\n",
    "    \"k_passages\": 3,\n",
    "    \"window\": 150,\n",
    "    \"overlap\": 50,\n",
    "}\n",
    "\n",
    "# QLD\n",
    "for mu in [500, 1000, 2000]:\n",
    "    for rerank in [True, False]:\n",
    "        PHASE_1_GRID.append({\n",
    "            \"retrieval_mgr\": build_retrieval_manager(\n",
    "                BASE_RETRIEVAL,\n",
    "                {\"method\": \"qld\", \"mu\": mu, \"use_rerank\": rerank},\n",
    "            ),\n",
    "            \"prompt_mgr\": PromptManager(),\n",
    "        })\n",
    "\n",
    "# BM25\n",
    "for k1, b in itertools.product([0.6, 0.9, 1.2], [0.4, 0.75]):\n",
    "    for rerank in [True, False]:\n",
    "        PHASE_1_GRID.append({\n",
    "            \"retrieval_mgr\": build_retrieval_manager(\n",
    "                BASE_RETRIEVAL,\n",
    "                {\"method\": \"bm25\", \"k1\": k1, \"b\": b, \"use_rerank\": rerank},\n",
    "            ),\n",
    "            \"prompt_mgr\": PromptManager(),\n",
    "        })\n",
    "\n",
    "BEST_PHASE_1 = run_phase(\n",
    "    phase_name=\"PHASE 1 — RETRIEVAL METHOD & RERANKING\",\n",
    "    grid=PHASE_1_GRID,\n",
    "    validation_data=validation_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c448b842",
   "metadata": {},
   "source": [
    "### Phase 2: Passage Segmentation\n",
    "Goal: tune window / overlap with best retrieval params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86e95ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2 — PASSAGE WINDOW & OVERLAP\n",
      "================================================================================\n",
      "Total configs: 30\n",
      "Pending configs: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Best configs selected:\n",
      "✓ QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win250_ovl70_promptdefault_sample1_temp0.1_q100 | F1=23.39\n",
      "✓ BM25_kdocs5_kpass3_RERANK_BM25_k11.2_b0.4_win250_ovl70_promptdefault_sample1_temp0.1_q100 | F1=21.62\n"
     ]
    }
   ],
   "source": [
    "PHASE_2_GRID = []\n",
    "\n",
    "for best in BEST_PHASE_1:\n",
    "    for window, overlap in itertools.product([100, 150, 200, 250, 300], [30, 50, 70]):\n",
    "        PHASE_2_GRID.append({\n",
    "            \"retrieval_mgr\": build_retrieval_manager(\n",
    "                best[\"retrieval_mgr\"].__dict__,\n",
    "                {\"window\": window, \"overlap\": overlap},\n",
    "            ),\n",
    "            \"prompt_mgr\": best[\"prompt_mgr\"],\n",
    "        })\n",
    "\n",
    "BEST_PHASE_2 = run_phase(\n",
    "    phase_name=\"PHASE 2 — PASSAGE WINDOW & OVERLAP\",\n",
    "    grid=PHASE_2_GRID,\n",
    "    validation_data=validation_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748949d",
   "metadata": {},
   "source": [
    "### Phase 3 — k_docs / k_passages Tradeoff\n",
    "Goal: tune recall vs precision tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f47ad0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3 — RETRIEVAL DEPTH\n",
      "================================================================================\n",
      "Total configs: 14\n",
      "Pending configs: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Best configs selected:\n",
      "✓ QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win250_ovl70_promptdefault_sample1_temp0.1_q100 | F1=23.39\n",
      "✓ BM25_kdocs10_kpass5_RERANK_BM25_k11.2_b0.4_win250_ovl70_promptdefault_sample1_temp0.1_q100 | F1=22.90\n"
     ]
    }
   ],
   "source": [
    "PHASE_3_GRID = []\n",
    "\n",
    "K_VARIANTS = [\n",
    "    {\"k_docs\": 3,  \"k_passages\": 2},\n",
    "    {\"k_docs\": 5,  \"k_passages\": 3},   # baseline\n",
    "    {\"k_docs\": 8,  \"k_passages\": 3},\n",
    "    {\"k_docs\": 10, \"k_passages\": 5},\n",
    "    {\"k_docs\": 15, \"k_passages\": 5},\n",
    "    {\"k_docs\": 20, \"k_passages\": 7},\n",
    "    {\"k_docs\": 25, \"k_passages\": 10},\n",
    "]\n",
    "\n",
    "for best in BEST_PHASE_2:\n",
    "    for k_cfg in K_VARIANTS:\n",
    "        PHASE_3_GRID.append({\n",
    "            \"retrieval_mgr\": build_retrieval_manager(\n",
    "                best[\"retrieval_mgr\"].__dict__,\n",
    "                k_cfg,\n",
    "            ),\n",
    "            \"prompt_mgr\": best[\"prompt_mgr\"],\n",
    "        })\n",
    "\n",
    "BEST_PHASE_3 = run_phase(\n",
    "    phase_name=\"PHASE 3 — RETRIEVAL DEPTH\",\n",
    "    grid=PHASE_3_GRID,\n",
    "    validation_data=validation_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25681341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 4 — PROMPT & DECODING\n",
      "================================================================================\n",
      "Total configs: 16\n",
      "Pending configs: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Best configs selected:\n",
      "✓ QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win250_ovl70_promptdefault_sample0_temp0.1_q100 | F1=24.35\n",
      "✓ BM25_kdocs10_kpass5_RERANK_BM25_k11.2_b0.4_win250_ovl70_promptdefault_sample1_temp0.1_q100 | F1=22.90\n"
     ]
    }
   ],
   "source": [
    "PROMPT_VARIANTS = {\n",
    "    \"default\": {\n",
    "        \"system\": DEFAULT_SYSTEM_PROMPT,\n",
    "        \"user\": DEFAULT_USER_PROMPT,\n",
    "    },\n",
    "    \"strict\": {\n",
    "        \"system\": (\n",
    "            \"You are a strict information extraction system.\\n\"\n",
    "            \"Answer the question using ONLY the provided passages.\\n\"\n",
    "            \"The answer MUST be an exact span copied verbatim from the passages.\\n\"\n",
    "            \"Do NOT paraphrase, infer, or add information.\\n\"\n",
    "            \"If the answer does not appear explicitly in the passages, respond with: unknown.\"\n",
    "        ),\n",
    "        \"user\": (\n",
    "            \"PASSAGES:\\n\"\n",
    "            \"{context}\\n\\n\"\n",
    "            \"QUESTION:\\n\"\n",
    "            \"{question}\\n\\n\"\n",
    "            \"INSTRUCTIONS:\\n\"\n",
    "            \"- Answer with a single concise phrase, copied or extracted directly from the passages.\\n\"\n",
    "            \"- If no answer is present, answer: unknown.\\n\\n\"\n",
    "            \"ANSWER:\"\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "DECODING_VARIANTS = [\n",
    "    {\"temperature\": 0.1, \"do_sample\": True},    # baseline\n",
    "    {\"temperature\": 0.1, \"do_sample\": False},   # greedy\n",
    "    {\"temperature\": 0.3, \"do_sample\": True},    # diverse\n",
    "    {\"temperature\": 0.6, \"do_sample\": True},    # more diverse\n",
    "]\n",
    "\n",
    "PHASE_4_GRID = []\n",
    "\n",
    "for best in BEST_PHASE_3:\n",
    "    for prompt_id, prompts in PROMPT_VARIANTS.items():\n",
    "        for decode in DECODING_VARIANTS:\n",
    "            PHASE_4_GRID.append({\n",
    "                \"retrieval_mgr\": best[\"retrieval_mgr\"],\n",
    "                \"prompt_mgr\": PromptManager(\n",
    "                    system_prompt=prompts[\"system\"],\n",
    "                    user_prompt=prompts[\"user\"],\n",
    "                    temperature=decode[\"temperature\"],\n",
    "                    do_sample=decode[\"do_sample\"],\n",
    "                    prompt_id=prompt_id,\n",
    "                ),\n",
    "            })\n",
    "\n",
    "BEST_PHASE_4 = run_phase(\n",
    "    phase_name=\"PHASE 4 — PROMPT & DECODING\",\n",
    "    grid=PHASE_4_GRID,\n",
    "    validation_data=validation_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beee72d",
   "metadata": {},
   "source": [
    "### Hybrid approach test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edce7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Literal\n",
    "import json\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HybridRetrievalManager(RetrievalManager):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval manager combining QLD and BM25\n",
    "    using rank-based score interpolation.\n",
    "    \"\"\"\n",
    "    method: Literal[\"hybrid\"] = \"hybrid\"\n",
    "    alpha: float = 0.5  # alpha * QLD + (1 - alpha) * BM25\n",
    "\n",
    "    def __str__(self):\n",
    "        method_str = f\"HYBRID(α={self.alpha}, μ={self.mu}, k1={self.k1}, b={self.b})\"\n",
    "        rerank_str = \"BiEncoder\" if self.use_rerank else \"NoRerank\"\n",
    "        return (\n",
    "            f\"{method_str} → {rerank_str} | \"\n",
    "            f\"k_docs={self.k_docs}, k_passages={self.k_passages}\"\n",
    "        )\n",
    "\n",
    "    def retrieve_context(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieve passages using hybrid QLD + BM25,\n",
    "        optionally followed by bi-encoder reranking.\n",
    "        \"\"\"\n",
    "        # Hybrid document retrieval\n",
    "        def collect_hits(hits, weight, scores):\n",
    "            for rank, hit in enumerate(hits):\n",
    "                score = weight / (rank + 1)\n",
    "                scores[hit.docid] = max(scores.get(hit.docid, 0.0), score)\n",
    "\n",
    "        scores = {}\n",
    "\n",
    "        # QLD\n",
    "        searcher.set_qld(self.mu)\n",
    "        qld_hits = searcher.search(query, self.k_docs)\n",
    "        collect_hits(qld_hits, self.alpha, scores)\n",
    "\n",
    "        # BM25\n",
    "        searcher.set_bm25(self.k1, self.b)\n",
    "        bm25_hits = searcher.search(query, self.k_docs)\n",
    "        collect_hits(bm25_hits, 1.0 - self.alpha, scores)\n",
    "\n",
    "        # Top-k documents by combined score\n",
    "        top_docids = sorted(scores, key=scores.get, reverse=True)[:self.k_docs]\n",
    "        docs = [searcher.doc(docid) for docid in top_docids]\n",
    "\n",
    "        passages: List[str] = []\n",
    "        \n",
    "        for doc in docs:\n",
    "            try:\n",
    "                content = (\n",
    "                    json.loads(doc.raw())\n",
    "                    .get(\"contents\", \"\")\n",
    "                    .replace(\"\\n\", \" \")\n",
    "                )\n",
    "                passages.extend(self.extract_passages(content))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not self.use_rerank:\n",
    "            return passages[:self.k_passages]\n",
    "\n",
    "        return self.rerank(query, passages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74d5ca04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Parameter differences between best QLD and BM25:\n",
      "   k_docs: QLD=5, BM25=10\n",
      "   k_passages: QLD=3, BM25=5\n",
      "   → Testing both configurations for each alpha\n",
      "\n",
      "✓ Created 22 hybrid experiments:\n",
      "   11 alpha values × 2 parameter sets = 22 configs\n",
      "   Hybrid params: μ=1000, k1=1.2, b=0.4\n",
      "\n",
      "================================================================================\n",
      "PHASE 5 — HYBRID RETRIEVAL\n",
      "================================================================================\n",
      "Total configs: 22\n",
      "Pending configs: 1\n",
      "--------------------------------------------------------------------------------\n",
      "[1/1] Running: HYBRID_kdocs10_kpass5_RERANK_HYBRID_alpha1.0_mu1000_k11.2_b0.4_win250_ovl70_promptdefault_sample0_temp0.1_q100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HYBRID_kdocs10_kpass5_RERANK_HYBRID_alpha1.0_mu1000_k11.2_b0.4_win250_ovl70_promptdefault_sample0_temp0.1_q100: 100%|██████████| 25/25 [07:14<00:00, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HYBRID_kdocs10_kpass5_RERANK_HYBRID_alpha1.0_mu1000_k11.2_b0.4_win250_ovl70_promptdefault_sample0_temp0.1_q100\n",
      "   Retrieval: HYBRID(α=1.0, μ=1000, k1=1.2, b=0.4) → BiEncoder | k_docs=10, k_passages=5\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=18.38 | P=18.28 | R=30.20 | EM=9.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=18.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "HYBRID_ALPHAS = np.round(np.arange(0.0, 1.1, 0.1), 2).tolist()\n",
    "\n",
    "# Extract best QLD and BM25 configs from Phase 4\n",
    "qld_best = next(e for e in BEST_PHASE_4 if e[\"retrieval_mgr\"].method == \"qld\")\n",
    "bm25_best = next(e for e in BEST_PHASE_4 if e[\"retrieval_mgr\"].method == \"bm25\")\n",
    "\n",
    "# Extract retrieval-specific parameters (will be same for all hybrid configs)\n",
    "mu = qld_best[\"retrieval_mgr\"].mu\n",
    "k1 = bm25_best[\"retrieval_mgr\"].k1\n",
    "b = bm25_best[\"retrieval_mgr\"].b\n",
    "\n",
    "# Extract common parameters from both configs\n",
    "COMMON_PARAMS = [\"k_docs\", \"k_passages\", \"window\", \"overlap\", \"use_rerank\"]\n",
    "qld_params = {p: getattr(qld_best[\"retrieval_mgr\"], p) for p in COMMON_PARAMS}\n",
    "bm25_params = {p: getattr(bm25_best[\"retrieval_mgr\"], p) for p in COMMON_PARAMS}\n",
    "\n",
    "# Report differences\n",
    "diff_params = [p for p in COMMON_PARAMS if qld_params[p] != bm25_params[p]]\n",
    "if diff_params:\n",
    "    print(f\"⚠️  Parameter differences between best QLD and BM25:\")\n",
    "    for p in diff_params:\n",
    "        print(f\"   {p}: QLD={qld_params[p]}, BM25={bm25_params[p]}\")\n",
    "    print(f\"   → Testing both configurations for each alpha\")\n",
    "else:\n",
    "    print(f\"✓ QLD and BM25 share identical common parameters\")\n",
    "\n",
    "# Build hybrid grid: each alpha tested with both QLD and BM25 parameter sets\n",
    "HYBRID_GRID = []\n",
    "for alpha in HYBRID_ALPHAS:\n",
    "    for params in [qld_params, bm25_params]:\n",
    "        HYBRID_GRID.append({\n",
    "            \"retrieval_mgr\": HybridRetrievalManager(\n",
    "                mu=mu, k1=k1, b=b, alpha=alpha, **params\n",
    "            ),\n",
    "            \"prompt_mgr\": qld_best[\"prompt_mgr\"],\n",
    "        })\n",
    "\n",
    "print(f\"\\n✓ Created {len(HYBRID_GRID)} hybrid experiments:\")\n",
    "print(f\"   {len(HYBRID_ALPHAS)} alpha values × 2 parameter sets = {len(HYBRID_GRID)} configs\")\n",
    "print(f\"   Hybrid params: μ={mu}, k1={k1}, b={b}\")\n",
    "\n",
    "BEST_HYBRID = run_phase(\n",
    "    phase_name=\"PHASE 5 — HYBRID RETRIEVAL\",\n",
    "    grid=HYBRID_GRID,\n",
    "    validation_data=validation_data,\n",
    "    select_best=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f924d",
   "metadata": {},
   "source": [
    "## Identifying best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bbafb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Total configurations collected: 100\n"
     ]
    }
   ],
   "source": [
    "ALL_CONFIGS = (\n",
    "    PHASE_1_GRID +\n",
    "    PHASE_2_GRID +\n",
    "    PHASE_3_GRID +\n",
    "    PHASE_4_GRID +\n",
    "    HYBRID_GRID\n",
    ")\n",
    "\n",
    "print(f\"✓ Total configurations collected: {len(ALL_CONFIGS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32a32712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Top-10 configs by validation F1:\n",
      "  1. HYBRID_kdocs5_kpass3_RERANK_HYBRID_alpha0.7_mu1000_k11.2_b0.4_win250_ovl70_promptdefault_sample0_temp0.1_q100 (F1=24.71)\n",
      "  2. HYBRID_kdocs5_kpass3_RERANK_HYBRID_alpha0.8_mu1000_k11.2_b0.4_win250_ovl70_promptdefault_sample0_temp0.1_q100 (F1=24.71)\n",
      "  3. HYBRID_kdocs5_kpass3_RERANK_HYBRID_alpha0.9_mu1000_k11.2_b0.4_win250_ovl70_promptdefault_sample0_temp0.1_q100 (F1=24.35)\n",
      "  4. QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win250_ovl70_promptdefault_sample0_temp0.1_q100 (F1=24.35)\n",
      "  5. HYBRID_kdocs5_kpass3_RERANK_HYBRID_alpha1.0_mu1000_k11.2_b0.4_win250_ovl70_promptdefault_sample0_temp0.1_q100 (F1=24.35)\n",
      "  6. QLD_kdocs20_kpass7_RERANK_QLD_mu1000_win200_ovl50_promptstrict_sample1_temp0.3_q100 (F1=24.32)\n",
      "  7. HYBRID_kdocs5_kpass3_RERANK_HYBRID_alpha0.6_mu1000_k11.2_b0.4_win250_ovl70_promptdefault_sample0_temp0.1_q100 (F1=23.85)\n",
      "  8. QLD_kdocs20_kpass7_RERANK_QLD_mu1000_win200_ovl50_promptdefault_sample1_temp0.3_q100 (F1=23.40)\n",
      "  9. QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win250_ovl70_promptdefault_sample1_temp0.1_q100 (F1=23.39)\n",
      "  10. QLD_kdocs5_kpass3_RERANK_QLD_mu1000_win250_ovl50_promptdefault_sample1_temp0.1_q100 (F1=23.39)\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.read_csv(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "top10_df = (\n",
    "    df_results\n",
    "    .sort_values(\"f1\", ascending=False)\n",
    "    .head(10)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "top10_keys = set(top10_df[\"config_key\"])\n",
    "\n",
    "print(\"✓ Top-10 configs by validation F1:\")\n",
    "for i, row in top10_df.iterrows():\n",
    "    print(f\"  {i+1}. {row['config_key']} (F1={row['f1']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "888d19f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Matched 10 configs from ALL_CONFIGS\n"
     ]
    }
   ],
   "source": [
    "TOP10_CONFIGS = []\n",
    "\n",
    "for entry in ALL_CONFIGS:\n",
    "    key = generate_config_key(\n",
    "        entry[\"retrieval_mgr\"],\n",
    "        entry[\"prompt_mgr\"],\n",
    "    )\n",
    "    if key in top10_keys:\n",
    "        TOP10_CONFIGS.append(entry)\n",
    "\n",
    "if len(TOP10_CONFIGS) != 10:\n",
    "    raise ValueError(f\"Expected 10 configs, matched {len(TOP10_CONFIGS)}\")\n",
    "print(f\"✓ Matched {len(TOP10_CONFIGS)} configs from ALL_CONFIGS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f431d377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FULL-VAL-TOP1: 100%|██████████| 189/189 [39:51<00:00, 12.65s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FULL-VAL-TOP1\n",
      "   Retrieval: QLD(mu=1000) → BiEncoder | k_docs=5, k_passages=3\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=18.30 | P=18.20 | R=26.07 | EM=10.05\n",
      "   Questions: 756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FULL-VAL-TOP2:   6%|▌         | 11/189 [01:43<27:54,  9.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m↺ Skipping already completed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFULL-VAL-TOP\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretrieval_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrieval_mgr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_mgr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_questions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     save_results_to_csv(\n\u001b[1;32m     26\u001b[0m         result\u001b[38;5;241m=\u001b[39mresult,\n\u001b[1;32m     27\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[1;32m     28\u001b[0m         path\u001b[38;5;241m=\u001b[39mFULL_VAL_CSV,\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Full validation results saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFULL_VAL_CSV\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 39\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(name, df_data, retrieval_manager, prompt_manager, max_questions, batch_size, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m     batch_contexts\u001b[38;5;241m.\u001b[39mappend(contexts)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Generate answers in batch\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m batch_answers \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_generate_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_contexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Store predictions\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qid, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_qids, batch_answers):\n",
      "Cell \u001b[0;32mIn[23], line 72\u001b[0m, in \u001b[0;36mPromptManager.batch_generate_answers\u001b[0;34m(self, questions, contexts_list)\u001b[0m\n\u001b[1;32m     69\u001b[0m batch_messages \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_messages(q, ctx) \u001b[38;5;28;01mfor\u001b[39;00m q, ctx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(questions, contexts_list)]\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Process batch through pipeline\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Extract and clean answers\u001b[39;00m\n\u001b[1;32m     82\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:331\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1448\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1445\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1446\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1447\u001b[0m     )\n\u001b[0;32m-> 1448\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:126\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:127\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1374\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1373\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1374\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:432\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    430\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 432\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    435\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2787\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2787\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   2790\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2791\u001b[0m     outputs,\n\u001b[1;32m   2792\u001b[0m     model_kwargs,\n\u001b[1;32m   2793\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2794\u001b[0m )\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:459\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    441\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:1072\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1072\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:395\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 395\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    407\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    408\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    409\u001b[0m )\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:294\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:237\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    236\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 237\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    238\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    240\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 170\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/accelerate/hooks.py:360\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    353\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    354\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    355\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[1;32m    357\u001b[0m         ):\n\u001b[1;32m    358\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[0;32m--> 360\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    370\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    371\u001b[0m )\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/accelerate/utils/modeling.py:343\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[0m\n\u001b[1;32m    341\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 343\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FULL_VAL_CSV = \"results/full_val_top10.csv\"\n",
    "\n",
    "completed_full_val = load_completed_configs(FULL_VAL_CSV)\n",
    "\n",
    "for i, entry in enumerate(TOP10_CONFIGS, 1):\n",
    "    key = generate_config_key(\n",
    "        entry[\"retrieval_mgr\"],\n",
    "        entry[\"prompt_mgr\"],\n",
    "    )\n",
    "\n",
    "    if key in completed_full_val:\n",
    "        print(f\"↺ Skipping already completed: {key}\")\n",
    "        continue\n",
    "\n",
    "    result = run_experiment(\n",
    "        name=f\"FULL-VAL-TOP{i}\",\n",
    "        df_data=df_val,\n",
    "        retrieval_manager=entry[\"retrieval_mgr\"],\n",
    "        prompt_manager=entry[\"prompt_mgr\"],\n",
    "        max_questions=1000,\n",
    "        batch_size=4,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    save_results_to_csv(\n",
    "        result=result,\n",
    "        key=key,\n",
    "        path=FULL_VAL_CSV,\n",
    "    )\n",
    "\n",
    "print(f\"\\n✓ Full validation results saved to {FULL_VAL_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3601c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_val = pd.read_csv(FULL_VAL_CSV)\n",
    "\n",
    "best_row = df_full_val.sort_values(\"f1\", ascending=False).iloc[0]\n",
    "best_key = best_row[\"config_key\"]\n",
    "\n",
    "best_entry = next(\n",
    "    entry for entry in TOP10_CONFIGS\n",
    "    if generate_config_key(\n",
    "        entry[\"retrieval_mgr\"],\n",
    "        entry[\"prompt_mgr\"],\n",
    "    ) == best_key\n",
    ")\n",
    "\n",
    "print(\"✓ Best config after 1000-question validation:\")\n",
    "print(f\"  Config key: {best_key}\")\n",
    "print(f\"  F1={best_row['f1']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_TEST_CSV = \"results/final_test_best.csv\"\n",
    "\n",
    "completed_final = load_completed_configs(FINAL_TEST_CSV)\n",
    "\n",
    "if best_key in completed_final:\n",
    "    print(f\"↺ Final test already completed for {best_key}\")\n",
    "else:\n",
    "    final_result = run_experiment(\n",
    "        name=\"FINAL-TEST\",\n",
    "        df_data=df_train,\n",
    "        retrieval_manager=best_entry[\"retrieval_mgr\"],\n",
    "        prompt_manager=best_entry[\"prompt_mgr\"],\n",
    "        max_questions=None,\n",
    "        batch_size=4,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    save_results_to_csv(\n",
    "        result=final_result,\n",
    "        key=best_key,\n",
    "        path=FINAL_TEST_CSV,\n",
    "    )\n",
    "\n",
    "    print(\"✓ Final test completed and saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2fc8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "SUBMISSION_CSV = \"submission.csv\"\n",
    "TEST_CSV_PATH = \"data/test.csv\"\n",
    "\n",
    "df_test = pd.read_csv(TEST_CSV_PATH)\n",
    "print(f\"✓ Loaded test set with {len(df_test)} questions\")\n",
    "\n",
    "retrieval_manager: RetrievalManager = best_entry[\"retrieval_mgr\"]\n",
    "prompt_manager: PromptManager = best_entry[\"prompt_mgr\"]\n",
    "\n",
    "print(\"✓ Using best configuration:\")\n",
    "print(f\"  Retrieval: {retrieval_manager}\")\n",
    "print(f\"  Prompt: {prompt_manager}\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "batch_size = 4\n",
    "num_batches = (len(df_test) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches), desc=\"Kaggle Submission\"):\n",
    "    start = batch_idx * batch_size\n",
    "    end = min(start + batch_size, len(df_test))\n",
    "    batch_df = df_test.iloc[start:end]\n",
    "\n",
    "    batch_questions = []\n",
    "    batch_ids = []\n",
    "    batch_contexts = []\n",
    "\n",
    "    for _, row in batch_df.iterrows():\n",
    "        qid = row[\"id\"]\n",
    "        question = row[\"question\"]\n",
    "\n",
    "        contexts = retrieval_manager.retrieve_context(question)\n",
    "\n",
    "        batch_ids.append(qid)\n",
    "        batch_questions.append(question)\n",
    "        batch_contexts.append(contexts)\n",
    "\n",
    "    batch_answers = prompt_manager.batch_generate_answers(\n",
    "        batch_questions,\n",
    "        batch_contexts\n",
    "    )\n",
    "\n",
    "    for qid, answer in zip(batch_ids, batch_answers):\n",
    "        rows.append({\n",
    "            \"id\": qid,\n",
    "            \"prediction\": answer,\n",
    "        })\n",
    "\n",
    "df_prediction = pd.DataFrame(rows)\n",
    "\n",
    "# Kaggle-required formatting\n",
    "df_prediction[\"prediction\"] = df_prediction[\"prediction\"].apply(\n",
    "    lambda x: json.dumps([x], ensure_ascii=False)\n",
    ")\n",
    "\n",
    "df_prediction.to_csv(SUBMISSION_CSV, index=False)\n",
    "\n",
    "print(f\"\\n✓ Submission written to {SUBMISSION_CSV}\")\n",
    "print(f\"✓ Columns: {list(df_prediction.columns)}\")\n",
    "print(\"✓ Ready for Kaggle upload\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
