{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80249d88",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "660a3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Literal\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79df6",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "38133d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4cf0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4783a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff9235",
   "metadata": {},
   "source": [
    "### Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "37f1afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged into Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(os.getenv('HUGGING_FACE_TOKEN'))\n",
    "print(\"✓ Logged into Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a17cc",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bb3486b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3778 questions\n",
      "Test set: 2032 questions\n",
      "\n",
      "Sample question: what is the name of justin bieber brother?\n",
      "Sample answers: ['Jazmyn Bieber', 'Jaxon Bieber']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(\"./data/train.csv\", converters={\"answers\": json.loads})\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(f\"Train set: {len(df_train)} questions\")\n",
    "print(f\"Test set: {len(df_test)} questions\")\n",
    "print(f\"\\nSample question: {df_train.iloc[0]['question']}\")\n",
    "print(f\"Sample answers: {df_train.iloc[0]['answers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2b208668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Split data:\n",
      "   Training: 3022 questions\n",
      "   Validation: 756 questions\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation split for experiments\n",
    "RANDOM_SEED = 42\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "df_train_split = df_train.sample(frac=1-VAL_SIZE, random_state=RANDOM_SEED)\n",
    "df_val = df_train.drop(df_train_split.index).reset_index(drop=True)\n",
    "df_train_split = df_train_split.reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Split data:\")\n",
    "print(f\"   Training: {len(df_train_split)} questions\")\n",
    "print(f\"   Validation: {len(df_val)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14492e1",
   "metadata": {},
   "source": [
    "## 3. Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8ef07078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pyserini index...\n",
      "SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n",
      "✓ Index loaded: 5903530 documents\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "# Load Pyserini index\n",
    "print(\"Loading Pyserini index...\")\n",
    "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "\n",
    "print(f\"✓ Index loaded: {index_reader.stats()['documents']} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load bi-encoder\n",
    "print(\"Loading bi-encoder...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bi_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "print(\"✓ Bi-encoder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d0537dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing QLD\n",
      "QLD(k=3, mu=1000) | docs: 300chars\n",
      "✓ Retrieved 3 documents\n",
      "  1. Harry Potter Harry Potter is a series of fantasy novels written by British autho...\n",
      "  2. Bonnie Wright Bonnie Francesca Wright (born 17 February 1991) is an English actr...\n",
      "  3. Politics of Harry Potter There are many published theories about the politics of...\n",
      "\n",
      "============================================================\n",
      "Testing BM25\n",
      "BM25(k=3, k1=0.9, b=0.4) | docs: 300chars\n",
      "✓ Retrieved 3 documents\n",
      "  1. Bonnie Wright Bonnie Francesca Wright (born 17 February 1991) is an English actr...\n",
      "  2. The Magical Worlds of Harry Potter The Magical Worlds of Harry Potter: A Treasur...\n",
      "  3. Harry Potter Harry Potter is a series of fantasy novels written by British autho...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Literal\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievalConfig:\n",
    "    \"\"\"Configuration for document/passage retrieval.\"\"\"\n",
    "    k: int = 3\n",
    "    method: Literal['bm25', 'qld'] = 'qld'\n",
    "    mu: int = 1000\n",
    "    k1: float = 0.9\n",
    "    b: float = 0.4\n",
    "    use_passages: bool = False\n",
    "    window: int = 150\n",
    "    overlap: int = 50\n",
    "    min_passage_words: int = 30\n",
    "    max_passages: int = 3\n",
    "    max_chars: int = 300\n",
    "    \n",
    "    def __str__(self):\n",
    "        method_params = \"\"\n",
    "        if self.method == 'qld':\n",
    "            method_params = f\"mu={self.mu}\"\n",
    "        elif self.method == 'bm25':\n",
    "            method_params = f\"k1={self.k1}, b={self.b}\"        \n",
    "        \n",
    "        if self.use_passages:\n",
    "            mode_info = f\"passages: window={self.window}, overlap={self.overlap}, max={self.max_passages}\"\n",
    "        else:\n",
    "            mode_info = f\"docs: {self.max_chars}chars\"\n",
    "        \n",
    "        return f\"{self.method.upper()}(k={self.k}, {method_params}) | {mode_info}\"\n",
    "\n",
    "\n",
    "def extract_passages(text, window=150, overlap=50, min_words=30):\n",
    "    \"\"\"Extract overlapping passages from text.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    words = text.split()\n",
    "    if len(words) < min_words:\n",
    "        return []\n",
    "    \n",
    "    passages = []\n",
    "    step = max(1, window - overlap)\n",
    "    \n",
    "    for start in range(0, len(words), step):\n",
    "        chunk = words[start:start + window]\n",
    "        \n",
    "        if len(chunk) < min_words:\n",
    "            if passages:\n",
    "                passages[-1] += \" \" + \" \".join(chunk)\n",
    "            else:\n",
    "                passages.append(\" \".join(chunk))\n",
    "            break\n",
    "        \n",
    "        passages.append(\" \".join(chunk))\n",
    "    \n",
    "    return passages\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "\n",
    "def compute_idf(passages: List[str]):\n",
    "    df = Counter()\n",
    "    N = len(passages)\n",
    "\n",
    "    for p in passages:\n",
    "        df.update(set(tokenize(p)))\n",
    "\n",
    "    return {\n",
    "        t: math.log((N + 1) / (df[t] + 1))\n",
    "        for t in df\n",
    "    }\n",
    "\n",
    "\n",
    "def rerank_passages(query: str, passages: List[str]) -> List[str]:\n",
    "    \"\"\"Rerank passages using TF×IDF-weighted query overlap.\"\"\"\n",
    "    q_tokens = Counter(tokenize(query))\n",
    "    idf = compute_idf(passages)\n",
    "    scored = []\n",
    "\n",
    "    for p in passages:\n",
    "        p_tokens = Counter(tokenize(p))\n",
    "        score = sum(\n",
    "            q_tokens[t] * p_tokens.get(t, 0) * idf.get(t, 0.0)\n",
    "            for t in q_tokens\n",
    "        )\n",
    "        scored.append((score, p))\n",
    "\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [p for _, p in scored]\n",
    "\n",
    "\n",
    "def retrieve_context(query: str, config: RetrievalConfig) -> List[str]:\n",
    "    \"\"\"Retrieve documents or passages for a given query based on the retrieval configuration.\"\"\"\n",
    "    if config.method == 'bm25':\n",
    "        searcher.set_bm25(config.k1, config.b)\n",
    "    else:\n",
    "        searcher.set_qld(config.mu)\n",
    "    \n",
    "    hits = searcher.search(query, config.k)\n",
    "    contexts = []\n",
    "\n",
    "    for hit in hits:\n",
    "        try:\n",
    "            doc = searcher.doc(hit.docid)\n",
    "            data = json.loads(doc.raw())\n",
    "            content = data['contents'].replace('\\n', ' ')\n",
    "            \n",
    "            if config.use_passages:\n",
    "                passages = extract_passages(\n",
    "                    content,\n",
    "                    config.window,\n",
    "                    config.overlap,\n",
    "                    config.min_passage_words\n",
    "                )\n",
    "                contexts.extend(passages)\n",
    "            else:\n",
    "                contexts.append(content[:config.max_chars])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if config.use_passages:\n",
    "        contexts = rerank_passages(query, contexts)\n",
    "        return contexts[:config.max_passages]\n",
    "\n",
    "    return contexts\n",
    "\n",
    "\n",
    "# Test retrieval\n",
    "query = \"Who wrote Harry Potter?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing QLD\")\n",
    "test_config_qld = RetrievalConfig(k=3, method='qld', mu=1000)\n",
    "print(f\"{test_config_qld}\")\n",
    "test_docs_qld = retrieve_context(query, test_config_qld)\n",
    "print(f\"✓ Retrieved {len(test_docs_qld)} documents\")\n",
    "for i, doc in enumerate(test_docs_qld, 1):\n",
    "    print(f\"  {i}. {doc[:80]}...\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing BM25\")\n",
    "test_config_bm25 = RetrievalConfig(k=3, method='bm25', k1=0.9, b=0.4)\n",
    "print(f\"{test_config_bm25}\")\n",
    "test_docs_bm25 = retrieve_context(query, test_config_bm25)\n",
    "print(f\"✓ Retrieved {len(test_docs_bm25)} documents\")\n",
    "for i, doc in enumerate(test_docs_bm25, 1):\n",
    "    print(f\"  {i}. {doc[:80]}...\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "test_passages = test_docs_qld\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e849f6",
   "metadata": {},
   "source": [
    "## 4. LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b4b616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model...\n",
      "✓ Model loaded on: GPU\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Suppress transformers warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Loading LLM model...\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Set pad_token for batch processing\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "\n",
    "print(f\"✓ Model loaded on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97ae5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: temp=0.1, top_p=0.9, max_tokens=256\n",
      "✓ Generated answer: 'J. K. Rowling'\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You must respond based strictly on the information in provided passages.\"\n",
    "    \"Do not incorporate any external knowledge or infer any details beyond what is given.\"\n",
    "    \"If the answer is not in the context, return 'I dont know'.\"\n",
    "    \"Do not include explanations, only the final answer!\"\n",
    ")\n",
    "\n",
    "DEFAULT_USER_PROMPT = (\n",
    "    \"Based on the following documents, provide a concise answer to the question.\\n\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class PromptConfig:\n",
    "    \"\"\"Configuration for prompt generation and LLM parameters.\"\"\"\n",
    "    system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    "    user_prompt: str = DEFAULT_USER_PROMPT\n",
    "    temperature: float = 0.1\n",
    "    top_p: float = 0.9\n",
    "    max_new_tokens: int = 256\n",
    "    do_sample: bool = True\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"temp={self.temperature}, top_p={self.top_p}, max_tokens={self.max_new_tokens}\"\n",
    "\n",
    "def clean_answer(answer: str) -> str:\n",
    "    \"\"\"Clean and standardize the generated answer.\"\"\"\n",
    "    answer = re.sub(r'^(Answer|The answer is|Based on the .*?,):?\\s*', '', answer, flags=re.I)\n",
    "    answer = answer.rstrip('.')\n",
    "    if any(phrase in answer.lower() for phrase in [\"dont know\", \"don't know\", \"do not know\", \"unknown\"]):\n",
    "        return \"unknown\"\n",
    "    return answer.strip()\n",
    "\n",
    "def create_messages(question: str, contexts: List[str], config: PromptConfig) -> List[Dict]:\n",
    "    \"\"\"Create messages for the LLM based on the question, contexts, and prompt configuration.\"\"\"\n",
    "    if not contexts:\n",
    "        context_str = \"No relevant documents found.\"\n",
    "    else:\n",
    "        context_str = '\\n\\n'.join([f\"Document {i+1}: {ctx}\" for i, ctx in enumerate(contexts)])\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": config.system_prompt},\n",
    "        {\"role\": \"user\", \"content\": config.user_prompt.format(context=context_str, question=question)}\n",
    "    ]\n",
    "\n",
    "def generate_answer(question: str, contexts: List[str], config: PromptConfig) -> str:\n",
    "    \"\"\"Generate an answer using the LLM based on the question, contexts, and prompt configuration.\"\"\"\n",
    "    messages = create_messages(question, contexts, config)\n",
    "    \n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=config.do_sample,\n",
    "        temperature=config.temperature,\n",
    "        top_p=config.top_p,\n",
    "    )\n",
    "    \n",
    "    answer = outputs[0][\"generated_text\"][-1].get('content', '')\n",
    "    return clean_answer(answer)\n",
    "\n",
    "def batch_generate_answers(questions: List[str], contexts_list: List[List[str]], config: PromptConfig) -> List[str]:\n",
    "    \"\"\"Generate answers for multiple questions in batch.\"\"\"\n",
    "    # Create messages for all questions\n",
    "    batch_messages = [create_messages(q, ctx, config) for q, ctx in zip(questions, contexts_list)]\n",
    "    \n",
    "    # Process batch through pipeline\n",
    "    outputs = pipeline(\n",
    "        batch_messages,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=config.do_sample,\n",
    "        temperature=config.temperature,\n",
    "        top_p=config.top_p\n",
    "    )\n",
    "    \n",
    "    # Extract and clean answers\n",
    "    answers = []\n",
    "    for output in outputs:\n",
    "        answer = output[0][\"generated_text\"][-1].get('content', '')\n",
    "        answers.append(clean_answer(answer))\n",
    "    \n",
    "    return answers\n",
    "\n",
    "test_prompt_config = PromptConfig(temperature=0.1)\n",
    "print(f\"Testing: {test_prompt_config}\")\n",
    "test_answer = generate_answer(query, test_passages, test_prompt_config)\n",
    "print(f\"✓ Generated answer: '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873c1ef",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d9b4c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation test: F1=66.67, P=66.67, R=66.67, EM=66.67\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer for comparison\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction: str, ground_truth: str) -> float:\n",
    "    \"\"\"Compute token-level F1 score\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        return int(pred_tokens == gt_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def evaluate_predictions(df_gold: pd.DataFrame, predictions: Dict[int, str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate predictions against ground truth.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with average metrics and individual scores:\n",
    "        {\n",
    "            'f1': average_f1,\n",
    "            'precision': average_precision,\n",
    "            'recall': average_recall,\n",
    "            'exact_match': exact_match_percentage,\n",
    "            'f1_scores': list of individual f1 scores,\n",
    "            'precision_scores': list of individual precision scores,\n",
    "            'recall_scores': list of individual recall scores,\n",
    "            'exact_matches': list of individual exact match flags\n",
    "        }\n",
    "    \"\"\"\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    exact_matches = []\n",
    "    \n",
    "    for _, row in df_gold.iterrows():\n",
    "        qid = row['id']\n",
    "        if qid not in predictions:\n",
    "            f1_scores.append(0.0)\n",
    "            precision_scores.append(0.0)\n",
    "            recall_scores.append(0.0)\n",
    "            exact_matches.append(0)\n",
    "            continue\n",
    "        \n",
    "        prediction = predictions[qid]\n",
    "        ground_truths = row['answers']\n",
    "        \n",
    "        # Compute metrics for each ground truth and take the best\n",
    "        best_f1 = 0.0\n",
    "        best_precision = 0.0\n",
    "        best_recall = 0.0\n",
    "        is_exact = 0\n",
    "        \n",
    "        for gt in ground_truths:\n",
    "            # F1 score\n",
    "            pred_tokens = normalize_answer(prediction).split()\n",
    "            gt_tokens = normalize_answer(gt).split()\n",
    "            \n",
    "            common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "            num_same = sum(common.values())\n",
    "            \n",
    "            if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "                f1 = int(pred_tokens == gt_tokens)\n",
    "                prec = int(pred_tokens == gt_tokens)\n",
    "                rec = int(pred_tokens == gt_tokens)\n",
    "            elif num_same == 0:\n",
    "                f1 = 0.0\n",
    "                prec = 0.0\n",
    "                rec = 0.0\n",
    "            else:\n",
    "                prec = num_same / len(pred_tokens)\n",
    "                rec = num_same / len(gt_tokens)\n",
    "                f1 = (2 * prec * rec) / (prec + rec)\n",
    "            \n",
    "            # Track best scores\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_precision = prec\n",
    "                best_recall = rec\n",
    "            \n",
    "            # Exact match\n",
    "            if normalize_answer(prediction) == normalize_answer(gt):\n",
    "                is_exact = 1\n",
    "        \n",
    "        f1_scores.append(best_f1)\n",
    "        precision_scores.append(best_precision)\n",
    "        recall_scores.append(best_recall)\n",
    "        exact_matches.append(is_exact)\n",
    "    \n",
    "    return {\n",
    "        'f1': 100.0 * sum(f1_scores) / len(f1_scores) if f1_scores else 0.0,\n",
    "        'precision': 100.0 * sum(precision_scores) / len(precision_scores) if precision_scores else 0.0,\n",
    "        'recall': 100.0 * sum(recall_scores) / len(recall_scores) if recall_scores else 0.0,\n",
    "        'exact_match': 100.0 * sum(exact_matches) / len(exact_matches) if exact_matches else 0.0,\n",
    "        'f1_scores': f1_scores,\n",
    "        'precision_scores': precision_scores,\n",
    "        'recall_scores': recall_scores,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "test_predictions = {1: \"J.K. Rowling\", 2: \"Paris\", 3: \"Shakespeare\"}\n",
    "test_gold = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'answers': [[\"J.K. Rowling\", \"Rowling\"], [\"Earth\"], [\"William Shakespeare\", \"Shakespeare\"]]\n",
    "})\n",
    "\n",
    "test_metrics = evaluate_predictions(test_gold, test_predictions)\n",
    "print(f\"✓ Evaluation test: F1={test_metrics['f1']:.2f}, P={test_metrics['precision']:.2f}, R={test_metrics['recall']:.2f}, EM={test_metrics['exact_match']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956e446",
   "metadata": {},
   "source": [
    "## 6. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "721292c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing experiment with:\n",
      "  Retrieval: QLD(k=3, mu=1000) | docs: 300chars\n",
      "  Prompt: temp=0.1, top_p=0.9, max_tokens=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quick Test: 100%|██████████| 2/2 [00:30<00:00, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick Test\n",
      "   Retrieval: QLD(k=3, mu=1000) | docs: 300chars\n",
      "   Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "   F1=14.67 | P=10.67 | R=30.00 | EM=0.00\n",
      "   Questions: 5\n",
      "\n",
      "✓ Experiment framework ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    name: str,\n",
    "    df_data: pd.DataFrame,\n",
    "    retrieval_config: RetrievalConfig,\n",
    "    prompt_config: PromptConfig,\n",
    "    max_questions: Optional[int] = None,\n",
    "    batch_size: int = 4,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    if max_questions:\n",
    "        df_data = df_data.head(max_questions)\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Process in batches\n",
    "    num_batches = (len(df_data) + batch_size - 1) // batch_size\n",
    "    iterator = tqdm(range(num_batches), desc=name) if verbose else range(num_batches)\n",
    "    \n",
    "    for batch_idx in iterator:\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(df_data))\n",
    "        batch_df = df_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Retrieve contexts for all questions in batch\n",
    "        batch_questions = []\n",
    "        batch_qids = []\n",
    "        batch_contexts = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            question = row['question']\n",
    "            qid = row['id']\n",
    "            contexts = retrieve_context(question, retrieval_config)\n",
    "            \n",
    "            batch_questions.append(question)\n",
    "            batch_qids.append(qid)\n",
    "            batch_contexts.append(contexts)\n",
    "        \n",
    "        # Generate answers in batch\n",
    "        batch_answers = batch_generate_answers(batch_questions, batch_contexts, prompt_config)\n",
    "        \n",
    "        # Store predictions\n",
    "        for qid, answer in zip(batch_qids, batch_answers):\n",
    "            predictions[qid] = answer\n",
    "    \n",
    "    metrics = evaluate_predictions(df_data, predictions)\n",
    "    \n",
    "    result = {\n",
    "        'name': name,\n",
    "        'retrieval': retrieval_config,\n",
    "        'prompt': prompt_config,\n",
    "        'f1_score': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'exact_match': metrics['exact_match'],\n",
    "        'num_questions': len(df_data),\n",
    "        'predictions': predictions,\n",
    "        'f1_scores': metrics['f1_scores'],\n",
    "        'precision_scores': metrics['precision_scores'],\n",
    "        'recall_scores': metrics['recall_scores'],\n",
    "        'exact_matches': metrics['exact_matches']\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"   Retrieval: {retrieval_config}\")\n",
    "        print(f\"   Prompt: {prompt_config}\")\n",
    "        print(f\"   F1={metrics['f1']:.2f} | P={metrics['precision']:.2f} | R={metrics['recall']:.2f} | EM={metrics['exact_match']:.2f}\")\n",
    "        print(f\"   Questions: {len(df_data)}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test experiment\n",
    "test_retrieval = RetrievalConfig(k=3, method='qld')\n",
    "test_prompt = PromptConfig(temperature=0.1)\n",
    "print(f\"Testing experiment with:\")\n",
    "print(f\"  Retrieval: {test_retrieval}\")\n",
    "print(f\"  Prompt: {test_prompt}\")\n",
    "\n",
    "test_exp = run_experiment(\n",
    "    \"Quick Test\",\n",
    "    df_val.head(5),\n",
    "    test_retrieval,\n",
    "    test_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Experiment framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dbefa",
   "metadata": {},
   "source": [
    "## 7. Grid Search Experimental Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9b46b",
   "metadata": {},
   "source": [
    "### Grid Search Configuration\n",
    "\n",
    "**Hyperparameters to test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "909432be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GRID SEARCH EXPERIMENTAL FRAMEWORK\n",
      "================================================================================\n",
      "Validation questions per config: 100\n",
      "Random seed: 42\n",
      "Results cache: ./results/grid_search_results.csv\n",
      "\n",
      "Parameter Grid:\n",
      "  method              : ['qld', 'bm25']\n",
      "  k                   : [3, 5, 10]\n",
      "  mu                  : [500, 1000, 2000]\n",
      "  k1                  : [0.6, 0.9, 1.2]\n",
      "  b                   : [0.4, 0.75]\n",
      "  use_passages        : [False, True]\n",
      "  max_chars           : [200, 300, 400]\n",
      "  window              : [100, 150]\n",
      "  overlap             : [30, 50]\n",
      "  max_passages        : [3, 5]\n",
      "  temperature         : [0.1, 0.6]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Global experiment settings\n",
    "EXPERIMENT_SEED = 42\n",
    "EXPERIMENT_QUESTIONS = 100  # Number of questions per configuration\n",
    "EXPERIMENT_LOG_PATH = \"./results/grid_search_results.csv\"\n",
    "\n",
    "# Grid search parameter space\n",
    "GRID_PARAMS = {\n",
    "    # Retrieval parameters\n",
    "    'method': ['qld', 'bm25'],\n",
    "    'k': [3, 5, 10],\n",
    "    'mu': [500, 1000, 2000],\n",
    "    'k1': [0.6, 0.9, 1.2],\n",
    "    'b': [0.4, 0.75],\n",
    "    'use_passages': [False, True],\n",
    "    'max_chars': [200, 300, 400],\n",
    "    'window': [100, 150],\n",
    "    'overlap': [30, 50],\n",
    "    'max_passages': [3, 5],\n",
    "    \n",
    "    # Prompt parameters\n",
    "    'temperature': [0.1, 0.6],\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GRID SEARCH EXPERIMENTAL FRAMEWORK\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Validation questions per config: {EXPERIMENT_QUESTIONS}\")\n",
    "print(f\"Random seed: {EXPERIMENT_SEED}\")\n",
    "print(f\"Results cache: {EXPERIMENT_LOG_PATH}\")\n",
    "print(\"\\nParameter Grid:\")\n",
    "for param, values in GRID_PARAMS.items():\n",
    "    print(f\"  {param:20s}: {values}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9adce19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Grid search helper functions ready\n"
     ]
    }
   ],
   "source": [
    "def generate_config_key(retrieval_config: RetrievalConfig, prompt_config: PromptConfig) -> str:\n",
    "    \"\"\"Generate unique key from retrieval and prompt configuration.\"\"\"\n",
    "    key_parts = []\n",
    "    key_parts.append(f\"method={retrieval_config.method}\")\n",
    "    key_parts.append(f\"k={retrieval_config.k}\")\n",
    "    \n",
    "    if retrieval_config.method == 'qld':\n",
    "        key_parts.append(f\"mu={retrieval_config.mu}\")\n",
    "    elif retrieval_config.method == 'bm25':\n",
    "        key_parts.append(f\"k1={retrieval_config.k1}\")\n",
    "        key_parts.append(f\"b={retrieval_config.b}\")\n",
    "    \n",
    "    if retrieval_config.use_passages:\n",
    "        key_parts.append(\"mode=passages\")\n",
    "        key_parts.append(f\"win={retrieval_config.window}\")\n",
    "        key_parts.append(f\"ovlp={retrieval_config.overlap}\")\n",
    "        key_parts.append(f\"maxp={retrieval_config.max_passages}\")\n",
    "    else:\n",
    "        key_parts.append(\"mode=docs\")\n",
    "        key_parts.append(f\"chars={retrieval_config.max_chars}\")\n",
    "    \n",
    "    key_parts.append(f\"temp={prompt_config.temperature}\")\n",
    "    \n",
    "    return \"__\".join(key_parts)\n",
    "\n",
    "def save_result_to_csv(result: Dict, config_key: str, filepath: str):\n",
    "    \"\"\"Save single experiment result to CSV (append mode).\"\"\"    \n",
    "    retrieval = result['retrieval']\n",
    "    prompt = result['prompt']\n",
    "    \n",
    "    row = {\n",
    "        'config_key': config_key,\n",
    "        'f1_score': result['f1_score'],\n",
    "        'precision': result['precision'],\n",
    "        'recall': result['recall'],\n",
    "        'exact_match': result['exact_match'],\n",
    "        'num_questions': result['num_questions'],\n",
    "        \n",
    "        # Retrieval params\n",
    "        'method': retrieval.method,\n",
    "        'k': retrieval.k,\n",
    "        'mu': retrieval.mu if retrieval.method == 'qld' else None,\n",
    "        'k1': retrieval.k1 if retrieval.method == 'bm25' else None,\n",
    "        'b': retrieval.b if retrieval.method == 'bm25' else None,\n",
    "        'use_passages': retrieval.use_passages,\n",
    "        'max_chars': retrieval.max_chars if not retrieval.use_passages else None,\n",
    "        'window': retrieval.window if retrieval.use_passages else None,\n",
    "        'overlap': retrieval.overlap if retrieval.use_passages else None,\n",
    "        'max_passages': retrieval.max_passages if retrieval.use_passages else None,\n",
    "        'min_passage_words': retrieval.min_passage_words if retrieval.use_passages else None,\n",
    "        \n",
    "        # Prompt params\n",
    "        'temperature': prompt.temperature,\n",
    "        'top_p': prompt.top_p,\n",
    "        'max_new_tokens': prompt.max_new_tokens,\n",
    "        'do_sample': prompt.do_sample,\n",
    "    }\n",
    "    \n",
    "    df_row = pd.DataFrame([row])\n",
    "    \n",
    "    # Create directory if needed\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    \n",
    "    # Append to CSV (create with header if doesn't exist)\n",
    "    if os.path.exists(filepath):\n",
    "        df_row.to_csv(filepath, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(filepath, mode='w', header=True, index=False)\n",
    "\n",
    "def load_existing_results(filepath: str) -> set:\n",
    "    \"\"\"Load set of already-completed configuration keys.\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        return set(df['config_key'].values)\n",
    "    return set()\n",
    "\n",
    "def generate_all_configs(grid_params: Dict) -> List[Tuple[str, RetrievalConfig, PromptConfig]]:\n",
    "    \"\"\"Generate all valid configurations from parameter grid.\"\"\"\n",
    "    configs = []\n",
    "    \n",
    "    # Generate all combinations using itertools.product\n",
    "    for temp, method, k, use_passages in itertools.product(\n",
    "        grid_params['temperature'],\n",
    "        grid_params['method'],\n",
    "        grid_params['k'],\n",
    "        grid_params['use_passages']\n",
    "    ):\n",
    "        # Create prompt config\n",
    "        prompt_config = PromptConfig(temperature=temp, top_p=0.9, max_new_tokens=256)\n",
    "        \n",
    "        # Method-specific parameters (mu for QLD, k1/b for BM25)\n",
    "        if method == 'qld':\n",
    "            method_params = [(mu, 0.9, 0.4) for mu in grid_params['mu']]\n",
    "        elif method == 'bm25':\n",
    "            method_params = [\n",
    "                (1000, k1, b) \n",
    "                for k1, b in itertools.product(grid_params['k1'], grid_params['b'])\n",
    "            ]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Generate configs for each method parameter combination\n",
    "        for mu, k1, b in method_params:\n",
    "            if use_passages:\n",
    "                # Passage mode: combine window, overlap, max_passages\n",
    "                for window, overlap, max_passages in itertools.product(\n",
    "                    grid_params['window'],\n",
    "                    grid_params['overlap'],\n",
    "                    grid_params['max_passages']\n",
    "                ):\n",
    "                    config = RetrievalConfig(\n",
    "                        k=k, method=method, mu=mu, k1=k1, b=b,\n",
    "                        use_passages=True, window=window, overlap=overlap, \n",
    "                        max_passages=max_passages\n",
    "                    )\n",
    "                    key = generate_config_key(config, prompt_config)\n",
    "                    configs.append((key, config, prompt_config))\n",
    "            else:\n",
    "                # Document mode: iterate over max_chars\n",
    "                for max_chars in grid_params['max_chars']:\n",
    "                    config = RetrievalConfig(\n",
    "                        k=k, method=method, mu=mu, k1=k1, b=b,\n",
    "                        use_passages=False, max_chars=max_chars\n",
    "                    )\n",
    "                    key = generate_config_key(config, prompt_config)\n",
    "                    configs.append((key, config, prompt_config))\n",
    "    \n",
    "\n",
    "    return configs\n",
    "\n",
    "print(\"✓ Grid search helper functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a71e40",
   "metadata": {},
   "source": [
    "### Generate Configuration Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2817365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated 594 unique configurations\n",
      "\n",
      "Configuration breakdown:\n",
      "  Methods: 2\n",
      "  k values: 3\n",
      "  QLD mu values: 3\n",
      "  BM25 (k1 × b): 3 × 2 = 6\n",
      "  Document mode (max_chars): 3\n",
      "  Passage mode (window × overlap × max_passages): 2 × 2 × 2 = 8\n",
      "  Temperature values: 2\n",
      "\n",
      "Sample configurations:\n",
      "  1. method=qld__k=3__mu=500__mode=docs__chars=200__temp=0.1\n",
      "     Retrieval: QLD(k=3, mu=500) | docs: 200chars\n",
      "     Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "\n",
      "  2. method=qld__k=3__mu=500__mode=docs__chars=300__temp=0.1\n",
      "     Retrieval: QLD(k=3, mu=500) | docs: 300chars\n",
      "     Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "\n",
      "  3. method=qld__k=3__mu=500__mode=docs__chars=400__temp=0.1\n",
      "     Retrieval: QLD(k=3, mu=500) | docs: 400chars\n",
      "     Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "\n",
      "  4. method=qld__k=3__mu=1000__mode=docs__chars=200__temp=0.1\n",
      "     Retrieval: QLD(k=3, mu=1000) | docs: 200chars\n",
      "     Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "\n",
      "  5. method=qld__k=3__mu=1000__mode=docs__chars=300__temp=0.1\n",
      "     Retrieval: QLD(k=3, mu=1000) | docs: 300chars\n",
      "     Prompt: temp=0.1, top_p=0.9, max_tokens=256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate all valid configurations\n",
    "all_configs = generate_all_configs(GRID_PARAMS)\n",
    "\n",
    "print(f\"✓ Generated {len(all_configs)} unique configurations\")\n",
    "print(f\"\\nConfiguration breakdown:\")\n",
    "print(f\"  Methods: {len(GRID_PARAMS['method'])}\")\n",
    "print(f\"  k values: {len(GRID_PARAMS['k'])}\")\n",
    "print(f\"  QLD mu values: {len(GRID_PARAMS['mu'])}\")\n",
    "print(f\"  BM25 (k1 × b): {len(GRID_PARAMS['k1'])} × {len(GRID_PARAMS['b'])} = {len(GRID_PARAMS['k1']) * len(GRID_PARAMS['b'])}\")\n",
    "print(f\"  Document mode (max_chars): {len(GRID_PARAMS['max_chars'])}\")\n",
    "print(f\"  Passage mode (window × overlap × max_passages): {len(GRID_PARAMS['window'])} × {len(GRID_PARAMS['overlap'])} × {len(GRID_PARAMS['max_passages'])} = {len(GRID_PARAMS['window']) * len(GRID_PARAMS['overlap']) * len(GRID_PARAMS['max_passages'])}\")\n",
    "print(f\"  Temperature values: {len(GRID_PARAMS['temperature'])}\")\n",
    "\n",
    "# Show sample configurations\n",
    "print(f\"\\nSample configurations:\")\n",
    "for i in range(min(5, len(all_configs))):\n",
    "    key, retrieval_config, prompt_config = all_configs[i]\n",
    "    print(f\"  {i+1}. {key}\")\n",
    "    print(f\"     Retrieval: {retrieval_config}\")\n",
    "    print(f\"     Prompt: {prompt_config}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445e680",
   "metadata": {},
   "source": [
    "### Run Grid Search\n",
    "\n",
    "**Important:** This will run all configurations sequentially. Each result is cached to CSV immediately after completion, so you can safely interrupt and resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b8370b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already completed: 122 configurations\n",
      "Remaining to run: 472 configurations\n",
      "================================================================================\n",
      "\n",
      "[1/472] Running: method=bm25__k=3__k1=0.6__b=0.4__mode=passages__win=150__ovlp=30__maxp=5__temp=0.1\n",
      "✓ F1=15.22 | P=15.32 | R=20.98 | EM=9.00 | Saved\n",
      "\n",
      "[2/472] Running: method=bm25__k=3__k1=0.6__b=0.4__mode=passages__win=150__ovlp=50__maxp=3__temp=0.1\n",
      "✓ F1=9.99 | P=10.77 | R=15.50 | EM=5.00 | Saved\n",
      "\n",
      "[3/472] Running: method=bm25__k=3__k1=0.6__b=0.4__mode=passages__win=150__ovlp=50__maxp=5__temp=0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pending_configs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretrieval_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretrieval_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Save to CSV immediately\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     save_result_to_csv(result, config_key, EXPERIMENT_LOG_PATH)\n",
      "Cell \u001b[0;32mIn[90], line 39\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(name, df_data, retrieval_config, prompt_config, max_questions, batch_size, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m     batch_contexts\u001b[38;5;241m.\u001b[39mappend(contexts)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Generate answers in batch\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m batch_answers \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_generate_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Store predictions\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qid, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_qids, batch_answers):\n",
      "Cell \u001b[0;32mIn[88], line 70\u001b[0m, in \u001b[0;36mbatch_generate_answers\u001b[0;34m(questions, contexts_list, config)\u001b[0m\n\u001b[1;32m     67\u001b[0m batch_messages \u001b[38;5;241m=\u001b[39m [create_messages(q, ctx, config) \u001b[38;5;28;01mfor\u001b[39;00m q, ctx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(questions, contexts_list)]\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Process batch through pipeline\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Extract and clean answers\u001b[39;00m\n\u001b[1;32m     80\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:331\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1448\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1445\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1446\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1447\u001b[0m     )\n\u001b[0;32m-> 1448\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:126\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:127\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1374\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1373\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1374\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:432\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    430\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 432\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    435\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2784\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2781\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 2784\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:459\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    441\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:1072\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1072\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:395\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 395\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    407\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    408\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    409\u001b[0m )\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:294\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:236\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    234\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m--> 236\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    237\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    238\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load already-completed configurations (if resuming)\n",
    "completed_keys = load_existing_results(EXPERIMENT_LOG_PATH)\n",
    "print(f\"Already completed: {len(completed_keys)} configurations\")\n",
    "\n",
    "# Filter to only pending configurations\n",
    "pending_configs = [(key, r_cfg, p_cfg) for key, r_cfg, p_cfg in all_configs if key not in completed_keys]\n",
    "print(f\"Remaining to run: {len(pending_configs)} configurations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run grid search\n",
    "validation_data = df_val.sample(n=EXPERIMENT_QUESTIONS, random_state=EXPERIMENT_SEED).reset_index(drop=True)\n",
    "\n",
    "for idx, (config_key, retrieval_config, prompt_config) in enumerate(pending_configs, start=1):\n",
    "    print(f\"\\n[{idx}/{len(pending_configs)}] Running: {config_key}\")\n",
    "    \n",
    "    try:\n",
    "        result = run_experiment(\n",
    "            name=config_key,\n",
    "            df_data=validation_data,\n",
    "            retrieval_config=retrieval_config,\n",
    "            prompt_config=prompt_config,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Save to CSV immediately\n",
    "        save_result_to_csv(result, config_key, EXPERIMENT_LOG_PATH)\n",
    "        \n",
    "        print(f\"✓ F1={result['f1_score']:.2f} | P={result['precision']:.2f} | R={result['recall']:.2f} | EM={result['exact_match']:.2f} | Saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ ERROR: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Grid search complete!\")\n",
    "print(f\"Results saved to: {EXPERIMENT_LOG_PATH}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33321a35",
   "metadata": {},
   "source": [
    "### Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f481924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configurations tested: 124\n",
      "\n",
      "Top 10 configurations by F1 Score:\n",
      "================================================================================\n",
      "\n",
      "Rank #1\n",
      "  Config: method=bm25__k=3__k1=0.6__b=0.4__mode=passages__win=150__ovlp=30__maxp=5__temp=0.1\n",
      "  F1=15.22 | Precision=15.32 | Recall=20.98 | EM=9.00\n",
      "  Method: bm25 | k=3\n",
      "  BM25: k1=0.6, b=0.4\n",
      "  Passages: window=150.0, overlap=30.0, max=5.0\n",
      "\n",
      "Rank #2\n",
      "  Config: method=qld__k=3__mu=2000__mode=passages__win=100__ovlp=30__maxp=3__temp=0.1\n",
      "  F1=13.67 | Precision=11.99 | Recall=20.23 | EM=1.00\n",
      "  Method: qld | k=3\n",
      "  QLD: mu=2000.0\n",
      "  Passages: window=100.0, overlap=30.0, max=3.0\n",
      "\n",
      "Rank #3\n",
      "  Config: method=bm25__k=3__k1=0.6__b=0.4__mode=passages__win=100__ovlp=30__maxp=3__temp=0.1\n",
      "  F1=13.63 | Precision=11.21 | Recall=23.27 | EM=1.00\n",
      "  Method: bm25 | k=3\n",
      "  BM25: k1=0.6, b=0.4\n",
      "  Passages: window=100.0, overlap=30.0, max=3.0\n",
      "\n",
      "Rank #4\n",
      "  Config: method=qld__k=3__mu=500__mode=passages__win=150__ovlp=30__maxp=5__temp=0.1\n",
      "  F1=13.17 | Precision=11.05 | Recall=24.40 | EM=1.00\n",
      "  Method: qld | k=3\n",
      "  QLD: mu=500.0\n",
      "  Passages: window=150.0, overlap=30.0, max=5.0\n",
      "\n",
      "Rank #5\n",
      "  Config: method=qld__k=5__mu=2000__mode=passages__win=100__ovlp=50__maxp=3__temp=0.1\n",
      "  F1=13.08 | Precision=10.90 | Recall=21.65 | EM=2.00\n",
      "  Method: qld | k=5\n",
      "  QLD: mu=2000.0\n",
      "  Passages: window=100.0, overlap=50.0, max=3.0\n",
      "\n",
      "Rank #6\n",
      "  Config: method=bm25__k=3__k1=0.6__b=0.4__mode=passages__win=100__ovlp=50__maxp=3__temp=0.1\n",
      "  F1=12.51 | Precision=10.94 | Recall=19.87 | EM=1.00\n",
      "  Method: bm25 | k=3\n",
      "  BM25: k1=0.6, b=0.4\n",
      "  Passages: window=100.0, overlap=50.0, max=3.0\n",
      "\n",
      "Rank #7\n",
      "  Config: method=qld__k=3__mu=2000__mode=passages__win=150__ovlp=30__maxp=5__temp=0.1\n",
      "  F1=12.32 | Precision=10.73 | Recall=20.73 | EM=2.00\n",
      "  Method: qld | k=3\n",
      "  QLD: mu=2000.0\n",
      "  Passages: window=150.0, overlap=30.0, max=5.0\n",
      "\n",
      "Rank #8\n",
      "  Config: method=qld__k=3__mu=500__mode=passages__win=100__ovlp=50__maxp=3__temp=0.1\n",
      "  F1=12.30 | Precision=10.85 | Recall=18.18 | EM=1.00\n",
      "  Method: qld | k=3\n",
      "  QLD: mu=500.0\n",
      "  Passages: window=100.0, overlap=50.0, max=3.0\n",
      "\n",
      "Rank #9\n",
      "  Config: method=qld__k=5__mu=2000__mode=passages__win=150__ovlp=30__maxp=3__temp=0.1\n",
      "  F1=12.27 | Precision=10.71 | Recall=21.23 | EM=1.00\n",
      "  Method: qld | k=5\n",
      "  QLD: mu=2000.0\n",
      "  Passages: window=150.0, overlap=30.0, max=3.0\n",
      "\n",
      "Rank #10\n",
      "  Config: method=qld__k=3__mu=1000__mode=docs__chars=300__temp=0.1\n",
      "  F1=12.09 | Precision=9.68 | Recall=19.73 | EM=0.00\n",
      "  Method: qld | k=3\n",
      "  QLD: mu=1000.0\n",
      "  Documents: max_chars=300.0\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Method comparison:\n",
      "            mean       std        max       min\n",
      "method                                         \n",
      "bm25    8.448665  2.642597  15.217744  4.248485\n",
      "qld     9.381603  1.637083  13.672420  5.239722\n",
      "\n",
      "Document vs Passage mode:\n",
      "                  mean       std        max       min\n",
      "use_passages                                         \n",
      "False         8.371886  2.003556  12.085592  4.248485\n",
      "True          9.661525  1.692017  15.217744  6.507699\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze results\n",
    "df_results = pd.read_csv(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "print(f\"Total configurations tested: {len(df_results)}\")\n",
    "print(f\"\\nTop 10 configurations by F1 Score:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_top = df_results.nlargest(10, 'f1_score')\n",
    "for idx, row in df_top.iterrows():\n",
    "    print(f\"\\nRank #{list(df_top.index).index(idx) + 1}\")\n",
    "    print(f\"  Config: {row['config_key']}\")\n",
    "    print(f\"  F1={row['f1_score']:.2f} | Precision={row['precision']:.2f} | Recall={row['recall']:.2f} | EM={row['exact_match']:.2f}\")\n",
    "    print(f\"  Method: {row['method']} | k={row['k']}\")\n",
    "    if row['method'] == 'qld':\n",
    "        print(f\"  QLD: mu={row['mu']}\")\n",
    "    elif row['method'] == 'bm25':\n",
    "        print(f\"  BM25: k1={row['k1']}, b={row['b']}\")\n",
    "    if row['use_passages']:\n",
    "        print(f\"  Passages: window={row['window']}, overlap={row['overlap']}, max={row['max_passages']}\")\n",
    "    else:\n",
    "        print(f\"  Documents: max_chars={row['max_chars']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nMethod comparison:\")\n",
    "print(df_results.groupby('method')['f1_score'].agg(['mean', 'std', 'max', 'min']))\n",
    "\n",
    "print(\"\\nDocument vs Passage mode:\")\n",
    "print(df_results.groupby('use_passages')['f1_score'].agg(['mean', 'std', 'max', 'min']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
