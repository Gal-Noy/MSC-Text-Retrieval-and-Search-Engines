{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80249d88",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660a3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Literal\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79df6",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38133d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b4cf0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"21.0.9\" 2025-10-21\n",
      "OpenJDK Runtime Environment (build 21.0.9+10-Ubuntu-122.04)\n",
      "OpenJDK 64-Bit Server VM (build 21.0.9+10-Ubuntu-122.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4783a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff9235",
   "metadata": {},
   "source": [
    "### Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f1afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged into Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(os.getenv('HUGGING_FACE_TOKEN'))\n",
    "print(\"✓ Logged into Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a17cc",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb3486b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3778 questions\n",
      "Test set: 2032 questions\n",
      "\n",
      "Sample question: what is the name of justin bieber brother?\n",
      "Sample answers: ['Jazmyn Bieber', 'Jaxon Bieber']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(\"./data/train.csv\", converters={\"answers\": json.loads})\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(f\"Train set: {len(df_train)} questions\")\n",
    "print(f\"Test set: {len(df_test)} questions\")\n",
    "print(f\"\\nSample question: {df_train.iloc[0]['question']}\")\n",
    "print(f\"Sample answers: {df_train.iloc[0]['answers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14492e1",
   "metadata": {},
   "source": [
    "## 3. Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef07078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-12-14 07:55:53.864584185 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pyserini index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dec 14, 2025 7:55:54 AM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n",
      "✓ Index loaded: 5903530 documents\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "# Load Pyserini index\n",
    "print(\"Loading Pyserini index...\")\n",
    "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "\n",
    "print(f\"✓ Index loaded: {index_reader.stats()['documents']} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d922144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bi-encoder...\n",
      "✓ Bi-encoder loaded\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load bi-encoder\n",
    "print(\"Loading bi-encoder...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bi_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "print(\"✓ Bi-encoder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0537dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=5 | window=150, overlap=50\n",
      "1. the film, has denied that Rowling ever saw it before writing her book. Rowling has said on record mu...\n",
      "2. to resolve an ongoing feud between the organisation's northern and southern branches that had sapped...\n",
      "3. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "4. Harry Potter (character) Harry James Potter is the titular protagonist of J. K. Rowling's \"Harry Pot...\n",
      "5. by Emily Brontë, \"Charlie and the Chocolate Factory\" by Roald Dahl, \"Robinson Crusoe\" by Daniel Defo...\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from functools import lru_cache\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_doc_content(docid: str) -> str:\n",
    "    \"\"\"Cache document content extraction.\"\"\"\n",
    "    try:\n",
    "        doc = searcher.doc(docid)\n",
    "        return json.loads(doc.raw()).get(\"contents\", \"\").replace(\"\\n\", \" \")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    \n",
    "@dataclass\n",
    "class RetrievalManager:\n",
    "    \"\"\"\n",
    "    Manages passage-based hybrid retrieval with RRF fusion and bi-encoder reranking.\n",
    "    \"\"\"\n",
    "    k_docs: int = 10\n",
    "    k_passages: int = 5\n",
    "    method: Literal['qld', 'bm25'] = 'hybrid'\n",
    "    mu: int = 1000           # QLD smoothing\n",
    "    k1: float = 0.9          # BM25\n",
    "    b: float = 0.4           # BM25\n",
    "    window: int = 150\n",
    "    overlap: int = 50\n",
    "    min_passage_words: int = 30\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Retrieval(RRF_k={self.rrf_k}, μ={self.mu}, k1={self.k1}, b={self.b}) | \"\n",
    "            f\"k_docs={self.k_docs}, k_passages={self.k_passages} | \"\n",
    "            f\"window={self.window}, overlap={self.overlap}\"\n",
    "        )\n",
    "    \n",
    "    def extract_passages(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into overlapping word windows.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        words = text.split()\n",
    "        if len(words) < self.min_passage_words:\n",
    "            return []\n",
    "        \n",
    "        step = max(1, self.window - self.overlap)\n",
    "        passages = []\n",
    "        \n",
    "        for i in range(0, len(words), step):\n",
    "            chunk = words[i:i + self.window]\n",
    "            if len(chunk) < self.min_passage_words:\n",
    "                break\n",
    "            passages.append(\" \".join(chunk))\n",
    "        \n",
    "        return passages\n",
    "    \n",
    "    def rerank(self, query: str, passages: List[str]) -> List[str]:\n",
    "        \"\"\"Rerank passages using bi-encoder with deduplication.\"\"\"\n",
    "        if not passages:\n",
    "            return []\n",
    "        \n",
    "        # Deduplicate exact matches\n",
    "        seen = set()\n",
    "        unique_passages = []\n",
    "        for p in passages:\n",
    "            if p not in seen:\n",
    "                seen.add(p)\n",
    "                unique_passages.append(p)\n",
    "        \n",
    "        if not unique_passages:\n",
    "            return []\n",
    "        \n",
    "        # Bi-encoder reranking\n",
    "        q_emb = bi_encoder.encode(query, convert_to_tensor=True, device=device)\n",
    "        p_embs = bi_encoder.encode(unique_passages, convert_to_tensor=True, device=device)\n",
    "        scores = util.cos_sim(q_emb, p_embs).squeeze(0)\n",
    "        \n",
    "        top_k = min(self.k_passages, len(unique_passages))\n",
    "        idx = torch.topk(scores, k=top_k).indices.tolist()\n",
    "        \n",
    "        return [unique_passages[i] for i in idx]\n",
    "    \n",
    "    def retrieve_context(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieve passages using Reciprocal Rank Fusion (RRF) + bi-encoder reranking.\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # QLD retrieval\n",
    "        searcher.set_qld(self.mu)\n",
    "        qld_hits = searcher.search(query, self.k_docs)\n",
    "        for rank, hit in enumerate(qld_hits):\n",
    "            scores[hit.docid] = scores.get(hit.docid, 0.0) + 1.0 / (self.rrf_k + rank + 1)\n",
    "        \n",
    "        # BM25 retrieval\n",
    "        searcher.set_bm25(self.k1, self.b)\n",
    "        bm25_hits = searcher.search(query, self.k_docs)\n",
    "        for rank, hit in enumerate(bm25_hits):\n",
    "            scores[hit.docid] = scores.get(hit.docid, 0.0) + 1.0 / (self.rrf_k + rank + 1)\n",
    "        \n",
    "        # Top-k documents by RRF score\n",
    "        top_docids = sorted(scores, key=scores.get, reverse=True)[:self.k_docs]\n",
    "        \n",
    "        # Extract passages from top documents\n",
    "        passages = []\n",
    "        for docid in top_docids:\n",
    "            content = get_doc_content(docid)\n",
    "            if content:\n",
    "                passages.extend(self.extract_passages(content))\n",
    "        \n",
    "        return self.rerank(query, passages)\n",
    "\n",
    "\n",
    "# Test the RetrievalManager\n",
    "query = \"Who wrote Harry Potter?\"\n",
    "    \n",
    "rm = RetrievalManager()\n",
    "\n",
    "print(rm)\n",
    "passages = rm.retrieve_context(query)\n",
    "\n",
    "for i, p in enumerate(passages, 1):\n",
    "    print(f\"{i}. {p[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e849f6",
   "metadata": {},
   "source": [
    "## 4. LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4b616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model...\n",
      "✓ Model loaded on: GPU\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Suppress transformers warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Loading LLM model...\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Set pad_token for batch processing\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "\n",
    "print(f\"✓ Model loaded on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97ae5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: temp=0.0, top_p=1.0, max_tokens=256\n",
      "✓ Generated answer: 'J.K. Rowling'\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You must respond based strictly on the information in provided passages.\"\n",
    "    \"Do not incorporate any external knowledge or infer any details beyond what is given.\"\n",
    "    \"If the answer is not in the context, return 'I dont know'.\"\n",
    "    \"Do not include explanations, only the final answer!\"\n",
    ")\n",
    "\n",
    "DEFAULT_USER_PROMPT = (\n",
    "    \"Based on the following documents, provide a concise answer to the question.\\n\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class PromptManager:\n",
    "    \"\"\"Manages prompt generation and LLM answer generation.\"\"\"\n",
    "    system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    "    user_prompt: str = DEFAULT_USER_PROMPT\n",
    "    temperature: float = 0.0\n",
    "    top_p: float = 1.0\n",
    "    max_new_tokens: int = 256\n",
    "    do_sample: bool = False\n",
    "    prompt_id: str = \"default\"  # For later use in prompt tuning\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"temp={self.temperature}, top_p={self.top_p}, max_tokens={self.max_new_tokens}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_answer(answer: str) -> str:\n",
    "        \"\"\"Clean and standardize the generated answer.\"\"\"\n",
    "        answer = re.sub(r'^(Answer|The answer is|Based on the .*?,):?\\s*', '', answer, flags=re.I)\n",
    "        answer = answer.rstrip('.')\n",
    "        if any(phrase in answer.lower() for phrase in [\"dont know\", \"don't know\", \"do not know\", \"unknown\"]):\n",
    "            return \"unknown\"\n",
    "        return answer.strip()\n",
    "\n",
    "    def create_messages(self, question: str, contexts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Create messages for the LLM based on the question and contexts.\"\"\"\n",
    "        if not contexts:\n",
    "            context_str = \"No relevant documents found.\"\n",
    "        else:\n",
    "            context_str = '\\n\\n'.join([f\"Document {i+1}: {ctx}\" for i, ctx in enumerate(contexts)])\n",
    "        \n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": self.user_prompt.format(context=context_str, question=question)}\n",
    "        ]\n",
    "\n",
    "    def generate_answer(self, question: str, contexts: List[str]) -> str:\n",
    "        \"\"\"Generate an answer using the LLM based on the question and contexts.\"\"\"\n",
    "        messages = self.create_messages(question, contexts)\n",
    "        \n",
    "        outputs = pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=self.do_sample,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "        )\n",
    "        \n",
    "        answer = outputs[0][\"generated_text\"][-1].get('content', '')\n",
    "        return self.clean_answer(answer)\n",
    "\n",
    "    def batch_generate_answers(self, questions: List[str], contexts_list: List[List[str]]) -> List[str]:\n",
    "        \"\"\"Generate answers for multiple questions in batch.\"\"\"\n",
    "        # Create messages for all questions\n",
    "        batch_messages = [self.create_messages(q, ctx) for q, ctx in zip(questions, contexts_list)]\n",
    "        \n",
    "        # Process batch through pipeline\n",
    "        outputs = pipeline(\n",
    "            batch_messages,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=self.do_sample,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p\n",
    "        )\n",
    "        \n",
    "        # Extract and clean answers\n",
    "        answers = []\n",
    "        for output in outputs:\n",
    "            answer = output[0][\"generated_text\"][-1].get('content', '')\n",
    "            answers.append(self.clean_answer(answer))\n",
    "        \n",
    "        return answers\n",
    "\n",
    "\n",
    "# Test the PromptManager\n",
    "test_prompt_manager = PromptManager()\n",
    "print(f\"Testing: {test_prompt_manager}\")\n",
    "test_answer = test_prompt_manager.generate_answer(query, passages)\n",
    "print(f\"✓ Generated answer: '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873c1ef",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9b4c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation test: F1=66.67, P=66.67, R=66.67, EM=66.67\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer for comparison\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_token_metrics(prediction: str, ground_truth: str) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score for token-level comparison.\n",
    "    Returns: (precision, recall, f1)\n",
    "    \"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    # Handle empty cases\n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        match = int(pred_tokens == gt_tokens)\n",
    "        return match, match, match\n",
    "    \n",
    "    # Compute overlap\n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate_predictions(df_gold: pd.DataFrame, predictions: Dict[int, str]) -> Dict:\n",
    "    \"\"\"Evaluate predictions against ground truth.\"\"\"\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    exact_matches = []\n",
    "    \n",
    "    for _, row in df_gold.iterrows():\n",
    "        qid = row['id']\n",
    "        \n",
    "        # Handle missing predictions\n",
    "        if qid not in predictions:\n",
    "            f1_scores.append(0.0)\n",
    "            precision_scores.append(0.0)\n",
    "            recall_scores.append(0.0)\n",
    "            exact_matches.append(0)\n",
    "            continue\n",
    "        \n",
    "        prediction = predictions[qid]\n",
    "        ground_truths = row['answers']\n",
    "        \n",
    "        # Normalize once\n",
    "        norm_prediction = normalize_answer(prediction)\n",
    "        \n",
    "        # Find best match across all ground truths\n",
    "        best_f1 = 0.0\n",
    "        best_precision = 0.0\n",
    "        best_recall = 0.0\n",
    "        is_exact = 0\n",
    "        \n",
    "        for gt in ground_truths:\n",
    "            norm_gt = normalize_answer(gt)\n",
    "            \n",
    "            # Compute metrics\n",
    "            prec, rec, f1 = compute_token_metrics(prediction, gt)\n",
    "            \n",
    "            # Track best scores\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_precision = prec\n",
    "                best_recall = rec\n",
    "            \n",
    "            # Check exact match\n",
    "            if norm_prediction == norm_gt:\n",
    "                is_exact = 1\n",
    "        \n",
    "        f1_scores.append(best_f1)\n",
    "        precision_scores.append(best_precision)\n",
    "        recall_scores.append(best_recall)\n",
    "        exact_matches.append(is_exact)\n",
    "    \n",
    "    return {\n",
    "        'f1': 100.0 * sum(f1_scores) / len(f1_scores) if f1_scores else 0.0,\n",
    "        'precision': 100.0 * sum(precision_scores) / len(precision_scores) if precision_scores else 0.0,\n",
    "        'recall': 100.0 * sum(recall_scores) / len(recall_scores) if recall_scores else 0.0,\n",
    "        'exact_match': 100.0 * sum(exact_matches) / len(exact_matches) if exact_matches else 0.0,\n",
    "        'f1_scores': f1_scores,\n",
    "        'precision_scores': precision_scores,\n",
    "        'recall_scores': recall_scores,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "test_predictions = {1: \"J.K. Rowling\", 2: \"Paris\", 3: \"Shakespeare\"}\n",
    "test_gold = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'answers': [[\"J.K. Rowling\", \"Rowling\"], [\"Earth\"], [\"William Shakespeare\", \"Shakespeare\"]]\n",
    "})\n",
    "\n",
    "test_metrics = evaluate_predictions(test_gold, test_predictions)\n",
    "print(f\"✓ Evaluation test: F1={test_metrics['f1']:.2f}, P={test_metrics['precision']:.2f}, R={test_metrics['recall']:.2f}, EM={test_metrics['exact_match']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956e446",
   "metadata": {},
   "source": [
    "## 6. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "721292c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing experiment with:\n",
      "  Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=5 | window=150, overlap=50\n",
      "  Prompt: temp=0.0, top_p=1.0, max_tokens=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quick Test: 100%|██████████| 2/2 [00:05<00:00,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick Test\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=5 | window=150, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=22.86 | P=21.67 | R=30.00 | EM=20.00\n",
      "   Questions: 5\n",
      "\n",
      "✓ Experiment framework ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    name: str,\n",
    "    df_data: pd.DataFrame,\n",
    "    retrieval_manager: RetrievalManager,\n",
    "    prompt_manager: PromptManager,\n",
    "    max_questions: Optional[int] = None,\n",
    "    batch_size: int = 4,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    if max_questions:\n",
    "        df_data = df_data.head(max_questions)\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Process in batches\n",
    "    num_batches = (len(df_data) + batch_size - 1) // batch_size\n",
    "    iterator = tqdm(range(num_batches), desc=name) if verbose else range(num_batches)\n",
    "    \n",
    "    for batch_idx in iterator:\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(df_data))\n",
    "        batch_df = df_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Retrieve contexts for all questions in batch\n",
    "        batch_questions = []\n",
    "        batch_qids = []\n",
    "        batch_contexts = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            question = row['question']\n",
    "            qid = row['id']\n",
    "            contexts = retrieval_manager.retrieve_context(question)\n",
    "            \n",
    "            batch_questions.append(question)\n",
    "            batch_qids.append(qid)\n",
    "            batch_contexts.append(contexts)\n",
    "        \n",
    "        # Generate answers in batch\n",
    "        batch_answers = prompt_manager.batch_generate_answers(batch_questions, batch_contexts)\n",
    "        \n",
    "        # Store predictions\n",
    "        for qid, answer in zip(batch_qids, batch_answers):\n",
    "            predictions[qid] = answer\n",
    "    \n",
    "    metrics = evaluate_predictions(df_data, predictions)\n",
    "    \n",
    "    result = {\n",
    "        'name': name,\n",
    "        'retrieval': retrieval_manager,\n",
    "        'prompt': prompt_manager,\n",
    "        'f1_score': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'exact_match': metrics['exact_match'],\n",
    "        'num_questions': len(df_data),\n",
    "        'predictions': predictions,\n",
    "        'f1_scores': metrics['f1_scores'],\n",
    "        'precision_scores': metrics['precision_scores'],\n",
    "        'recall_scores': metrics['recall_scores'],\n",
    "        'exact_matches': metrics['exact_matches']\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"   Retrieval: {retrieval_manager}\")\n",
    "        print(f\"   Prompt: {prompt_manager}\")\n",
    "        print(f\"   F1={metrics['f1']:.2f} | P={metrics['precision']:.2f} | R={metrics['recall']:.2f} | EM={metrics['exact_match']:.2f}\")\n",
    "        print(f\"   Questions: {len(df_data)}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test experiment\n",
    "test_retrieval = RetrievalManager()\n",
    "test_prompt = PromptManager()\n",
    "print(f\"Testing experiment with:\")\n",
    "print(f\"  Retrieval: {test_retrieval}\")\n",
    "print(f\"  Prompt: {test_prompt}\")\n",
    "\n",
    "test_exp = run_experiment(\n",
    "    \"Quick Test\",\n",
    "    df_train.head(5),\n",
    "    test_retrieval,\n",
    "    test_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Experiment framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78eead",
   "metadata": {},
   "source": [
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54f379",
   "metadata": {},
   "source": [
    "### Experiments global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac8c7605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASED RETRIEVAL + GENERATION EXPERIMENT FRAMEWORK\n",
      "================================================================================\n",
      "Validation questions per config: 100\n",
      "Random seed: 42\n",
      "Results cache: ./results/grid_search_results_q100.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_SEED = 42\n",
    "EXPERIMENT_QUESTIONS = 100\n",
    "\n",
    "EXPERIMENT_LOG_PATH = (\n",
    "    f\"./results/grid_search_results_q{EXPERIMENT_QUESTIONS}.csv\"\n",
    ")\n",
    "\n",
    "validation_data = df_train.sample(\n",
    "    n=EXPERIMENT_QUESTIONS,\n",
    "    random_state=EXPERIMENT_SEED\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASED RETRIEVAL + GENERATION EXPERIMENT FRAMEWORK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Validation questions per config: {EXPERIMENT_QUESTIONS}\")\n",
    "print(f\"Random seed: {EXPERIMENT_SEED}\")\n",
    "print(f\"Results cache: {EXPERIMENT_LOG_PATH}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9a89e",
   "metadata": {},
   "source": [
    "### Experiments utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4d8b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_retrieval_manager(base: dict, override: dict) -> RetrievalManager:\n",
    "    \"\"\"Build RetrievalManager safely.\"\"\"\n",
    "    return RetrievalManager(**{**base, **override})\n",
    "\n",
    "\n",
    "def generate_config_key(\n",
    "    retrieval_mgr: RetrievalManager,\n",
    "    prompt_mgr: PromptManager,\n",
    ") -> str:\n",
    "    \"\"\"Generate unique config key for RRF-based retrieval.\"\"\"\n",
    "    return (\n",
    "        f\"RRF_k{retrieval_mgr.rrf_k}_\"\n",
    "        f\"mu{retrieval_mgr.mu}_\"\n",
    "        f\"k1{retrieval_mgr.k1}_b{retrieval_mgr.b}_\"\n",
    "        f\"kdocs{retrieval_mgr.k_docs}_\"\n",
    "        f\"kpass{retrieval_mgr.k_passages}_\"\n",
    "        f\"win{retrieval_mgr.window}_ovl{retrieval_mgr.overlap}_\"\n",
    "        f\"prompt{prompt_mgr.prompt_id}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def save_results_to_csv(result: dict, key: str, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    row = {\n",
    "        \"config_key\": key,\n",
    "        \"f1\": result[\"f1_score\"],\n",
    "        \"precision\": result[\"precision\"],\n",
    "        \"recall\": result[\"recall\"],\n",
    "        \"exact_match\": result[\"exact_match\"],\n",
    "        \"num_questions\": result[\"num_questions\"],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([row])\n",
    "    if not os.path.exists(path):\n",
    "        df.to_csv(path, index=False)\n",
    "    else:\n",
    "        df.to_csv(path, mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "def load_completed_configs(path: str) -> set[str]:\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    return set(pd.read_csv(path)[\"config_key\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb074e07",
   "metadata": {},
   "source": [
    "### Best-config selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "987015b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k_configs(\n",
    "    retrieval_managers: list[RetrievalManager],\n",
    "    prompt_managers: list[PromptManager],\n",
    "    *,\n",
    "    top_k: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return the top-K configurations by validation F1 score.\n",
    "    Always sorted by descending F1.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    scored_entries = []\n",
    "\n",
    "    for r_mgr, p_mgr in zip(retrieval_managers, prompt_managers):\n",
    "        key = generate_config_key(r_mgr, p_mgr)\n",
    "        row = df[df[\"config_key\"] == key]\n",
    "        if row.empty:\n",
    "            continue\n",
    "\n",
    "        scored_entries.append({\n",
    "            \"retrieval_mgr\": r_mgr,\n",
    "            \"prompt_mgr\": p_mgr,\n",
    "            \"f1\": float(row.iloc[0][\"f1\"]),\n",
    "            \"config_key\": key,\n",
    "        })\n",
    "\n",
    "    # Sort before slicing\n",
    "    scored_entries.sort(\n",
    "        key=lambda x: (x[\"f1\"], x[\"config_key\"]),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    return scored_entries[:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f416f",
   "metadata": {},
   "source": [
    "### Phase runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "827db996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phase(\n",
    "    *,\n",
    "    phase_name: str,\n",
    "    grid: list[dict],\n",
    "    validation_data,\n",
    "    top_k: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a single experiment phase.\n",
    "\n",
    "    Each grid item must contain:\n",
    "      - retrieval_mgr: RetrievalManager\n",
    "      - prompt_mgr: PromptManager\n",
    "\n",
    "    Returns:\n",
    "      - top-K configs sorted by F1 (if top_k is provided)\n",
    "      - otherwise, the full grid sorted by F1\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(phase_name)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    completed = load_completed_configs(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    pending = [\n",
    "        g for g in grid\n",
    "        if generate_config_key(g[\"retrieval_mgr\"], g[\"prompt_mgr\"]) not in completed\n",
    "    ]\n",
    "\n",
    "    print(f\"Total configs: {len(grid)}\")\n",
    "    print(f\"Completed configs: {len(grid) - len(pending)}\")\n",
    "    print(f\"Pending configs: {len(pending)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, entry in enumerate(pending, start=1):\n",
    "        retrieval_mgr = entry[\"retrieval_mgr\"]\n",
    "        prompt_mgr = entry[\"prompt_mgr\"]\n",
    "\n",
    "        key = generate_config_key(retrieval_mgr, prompt_mgr)\n",
    "        print(f\"[{i}/{len(pending)}] Running: {key}\")\n",
    "\n",
    "        result = run_experiment(\n",
    "            name=key,\n",
    "            df_data=validation_data,\n",
    "            retrieval_manager=retrieval_mgr,\n",
    "            prompt_manager=prompt_mgr,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        save_results_to_csv(result, key, EXPERIMENT_LOG_PATH)\n",
    "        print(f\"✓ F1={result['f1_score']:.4f}\")\n",
    "\n",
    "    # Load results once for consistent sorting\n",
    "    df = pd.read_csv(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    def get_f1(entry):\n",
    "        key = generate_config_key(entry[\"retrieval_mgr\"], entry[\"prompt_mgr\"])\n",
    "        row = df[df[\"config_key\"] == key]\n",
    "        return float(row.iloc[0][\"f1\"]) if not row.empty else -1.0\n",
    "\n",
    "    # Sort full grid by F1\n",
    "    sorted_grid = sorted(\n",
    "        grid,\n",
    "        key=lambda g: (get_f1(g), generate_config_key(g[\"retrieval_mgr\"], g[\"prompt_mgr\"])),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    if top_k is None:\n",
    "        return sorted_grid\n",
    "\n",
    "    top_configs = select_top_k_configs(\n",
    "        [g[\"retrieval_mgr\"] for g in sorted_grid],\n",
    "        [g[\"prompt_mgr\"] for g in sorted_grid],\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop configs selected:\")\n",
    "    for i, entry in enumerate(top_configs, 1):\n",
    "        print(\n",
    "            f\"{i}. {entry['config_key']} | \"\n",
    "            f\"F1={entry['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "    return top_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03cdf5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 1 grid size: 12\n",
      "\n",
      "================================================================================\n",
      "PHASE 1 — Passage Segmentation\n",
      "================================================================================\n",
      "Total configs: 12\n",
      "Completed configs: 12\n",
      "Pending configs: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top configs selected:\n",
      "1. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win150_ovl50_promptdefault | F1=22.5510\n",
      "2. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win250_ovl50_promptdefault | F1=21.7384\n",
      "3. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win200_ovl25_promptdefault | F1=20.4083\n",
      "4. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win250_ovl75_promptdefault | F1=20.1787\n",
      "5. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win150_ovl75_promptdefault | F1=18.9896\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 1 — Passage Segmentation\n",
    "# ============================================================\n",
    "\n",
    "PHASE_1_GRID = []\n",
    "\n",
    "BASE_RETRIEVAL_PARAMS = {\n",
    "    \"k_docs\": 10,       \n",
    "    \"k_passages\": 5,\n",
    "    \"mu\": 1000,\n",
    "    \"k1\": 0.9,\n",
    "    \"b\": 0.4,\n",
    "}\n",
    "\n",
    "for window in [100, 150, 200, 250]:\n",
    "    for overlap in [25, 50, 75]:\n",
    "        PHASE_1_GRID.append({\n",
    "            \"retrieval_mgr\": RetrievalManager(\n",
    "                window=window,\n",
    "                overlap=overlap,\n",
    "                **BASE_RETRIEVAL_PARAMS,\n",
    "            ),\n",
    "            \"prompt_mgr\": PromptManager(),\n",
    "        })\n",
    "\n",
    "print(f\"✓ Phase 1 grid size: {len(PHASE_1_GRID)}\")\n",
    "\n",
    "PHASE_1_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 1 — Passage Segmentation\",\n",
    "    grid=PHASE_1_GRID,\n",
    "    validation_data=validation_data,\n",
    "    top_k=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86e95ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 2 grid size: 60\n",
      "\n",
      "================================================================================\n",
      "PHASE 2 — Retrieval Capacity\n",
      "================================================================================\n",
      "Total configs: 60\n",
      "Completed configs: 36\n",
      "Pending configs: 24\n",
      "--------------------------------------------------------------------------------\n",
      "[1/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass5_win200_ovl25_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass5_win200_ovl25_promptdefault: 100%|██████████| 25/25 [02:26<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass5_win200_ovl25_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=20, k_passages=5 | window=200, overlap=25\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=19.61 | P=20.17 | R=29.02 | EM=7.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=19.6068\n",
      "[2/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass7_win200_ovl25_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass7_win200_ovl25_promptdefault: 100%|██████████| 25/25 [02:42<00:00,  6.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass7_win200_ovl25_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=20, k_passages=7 | window=200, overlap=25\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=18.11 | P=17.89 | R=28.29 | EM=7.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=18.1083\n",
      "[3/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass3_win250_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass3_win250_ovl75_promptdefault: 100%|██████████| 25/25 [01:10<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass3_win250_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=5, k_passages=3 | window=250, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=16.14 | P=16.36 | R=23.55 | EM=4.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=16.1444\n",
      "[4/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass5_win250_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass5_win250_ovl75_promptdefault: 100%|██████████| 25/25 [01:32<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass5_win250_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=5, k_passages=5 | window=250, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=18.93 | P=18.49 | R=31.82 | EM=6.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=18.9258\n",
      "[5/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass7_win250_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass7_win250_ovl75_promptdefault: 100%|██████████| 25/25 [01:38<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass7_win250_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=5, k_passages=7 | window=250, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=9.49 | P=9.58 | R=15.00 | EM=4.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=9.4924\n",
      "[6/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass3_win250_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass3_win250_ovl75_promptdefault: 100%|██████████| 25/25 [01:38<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass3_win250_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=3 | window=250, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=17.38 | P=17.69 | R=26.55 | EM=4.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=17.3849\n",
      "[7/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass7_win250_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass7_win250_ovl75_promptdefault: 100%|██████████| 25/25 [01:54<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass7_win250_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=7 | window=250, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=12.39 | P=12.40 | R=17.70 | EM=6.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=12.3879\n",
      "[8/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass3_win250_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass3_win250_ovl75_promptdefault: 100%|██████████| 25/25 [01:57<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass3_win250_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=3 | window=250, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=17.86 | P=18.33 | R=27.55 | EM=4.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=17.8576\n",
      "[9/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win250_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win250_ovl75_promptdefault: 100%|██████████| 25/25 [02:12<00:00,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win250_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=5 | window=250, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=18.97 | P=19.82 | R=28.55 | EM=7.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=18.9684\n",
      "[10/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win250_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win250_ovl75_promptdefault: 100%|██████████| 25/25 [02:14<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win250_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=7 | window=250, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=13.97 | P=14.42 | R=19.93 | EM=5.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=13.9743\n",
      "[11/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass3_win250_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass3_win250_ovl75_promptdefault: 100%|██████████| 25/25 [02:13<00:00,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass3_win250_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=20, k_passages=3 | window=250, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=19.36 | P=19.49 | R=30.30 | EM=5.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=19.3639\n",
      "[12/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass5_win250_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass5_win250_ovl75_promptdefault: 100%|██████████| 25/25 [02:30<00:00,  6.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass5_win250_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=20, k_passages=5 | window=250, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=20.00 | P=20.70 | R=29.80 | EM=8.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=20.0019\n",
      "[13/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass7_win250_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass7_win250_ovl75_promptdefault: 100%|██████████| 25/25 [02:37<00:00,  6.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass7_win250_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=20, k_passages=7 | window=250, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=14.14 | P=14.01 | R=21.43 | EM=6.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=14.1372\n",
      "[14/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass3_win150_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass3_win150_ovl75_promptdefault: 100%|██████████| 25/25 [01:32<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass3_win150_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=5, k_passages=3 | window=150, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=12.08 | P=11.89 | R=19.70 | EM=5.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=12.0797\n",
      "[15/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass5_win150_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass5_win150_ovl75_promptdefault: 100%|██████████| 25/25 [01:49<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass5_win150_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=5, k_passages=5 | window=150, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=17.16 | P=17.76 | R=25.38 | EM=7.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=17.1553\n",
      "[16/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass7_win150_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass7_win150_ovl75_promptdefault: 100%|██████████| 25/25 [01:54<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs5_kpass7_win150_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=5, k_passages=7 | window=150, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=18.03 | P=17.92 | R=27.84 | EM=7.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=18.0282\n",
      "[17/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass3_win150_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass3_win150_ovl75_promptdefault: 100%|██████████| 25/25 [02:16<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass3_win150_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=3 | window=150, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=12.57 | P=12.66 | R=20.63 | EM=5.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=12.5740\n",
      "[18/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass7_win150_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass7_win150_ovl75_promptdefault: 100%|██████████| 25/25 [02:33<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass7_win150_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=7 | window=150, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=19.68 | P=20.65 | R=27.29 | EM=8.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=19.6778\n",
      "[19/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass3_win150_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass3_win150_ovl75_promptdefault: 100%|██████████| 25/25 [02:55<00:00,  7.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass3_win150_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=3 | window=150, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=12.06 | P=12.42 | R=18.38 | EM=4.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=12.0572\n",
      "[20/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win150_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win150_ovl75_promptdefault: 100%|██████████| 25/25 [03:08<00:00,  7.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win150_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=5 | window=150, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=15.28 | P=16.50 | R=23.23 | EM=5.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=15.2842\n",
      "[21/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win150_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win150_ovl75_promptdefault: 100%|██████████| 25/25 [03:17<00:00,  7.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win150_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=7 | window=150, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=19.73 | P=21.46 | R=28.37 | EM=8.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=19.7349\n",
      "[22/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass3_win150_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass3_win150_ovl75_promptdefault: 100%|██████████| 25/25 [03:30<00:00,  8.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass3_win150_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=20, k_passages=3 | window=150, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=14.40 | P=15.05 | R=21.63 | EM=4.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=14.3962\n",
      "[23/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass5_win150_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass5_win150_ovl75_promptdefault: 100%|██████████| 25/25 [03:46<00:00,  9.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass5_win150_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=20, k_passages=5 | window=150, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=16.08 | P=16.88 | R=24.56 | EM=5.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=16.0772\n",
      "[24/24] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass7_win150_ovl75_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass7_win150_ovl75_promptdefault: 100%|██████████| 25/25 [03:58<00:00,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass7_win150_ovl75_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=20, k_passages=7 | window=150, overlap=75\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=16.30 | P=17.23 | R=24.87 | EM=7.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=16.3039\n",
      "\n",
      "Top configs selected:\n",
      "1. RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win150_ovl50_promptdefault | F1=22.7270\n",
      "2. RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass7_win150_ovl50_promptdefault | F1=22.6114\n",
      "3. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win150_ovl50_promptdefault | F1=22.5510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 2 — Retrieval Capacity\n",
    "# ============================================================\n",
    "\n",
    "PHASE_2_GRID = []\n",
    "\n",
    "for entry in PHASE_1_TOP_CONFIGS:\n",
    "    base_mgr = entry[\"retrieval_mgr\"]\n",
    "\n",
    "    for k_docs in [5, 10, 15, 20]:\n",
    "        for k_passages in [3, 5, 7]:\n",
    "            PHASE_2_GRID.append({\n",
    "                \"retrieval_mgr\": RetrievalManager(\n",
    "                    k_docs=k_docs,\n",
    "                    k_passages=k_passages,\n",
    "                    window=base_mgr.window,\n",
    "                    overlap=base_mgr.overlap,\n",
    "                    mu=base_mgr.mu,\n",
    "                    k1=base_mgr.k1,\n",
    "                    b=base_mgr.b,\n",
    "                ),\n",
    "                \"prompt_mgr\": PromptManager(),\n",
    "            })\n",
    "\n",
    "print(f\"✓ Phase 2 grid size: {len(PHASE_2_GRID)}\")\n",
    "\n",
    "PHASE_2_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 2 — Retrieval Capacity\",\n",
    "    grid=PHASE_2_GRID,\n",
    "    validation_data=validation_data,\n",
    "    top_k=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f47ad0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 3 grid size: 27\n",
      "\n",
      "================================================================================\n",
      "PHASE 3 — Lexical Hyperparameters\n",
      "================================================================================\n",
      "Total configs: 27\n",
      "Completed configs: 3\n",
      "Pending configs: 24\n",
      "--------------------------------------------------------------------------------\n",
      "[1/24] Running: RRF_k60_mu500_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu500_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu500_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault: 100%|██████████| 25/25 [02:30<00:00,  6.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu500_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=500, k1=0.6, b=0.3) | k_docs=15, k_passages=5 | window=150, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=19.09 | P=19.57 | R=29.17 | EM=7.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=19.0933\n",
      "[2/24] Running: RRF_k60_mu1000_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault:  40%|████      | 10/25 [01:22<02:03,  8.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 39\u001b[0m\n\u001b[1;32m     24\u001b[0m             PHASE_3_GRID\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     25\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieval_mgr\u001b[39m\u001b[38;5;124m\"\u001b[39m: RetrievalManager(\n\u001b[1;32m     26\u001b[0m                     k_docs\u001b[38;5;241m=\u001b[39mbase_mgr\u001b[38;5;241m.\u001b[39mk_docs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_mgr\u001b[39m\u001b[38;5;124m\"\u001b[39m: PromptManager(),\n\u001b[1;32m     35\u001b[0m             })\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Phase 3 grid size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(PHASE_3_GRID)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m PHASE_3_TOP_CONFIGS \u001b[38;5;241m=\u001b[39m \u001b[43mrun_phase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPHASE 3 — Lexical Hyperparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPHASE_3_GRID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 42\u001b[0m, in \u001b[0;36mrun_phase\u001b[0;34m(phase_name, grid, validation_data, top_k)\u001b[0m\n\u001b[1;32m     39\u001b[0m key \u001b[38;5;241m=\u001b[39m generate_config_key(retrieval_mgr, prompt_mgr)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pending)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrieval_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretrieval_mgr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_mgr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m save_results_to_csv(result, key, EXPERIMENT_LOG_PATH)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 39\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(name, df_data, retrieval_manager, prompt_manager, max_questions, batch_size, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m     batch_contexts\u001b[38;5;241m.\u001b[39mappend(contexts)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Generate answers in batch\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m batch_answers \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_generate_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_contexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Store predictions\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qid, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_qids, batch_answers):\n",
      "Cell \u001b[0;32mIn[11], line 72\u001b[0m, in \u001b[0;36mPromptManager.batch_generate_answers\u001b[0;34m(self, questions, contexts_list)\u001b[0m\n\u001b[1;32m     69\u001b[0m batch_messages \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_messages(q, ctx) \u001b[38;5;28;01mfor\u001b[39;00m q, ctx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(questions, contexts_list)]\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Process batch through pipeline\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Extract and clean answers\u001b[39;00m\n\u001b[1;32m     82\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:331\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1448\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1445\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1446\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1447\u001b[0m     )\n\u001b[0;32m-> 1448\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:126\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:127\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1374\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1373\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1374\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:432\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    430\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 432\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    435\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2787\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2787\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   2790\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2791\u001b[0m     outputs,\n\u001b[1;32m   2792\u001b[0m     model_kwargs,\n\u001b[1;32m   2793\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2794\u001b[0m )\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:459\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    441\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:1072\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1072\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:395\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 395\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    407\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    408\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    409\u001b[0m )\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:308\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    307\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 308\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    310\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:65\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     63\u001b[0m input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     64\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 65\u001b[0m variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3 — Lexical Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "PHASE_3_GRID = []\n",
    "\n",
    "BM25_PARAMS = [\n",
    "    {\"k1\": 0.6, \"b\": 0.3},\n",
    "    {\"k1\": 0.9, \"b\": 0.4},   # baseline\n",
    "    {\"k1\": 1.2, \"b\": 0.6},\n",
    "]\n",
    "\n",
    "QLD_PARAMS = [\n",
    "    {\"mu\": 500},\n",
    "    {\"mu\": 1000},           # baseline\n",
    "    {\"mu\": 2000},\n",
    "]\n",
    "\n",
    "for entry in PHASE_2_TOP_CONFIGS:\n",
    "    base_mgr = entry[\"retrieval_mgr\"]\n",
    "\n",
    "    for bm25 in BM25_PARAMS:\n",
    "        for qld in QLD_PARAMS:\n",
    "            PHASE_3_GRID.append({\n",
    "                \"retrieval_mgr\": RetrievalManager(\n",
    "                    k_docs=base_mgr.k_docs,\n",
    "                    k_passages=base_mgr.k_passages,\n",
    "                    window=base_mgr.window,\n",
    "                    overlap=base_mgr.overlap,\n",
    "                    k1=bm25[\"k1\"],\n",
    "                    b=bm25[\"b\"],\n",
    "                    mu=qld[\"mu\"],\n",
    "                ),\n",
    "                \"prompt_mgr\": PromptManager(),\n",
    "            })\n",
    "\n",
    "print(f\"✓ Phase 3 grid size: {len(PHASE_3_GRID)}\")\n",
    "\n",
    "PHASE_3_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 3 — Lexical Hyperparameters\",\n",
    "    grid=PHASE_3_GRID,\n",
    "    validation_data=validation_data,\n",
    "    top_k=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a904e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SELECTION — Top 3 configs on 1,000 TRAIN questions\n",
    "# ============================================================\n",
    "\n",
    "FINAL_SELECTION_SEED = 123\n",
    "FINAL_SELECTION_SIZE = 1000\n",
    "\n",
    "# Take top-3 configs from Phase 3 (already sorted by F1)\n",
    "TOP_3_CONFIGS = PHASE_3_TOP_CONFIGS[:3]\n",
    "\n",
    "final_validation_data = df_train.sample(\n",
    "    n=FINAL_SELECTION_SIZE,\n",
    "    random_state=FINAL_SELECTION_SEED,\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL MODEL SELECTION ON 1,000 TRAIN QUESTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "FINAL_SELECTION_RESULTS = []\n",
    "\n",
    "for i, entry in enumerate(TOP_3_CONFIGS, 1):\n",
    "    retrieval_mgr = entry[\"retrieval_mgr\"]\n",
    "\n",
    "    prompt_mgr = PromptManager(\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        user_prompt=DEFAULT_USER_PROMPT,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "        top_p=1.0,\n",
    "        prompt_id=\"final\",\n",
    "    )\n",
    "\n",
    "    config_key = generate_config_key(retrieval_mgr, prompt_mgr)\n",
    "\n",
    "    print(f\"\\n[{i}/3] Evaluating config: {config_key}\")\n",
    "\n",
    "    result = run_experiment(\n",
    "        name=f\"{config_key}_final_select\",\n",
    "        df_data=final_validation_data,\n",
    "        retrieval_manager=retrieval_mgr,\n",
    "        prompt_manager=prompt_mgr,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"✓ F1={result['f1_score']:.4f} | \"\n",
    "        f\"EM={result['exact_match']:.4f} | \"\n",
    "        f\"P={result['precision']:.4f} | \"\n",
    "        f\"R={result['recall']:.4f}\"\n",
    "    )\n",
    "\n",
    "    FINAL_SELECTION_RESULTS.append({\n",
    "        \"retrieval_mgr\": retrieval_mgr,\n",
    "        \"prompt_mgr\": prompt_mgr,\n",
    "        \"config_key\": config_key,\n",
    "        **result,\n",
    "    })\n",
    "\n",
    "# Select the best config by F1\n",
    "FINAL_SELECTION_RESULTS.sort(\n",
    "    key=lambda x: x[\"f1_score\"],\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "BEST_FINAL_CONFIG = FINAL_SELECTION_RESULTS[0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ BEST FINAL CONFIG SELECTED\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"{BEST_FINAL_CONFIG['config_key']} | \"\n",
    "    f\"F1={BEST_FINAL_CONFIG['f1_score']:.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KAGGLE SUBMISSION — Final system on TEST set\n",
    "# ============================================================\n",
    "\n",
    "BEST_RETRIEVAL_MGR = BEST_FINAL_CONFIG[\"retrieval_mgr\"]\n",
    "BEST_PROMPT_MGR = BEST_FINAL_CONFIG[\"prompt_mgr\"]\n",
    "\n",
    "FINAL_CONFIG_KEY = BEST_FINAL_CONFIG[\"config_key\"]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KAGGLE SUBMISSION GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Using final config: {FINAL_CONFIG_KEY}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run inference only (no labels needed)\n",
    "test_questions = df_test[\"question\"].tolist()\n",
    "\n",
    "print(f\"Generating answers for {len(test_questions)} test questions...\")\n",
    "\n",
    "test_contexts = [\n",
    "    BEST_RETRIEVAL_MGR.retrieve_context(q)\n",
    "    for q in test_questions\n",
    "]\n",
    "\n",
    "test_answers = BEST_PROMPT_MGR.batch_generate_answers(\n",
    "    questions=test_questions,\n",
    "    contexts_list=test_contexts,\n",
    ")\n",
    "\n",
    "# Build Kaggle submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"],\n",
    "    \"answer\": test_answers,\n",
    "})\n",
    "\n",
    "SUBMISSION_PATH = \"./results/kaggle_submission.csv\"\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "print(f\"✓ Kaggle submission file saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"✓ Total rows: {len(submission_df)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
