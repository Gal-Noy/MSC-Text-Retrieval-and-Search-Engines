{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80249d88",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660a3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Literal\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79df6",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38133d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4cf0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"21.0.9\" 2025-10-21\n",
      "OpenJDK Runtime Environment (build 21.0.9+10-Ubuntu-122.04)\n",
      "OpenJDK 64-Bit Server VM (build 21.0.9+10-Ubuntu-122.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4783a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff9235",
   "metadata": {},
   "source": [
    "### Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f1afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged into Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(os.getenv('HUGGING_FACE_TOKEN'))\n",
    "print(\"✓ Logged into Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a17cc",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3486b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3778 questions\n",
      "Test set: 2032 questions\n",
      "\n",
      "Sample question: what is the name of justin bieber brother?\n",
      "Sample answers: ['Jazmyn Bieber', 'Jaxon Bieber']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(\"./data/train.csv\", converters={\"answers\": json.loads})\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(f\"Train set: {len(df_train)} questions\")\n",
    "print(f\"Test set: {len(df_test)} questions\")\n",
    "print(f\"\\nSample question: {df_train.iloc[0]['question']}\")\n",
    "print(f\"Sample answers: {df_train.iloc[0]['answers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14492e1",
   "metadata": {},
   "source": [
    "## 3. Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef07078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-12-14 08:59:25.513152735 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pyserini index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dec 14, 2025 8:59:26 AM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n",
      "✓ Index loaded: 5903530 documents\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "# Load Pyserini index\n",
    "print(\"Loading Pyserini index...\")\n",
    "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "\n",
    "print(f\"✓ Index loaded: {index_reader.stats()['documents']} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d922144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bi-encoder...\n",
      "✓ Bi-encoder loaded\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load bi-encoder\n",
    "print(\"Loading bi-encoder...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bi_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "print(\"✓ Bi-encoder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0537dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=5 | window=150, overlap=50\n",
      "1. the film, has denied that Rowling ever saw it before writing her book. Rowling has said on record mu...\n",
      "2. to resolve an ongoing feud between the organisation's northern and southern branches that had sapped...\n",
      "3. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "4. Harry Potter (character) Harry James Potter is the titular protagonist of J. K. Rowling's \"Harry Pot...\n",
      "5. by Emily Brontë, \"Charlie and the Chocolate Factory\" by Roald Dahl, \"Robinson Crusoe\" by Daniel Defo...\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from functools import lru_cache\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_doc_content(docid: str) -> str:\n",
    "    \"\"\"Cache document content extraction.\"\"\"\n",
    "    try:\n",
    "        doc = searcher.doc(docid)\n",
    "        return json.loads(doc.raw()).get(\"contents\", \"\").replace(\"\\n\", \" \")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    \n",
    "@dataclass\n",
    "class RetrievalManager:\n",
    "    \"\"\"\n",
    "    Manages passage-based hybrid retrieval with RRF fusion and bi-encoder reranking.\n",
    "    \"\"\"\n",
    "    k_docs: int = 10\n",
    "    k_passages: int = 5\n",
    "    rrf_k: int = 60          # RRF constant (typically 60)\n",
    "    mu: int = 1000           # QLD smoothing\n",
    "    k1: float = 0.9          # BM25\n",
    "    b: float = 0.4           # BM25\n",
    "    window: int = 150\n",
    "    overlap: int = 50\n",
    "    min_passage_words: int = 30\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Retrieval(RRF_k={self.rrf_k}, μ={self.mu}, k1={self.k1}, b={self.b}) | \"\n",
    "            f\"k_docs={self.k_docs}, k_passages={self.k_passages} | \"\n",
    "            f\"window={self.window}, overlap={self.overlap}\"\n",
    "        )\n",
    "    \n",
    "    def extract_passages(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into overlapping word windows.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        words = text.split()\n",
    "        if len(words) < self.min_passage_words:\n",
    "            return []\n",
    "        \n",
    "        step = max(1, self.window - self.overlap)\n",
    "        passages = []\n",
    "        \n",
    "        for i in range(0, len(words), step):\n",
    "            chunk = words[i:i + self.window]\n",
    "            if len(chunk) < self.min_passage_words:\n",
    "                break\n",
    "            passages.append(\" \".join(chunk))\n",
    "        \n",
    "        return passages\n",
    "    \n",
    "    def rerank(self, query: str, passages: List[str]) -> List[str]:\n",
    "        \"\"\"Rerank passages using bi-encoder with deduplication.\"\"\"\n",
    "        if not passages:\n",
    "            return []\n",
    "        \n",
    "        # Deduplicate exact matches\n",
    "        seen = set()\n",
    "        unique_passages = []\n",
    "        for p in passages:\n",
    "            if p not in seen:\n",
    "                seen.add(p)\n",
    "                unique_passages.append(p)\n",
    "        \n",
    "        if not unique_passages:\n",
    "            return []\n",
    "        \n",
    "        # Bi-encoder reranking\n",
    "        q_emb = bi_encoder.encode(query, convert_to_tensor=True, device=device)\n",
    "        p_embs = bi_encoder.encode(unique_passages, convert_to_tensor=True, device=device)\n",
    "        scores = util.cos_sim(q_emb, p_embs).squeeze(0)\n",
    "        \n",
    "        top_k = min(self.k_passages, len(unique_passages))\n",
    "        idx = torch.topk(scores, k=top_k).indices.tolist()\n",
    "        \n",
    "        return [unique_passages[i] for i in idx]\n",
    "    \n",
    "    def retrieve_context(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieve passages using Reciprocal Rank Fusion (RRF) + bi-encoder reranking.\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # QLD retrieval\n",
    "        searcher.set_qld(self.mu)\n",
    "        qld_hits = searcher.search(query, self.k_docs)\n",
    "        for rank, hit in enumerate(qld_hits):\n",
    "            scores[hit.docid] = scores.get(hit.docid, 0.0) + 1.0 / (self.rrf_k + rank + 1)\n",
    "        \n",
    "        # BM25 retrieval\n",
    "        searcher.set_bm25(self.k1, self.b)\n",
    "        bm25_hits = searcher.search(query, self.k_docs)\n",
    "        for rank, hit in enumerate(bm25_hits):\n",
    "            scores[hit.docid] = scores.get(hit.docid, 0.0) + 1.0 / (self.rrf_k + rank + 1)\n",
    "        \n",
    "        # Top-k documents by RRF score\n",
    "        top_docids = sorted(scores, key=scores.get, reverse=True)[:self.k_docs]\n",
    "        \n",
    "        # Extract passages from top documents\n",
    "        passages = []\n",
    "        for docid in top_docids:\n",
    "            content = get_doc_content(docid)\n",
    "            if content:\n",
    "                passages.extend(self.extract_passages(content))\n",
    "        \n",
    "        return self.rerank(query, passages)\n",
    "\n",
    "\n",
    "# Test the RetrievalManager\n",
    "query = \"Who wrote Harry Potter?\"\n",
    "    \n",
    "rm = RetrievalManager()\n",
    "\n",
    "print(rm)\n",
    "passages = rm.retrieve_context(query)\n",
    "\n",
    "for i, p in enumerate(passages, 1):\n",
    "    print(f\"{i}. {p[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e849f6",
   "metadata": {},
   "source": [
    "## 4. LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b4b616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model...\n",
      "✓ Model loaded on: GPU\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Suppress transformers warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Loading LLM model...\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Set pad_token for batch processing\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "\n",
    "print(f\"✓ Model loaded on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ae5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: temp=0.0, top_p=1.0, max_tokens=256\n",
      "✓ Generated answer: 'J.K. Rowling'\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You must respond based strictly on the information in provided passages.\"\n",
    "    \"Do not incorporate any external knowledge or infer any details beyond what is given.\"\n",
    "    \"If the answer is not in the context, return 'I dont know'.\"\n",
    "    \"Do not include explanations, only the final answer!\"\n",
    ")\n",
    "\n",
    "DEFAULT_USER_PROMPT = (\n",
    "    \"Based on the following documents, provide a concise answer to the question.\\n\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class PromptManager:\n",
    "    \"\"\"Manages prompt generation and LLM answer generation.\"\"\"\n",
    "    system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    "    user_prompt: str = DEFAULT_USER_PROMPT\n",
    "    temperature: float = 0.0\n",
    "    top_p: float = 1.0\n",
    "    max_new_tokens: int = 256\n",
    "    do_sample: bool = False\n",
    "    prompt_id: str = \"default\"  # For later use in prompt tuning\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"temp={self.temperature}, top_p={self.top_p}, max_tokens={self.max_new_tokens}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_answer(answer: str) -> str:\n",
    "        \"\"\"Clean and standardize the generated answer.\"\"\"\n",
    "        answer = re.sub(r'^(Answer|The answer is|Based on the .*?,):?\\s*', '', answer, flags=re.I)\n",
    "        answer = answer.rstrip('.')\n",
    "        if any(phrase in answer.lower() for phrase in [\"dont know\", \"don't know\", \"do not know\", \"unknown\"]):\n",
    "            return \"unknown\"\n",
    "        return answer.strip()\n",
    "\n",
    "    def create_messages(self, question: str, contexts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Create messages for the LLM based on the question and contexts.\"\"\"\n",
    "        if not contexts:\n",
    "            context_str = \"No relevant documents found.\"\n",
    "        else:\n",
    "            context_str = '\\n\\n'.join([f\"Document {i+1}: {ctx}\" for i, ctx in enumerate(contexts)])\n",
    "        \n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": self.user_prompt.format(context=context_str, question=question)}\n",
    "        ]\n",
    "\n",
    "    def generate_answer(self, question: str, contexts: List[str]) -> str:\n",
    "        \"\"\"Generate an answer using the LLM based on the question and contexts.\"\"\"\n",
    "        messages = self.create_messages(question, contexts)\n",
    "        \n",
    "        outputs = pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=self.do_sample,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "        )\n",
    "        \n",
    "        answer = outputs[0][\"generated_text\"][-1].get('content', '')\n",
    "        return self.clean_answer(answer)\n",
    "\n",
    "    def batch_generate_answers(self, questions: List[str], contexts_list: List[List[str]]) -> List[str]:\n",
    "        \"\"\"Generate answers for multiple questions in batch.\"\"\"\n",
    "        # Create messages for all questions\n",
    "        batch_messages = [self.create_messages(q, ctx) for q, ctx in zip(questions, contexts_list)]\n",
    "        \n",
    "        # Process batch through pipeline\n",
    "        outputs = pipeline(\n",
    "            batch_messages,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=self.do_sample,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p\n",
    "        )\n",
    "        \n",
    "        # Extract and clean answers\n",
    "        answers = []\n",
    "        for output in outputs:\n",
    "            answer = output[0][\"generated_text\"][-1].get('content', '')\n",
    "            answers.append(self.clean_answer(answer))\n",
    "        \n",
    "        return answers\n",
    "\n",
    "\n",
    "# Test the PromptManager\n",
    "test_prompt_manager = PromptManager()\n",
    "print(f\"Testing: {test_prompt_manager}\")\n",
    "test_answer = test_prompt_manager.generate_answer(query, passages)\n",
    "print(f\"✓ Generated answer: '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873c1ef",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9b4c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation test: F1=66.67, P=66.67, R=66.67, EM=66.67\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer for comparison\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_token_metrics(prediction: str, ground_truth: str) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score for token-level comparison.\n",
    "    Returns: (precision, recall, f1)\n",
    "    \"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    # Handle empty cases\n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        match = int(pred_tokens == gt_tokens)\n",
    "        return match, match, match\n",
    "    \n",
    "    # Compute overlap\n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate_predictions(df_gold: pd.DataFrame, predictions: Dict[int, str]) -> Dict:\n",
    "    \"\"\"Evaluate predictions against ground truth.\"\"\"\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    exact_matches = []\n",
    "    \n",
    "    for _, row in df_gold.iterrows():\n",
    "        qid = row['id']\n",
    "        \n",
    "        # Handle missing predictions\n",
    "        if qid not in predictions:\n",
    "            f1_scores.append(0.0)\n",
    "            precision_scores.append(0.0)\n",
    "            recall_scores.append(0.0)\n",
    "            exact_matches.append(0)\n",
    "            continue\n",
    "        \n",
    "        prediction = predictions[qid]\n",
    "        ground_truths = row['answers']\n",
    "        \n",
    "        # Normalize once\n",
    "        norm_prediction = normalize_answer(prediction)\n",
    "        \n",
    "        # Find best match across all ground truths\n",
    "        best_f1 = 0.0\n",
    "        best_precision = 0.0\n",
    "        best_recall = 0.0\n",
    "        is_exact = 0\n",
    "        \n",
    "        for gt in ground_truths:\n",
    "            norm_gt = normalize_answer(gt)\n",
    "            \n",
    "            # Compute metrics\n",
    "            prec, rec, f1 = compute_token_metrics(prediction, gt)\n",
    "            \n",
    "            # Track best scores\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_precision = prec\n",
    "                best_recall = rec\n",
    "            \n",
    "            # Check exact match\n",
    "            if norm_prediction == norm_gt:\n",
    "                is_exact = 1\n",
    "        \n",
    "        f1_scores.append(best_f1)\n",
    "        precision_scores.append(best_precision)\n",
    "        recall_scores.append(best_recall)\n",
    "        exact_matches.append(is_exact)\n",
    "    \n",
    "    return {\n",
    "        'f1': 100.0 * sum(f1_scores) / len(f1_scores) if f1_scores else 0.0,\n",
    "        'precision': 100.0 * sum(precision_scores) / len(precision_scores) if precision_scores else 0.0,\n",
    "        'recall': 100.0 * sum(recall_scores) / len(recall_scores) if recall_scores else 0.0,\n",
    "        'exact_match': 100.0 * sum(exact_matches) / len(exact_matches) if exact_matches else 0.0,\n",
    "        'f1_scores': f1_scores,\n",
    "        'precision_scores': precision_scores,\n",
    "        'recall_scores': recall_scores,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "test_predictions = {1: \"J.K. Rowling\", 2: \"Paris\", 3: \"Shakespeare\"}\n",
    "test_gold = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'answers': [[\"J.K. Rowling\", \"Rowling\"], [\"Earth\"], [\"William Shakespeare\", \"Shakespeare\"]]\n",
    "})\n",
    "\n",
    "test_metrics = evaluate_predictions(test_gold, test_predictions)\n",
    "print(f\"✓ Evaluation test: F1={test_metrics['f1']:.2f}, P={test_metrics['precision']:.2f}, R={test_metrics['recall']:.2f}, EM={test_metrics['exact_match']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956e446",
   "metadata": {},
   "source": [
    "## 6. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "721292c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing experiment with:\n",
      "  Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=5 | window=150, overlap=50\n",
      "  Prompt: temp=0.0, top_p=1.0, max_tokens=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quick Test: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick Test\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=5 | window=150, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=22.86 | P=21.67 | R=30.00 | EM=20.00\n",
      "   Questions: 5\n",
      "\n",
      "✓ Experiment framework ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    name: str,\n",
    "    df_data: pd.DataFrame,\n",
    "    retrieval_manager: RetrievalManager,\n",
    "    prompt_manager: PromptManager,\n",
    "    max_questions: Optional[int] = None,\n",
    "    batch_size: int = 4,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    if max_questions:\n",
    "        df_data = df_data.head(max_questions)\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Process in batches\n",
    "    num_batches = (len(df_data) + batch_size - 1) // batch_size\n",
    "    iterator = tqdm(range(num_batches), desc=name) if verbose else range(num_batches)\n",
    "    \n",
    "    for batch_idx in iterator:\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(df_data))\n",
    "        batch_df = df_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Retrieve contexts for all questions in batch\n",
    "        batch_questions = []\n",
    "        batch_qids = []\n",
    "        batch_contexts = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            question = row['question']\n",
    "            qid = row['id']\n",
    "            contexts = retrieval_manager.retrieve_context(question)\n",
    "            \n",
    "            batch_questions.append(question)\n",
    "            batch_qids.append(qid)\n",
    "            batch_contexts.append(contexts)\n",
    "        \n",
    "        # Generate answers in batch\n",
    "        batch_answers = prompt_manager.batch_generate_answers(batch_questions, batch_contexts)\n",
    "        \n",
    "        # Store predictions\n",
    "        for qid, answer in zip(batch_qids, batch_answers):\n",
    "            predictions[qid] = answer\n",
    "    \n",
    "    metrics = evaluate_predictions(df_data, predictions)\n",
    "    \n",
    "    result = {\n",
    "        'name': name,\n",
    "        'retrieval': retrieval_manager,\n",
    "        'prompt': prompt_manager,\n",
    "        'f1_score': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'exact_match': metrics['exact_match'],\n",
    "        'num_questions': len(df_data),\n",
    "        'predictions': predictions,\n",
    "        'f1_scores': metrics['f1_scores'],\n",
    "        'precision_scores': metrics['precision_scores'],\n",
    "        'recall_scores': metrics['recall_scores'],\n",
    "        'exact_matches': metrics['exact_matches']\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"   Retrieval: {retrieval_manager}\")\n",
    "        print(f\"   Prompt: {prompt_manager}\")\n",
    "        print(f\"   F1={metrics['f1']:.2f} | P={metrics['precision']:.2f} | R={metrics['recall']:.2f} | EM={metrics['exact_match']:.2f}\")\n",
    "        print(f\"   Questions: {len(df_data)}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test experiment\n",
    "test_retrieval = RetrievalManager()\n",
    "test_prompt = PromptManager()\n",
    "print(f\"Testing experiment with:\")\n",
    "print(f\"  Retrieval: {test_retrieval}\")\n",
    "print(f\"  Prompt: {test_prompt}\")\n",
    "\n",
    "test_exp = run_experiment(\n",
    "    \"Quick Test\",\n",
    "    df_train.head(5),\n",
    "    test_retrieval,\n",
    "    test_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Experiment framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78eead",
   "metadata": {},
   "source": [
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54f379",
   "metadata": {},
   "source": [
    "### Experiments global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8c7605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASED RETRIEVAL + GENERATION EXPERIMENT FRAMEWORK\n",
      "================================================================================\n",
      "Validation questions per config: 100\n",
      "Random seed: 42\n",
      "Results cache: ./results/grid_search_results_q100.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_SEED = 42\n",
    "EXPERIMENT_QUESTIONS = 100\n",
    "\n",
    "EXPERIMENT_LOG_PATH = (\n",
    "    f\"./results/grid_search_results_q{EXPERIMENT_QUESTIONS}.csv\"\n",
    ")\n",
    "\n",
    "validation_data = df_train.sample(\n",
    "    n=EXPERIMENT_QUESTIONS,\n",
    "    random_state=EXPERIMENT_SEED\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASED RETRIEVAL + GENERATION EXPERIMENT FRAMEWORK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Validation questions per config: {EXPERIMENT_QUESTIONS}\")\n",
    "print(f\"Random seed: {EXPERIMENT_SEED}\")\n",
    "print(f\"Results cache: {EXPERIMENT_LOG_PATH}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9a89e",
   "metadata": {},
   "source": [
    "### Experiments utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4d8b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_retrieval_manager(base: dict, override: dict) -> RetrievalManager:\n",
    "    \"\"\"Build RetrievalManager safely.\"\"\"\n",
    "    return RetrievalManager(**{**base, **override})\n",
    "\n",
    "\n",
    "def generate_config_key(\n",
    "    retrieval_mgr: RetrievalManager,\n",
    "    prompt_mgr: PromptManager,\n",
    ") -> str:\n",
    "    \"\"\"Generate unique config key for RRF-based retrieval.\"\"\"\n",
    "    return (\n",
    "        f\"RRF_k{retrieval_mgr.rrf_k}_\"\n",
    "        f\"mu{retrieval_mgr.mu}_\"\n",
    "        f\"k1{retrieval_mgr.k1}_b{retrieval_mgr.b}_\"\n",
    "        f\"kdocs{retrieval_mgr.k_docs}_\"\n",
    "        f\"kpass{retrieval_mgr.k_passages}_\"\n",
    "        f\"win{retrieval_mgr.window}_ovl{retrieval_mgr.overlap}_\"\n",
    "        f\"prompt{prompt_mgr.prompt_id}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def save_results_to_csv(result: dict, key: str, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    row = {\n",
    "        \"config_key\": key,\n",
    "        \"f1\": result[\"f1_score\"],\n",
    "        \"precision\": result[\"precision\"],\n",
    "        \"recall\": result[\"recall\"],\n",
    "        \"exact_match\": result[\"exact_match\"],\n",
    "        \"num_questions\": result[\"num_questions\"],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([row])\n",
    "    if not os.path.exists(path):\n",
    "        df.to_csv(path, index=False)\n",
    "    else:\n",
    "        df.to_csv(path, mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "def load_completed_configs(path: str) -> set[str]:\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    return set(pd.read_csv(path)[\"config_key\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb074e07",
   "metadata": {},
   "source": [
    "### Best-config selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "987015b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k_configs(\n",
    "    retrieval_managers: list[RetrievalManager],\n",
    "    prompt_managers: list[PromptManager],\n",
    "    *,\n",
    "    top_k: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return the top-K configurations by validation F1 score.\n",
    "    Always sorted by descending F1.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    scored_entries = []\n",
    "\n",
    "    for r_mgr, p_mgr in zip(retrieval_managers, prompt_managers):\n",
    "        key = generate_config_key(r_mgr, p_mgr)\n",
    "        row = df[df[\"config_key\"] == key]\n",
    "        if row.empty:\n",
    "            continue\n",
    "\n",
    "        scored_entries.append({\n",
    "            \"retrieval_mgr\": r_mgr,\n",
    "            \"prompt_mgr\": p_mgr,\n",
    "            \"f1\": float(row.iloc[0][\"f1\"]),\n",
    "            \"config_key\": key,\n",
    "        })\n",
    "\n",
    "    # Sort before slicing\n",
    "    scored_entries.sort(\n",
    "        key=lambda x: (x[\"f1\"], x[\"config_key\"]),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    return scored_entries[:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f416f",
   "metadata": {},
   "source": [
    "### Phase runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "827db996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phase(\n",
    "    *,\n",
    "    phase_name: str,\n",
    "    grid: list[dict],\n",
    "    validation_data,\n",
    "    top_k: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a single experiment phase.\n",
    "\n",
    "    Each grid item must contain:\n",
    "      - retrieval_mgr: RetrievalManager\n",
    "      - prompt_mgr: PromptManager\n",
    "\n",
    "    Returns:\n",
    "      - top-K configs sorted by F1 (if top_k is provided)\n",
    "      - otherwise, the full grid sorted by F1\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(phase_name)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    completed = load_completed_configs(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    pending = [\n",
    "        g for g in grid\n",
    "        if generate_config_key(g[\"retrieval_mgr\"], g[\"prompt_mgr\"]) not in completed\n",
    "    ]\n",
    "\n",
    "    print(f\"Total configs: {len(grid)}\")\n",
    "    print(f\"Completed configs: {len(grid) - len(pending)}\")\n",
    "    print(f\"Pending configs: {len(pending)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, entry in enumerate(pending, start=1):\n",
    "        retrieval_mgr = entry[\"retrieval_mgr\"]\n",
    "        prompt_mgr = entry[\"prompt_mgr\"]\n",
    "\n",
    "        key = generate_config_key(retrieval_mgr, prompt_mgr)\n",
    "        print(f\"[{i}/{len(pending)}] Running: {key}\")\n",
    "\n",
    "        result = run_experiment(\n",
    "            name=key,\n",
    "            df_data=validation_data,\n",
    "            retrieval_manager=retrieval_mgr,\n",
    "            prompt_manager=prompt_mgr,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        save_results_to_csv(result, key, EXPERIMENT_LOG_PATH)\n",
    "        print(f\"✓ F1={result['f1_score']:.4f}\")\n",
    "\n",
    "    # Load results once for consistent sorting\n",
    "    df = pd.read_csv(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    def get_f1(entry):\n",
    "        key = generate_config_key(entry[\"retrieval_mgr\"], entry[\"prompt_mgr\"])\n",
    "        row = df[df[\"config_key\"] == key]\n",
    "        return float(row.iloc[0][\"f1\"]) if not row.empty else -1.0\n",
    "\n",
    "    # Sort full grid by F1\n",
    "    sorted_grid = sorted(\n",
    "        grid,\n",
    "        key=lambda g: (get_f1(g), generate_config_key(g[\"retrieval_mgr\"], g[\"prompt_mgr\"])),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    if top_k is None:\n",
    "        return sorted_grid\n",
    "\n",
    "    top_configs = select_top_k_configs(\n",
    "        [g[\"retrieval_mgr\"] for g in sorted_grid],\n",
    "        [g[\"prompt_mgr\"] for g in sorted_grid],\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop configs selected:\")\n",
    "    for i, entry in enumerate(top_configs, 1):\n",
    "        print(\n",
    "            f\"{i}. {entry['config_key']} | \"\n",
    "            f\"F1={entry['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "    return top_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03cdf5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 1 grid size: 12\n",
      "\n",
      "================================================================================\n",
      "PHASE 1 — Passage Segmentation\n",
      "================================================================================\n",
      "Total configs: 12\n",
      "Completed configs: 12\n",
      "Pending configs: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top configs selected:\n",
      "1. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win150_ovl50_promptdefault | F1=22.5510\n",
      "2. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win250_ovl50_promptdefault | F1=21.7384\n",
      "3. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win200_ovl25_promptdefault | F1=20.4083\n",
      "4. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win250_ovl75_promptdefault | F1=20.1787\n",
      "5. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win150_ovl75_promptdefault | F1=18.9896\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 1 — Passage Segmentation\n",
    "# ============================================================\n",
    "\n",
    "PHASE_1_GRID = []\n",
    "\n",
    "BASE_RETRIEVAL_PARAMS = {\n",
    "    \"k_docs\": 10,       \n",
    "    \"k_passages\": 5,\n",
    "    \"mu\": 1000,\n",
    "    \"k1\": 0.9,\n",
    "    \"b\": 0.4,\n",
    "}\n",
    "\n",
    "for window in [100, 150, 200, 250]:\n",
    "    for overlap in [25, 50, 75]:\n",
    "        PHASE_1_GRID.append({\n",
    "            \"retrieval_mgr\": RetrievalManager(\n",
    "                window=window,\n",
    "                overlap=overlap,\n",
    "                **BASE_RETRIEVAL_PARAMS,\n",
    "            ),\n",
    "            \"prompt_mgr\": PromptManager(),\n",
    "        })\n",
    "\n",
    "print(f\"✓ Phase 1 grid size: {len(PHASE_1_GRID)}\")\n",
    "\n",
    "PHASE_1_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 1 — Passage Segmentation\",\n",
    "    grid=PHASE_1_GRID,\n",
    "    validation_data=validation_data,\n",
    "    top_k=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86e95ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 2 grid size: 60\n",
      "\n",
      "================================================================================\n",
      "PHASE 2 — Retrieval Capacity\n",
      "================================================================================\n",
      "Total configs: 60\n",
      "Completed configs: 60\n",
      "Pending configs: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top configs selected:\n",
      "1. RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win150_ovl50_promptdefault | F1=22.7270\n",
      "2. RRF_k60_mu1000_k10.9_b0.4_kdocs20_kpass7_win150_ovl50_promptdefault | F1=22.6114\n",
      "3. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win150_ovl50_promptdefault | F1=22.5510\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 2 — Retrieval Capacity\n",
    "# ============================================================\n",
    "\n",
    "PHASE_2_GRID = []\n",
    "\n",
    "for entry in PHASE_1_TOP_CONFIGS:\n",
    "    base_mgr = entry[\"retrieval_mgr\"]\n",
    "\n",
    "    for k_docs in [5, 10, 15, 20]:\n",
    "        for k_passages in [3, 5, 7]:\n",
    "            PHASE_2_GRID.append({\n",
    "                \"retrieval_mgr\": RetrievalManager(\n",
    "                    k_docs=k_docs,\n",
    "                    k_passages=k_passages,\n",
    "                    window=base_mgr.window,\n",
    "                    overlap=base_mgr.overlap,\n",
    "                    mu=base_mgr.mu,\n",
    "                    k1=base_mgr.k1,\n",
    "                    b=base_mgr.b,\n",
    "                ),\n",
    "                \"prompt_mgr\": PromptManager(),\n",
    "            })\n",
    "\n",
    "print(f\"✓ Phase 2 grid size: {len(PHASE_2_GRID)}\")\n",
    "\n",
    "PHASE_2_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 2 — Retrieval Capacity\",\n",
    "    grid=PHASE_2_GRID,\n",
    "    validation_data=validation_data,\n",
    "    top_k=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f47ad0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 3 grid size: 27\n",
      "\n",
      "================================================================================\n",
      "PHASE 3 — Lexical Hyperparameters\n",
      "================================================================================\n",
      "Total configs: 27\n",
      "Completed configs: 4\n",
      "Pending configs: 23\n",
      "--------------------------------------------------------------------------------\n",
      "[1/23] Running: RRF_k60_mu1000_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault: 100%|██████████| 25/25 [02:50<00:00,  6.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.6, b=0.3) | k_docs=15, k_passages=5 | window=150, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=20.59 | P=21.39 | R=31.67 | EM=8.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=20.5932\n",
      "[2/23] Running: RRF_k60_mu2000_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu2000_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault: 100%|██████████| 25/25 [03:04<00:00,  7.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu2000_k10.6_b0.3_kdocs15_kpass5_win150_ovl50_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=2000, k1=0.6, b=0.3) | k_docs=15, k_passages=5 | window=150, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=21.16 | P=21.85 | R=30.92 | EM=8.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=21.1596\n",
      "[3/23] Running: RRF_k60_mu500_k10.9_b0.4_kdocs15_kpass5_win150_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu500_k10.9_b0.4_kdocs15_kpass5_win150_ovl50_promptdefault: 100%|██████████| 25/25 [02:19<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu500_k10.9_b0.4_kdocs15_kpass5_win150_ovl50_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=500, k1=0.9, b=0.4) | k_docs=15, k_passages=5 | window=150, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=18.47 | P=19.64 | R=28.33 | EM=4.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=18.4695\n",
      "[4/23] Running: RRF_k60_mu2000_k10.9_b0.4_kdocs15_kpass5_win150_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu2000_k10.9_b0.4_kdocs15_kpass5_win150_ovl50_promptdefault:  48%|████▊     | 12/25 [01:30<01:38,  7.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 39\u001b[0m\n\u001b[1;32m     24\u001b[0m             PHASE_3_GRID\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     25\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieval_mgr\u001b[39m\u001b[38;5;124m\"\u001b[39m: RetrievalManager(\n\u001b[1;32m     26\u001b[0m                     k_docs\u001b[38;5;241m=\u001b[39mbase_mgr\u001b[38;5;241m.\u001b[39mk_docs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_mgr\u001b[39m\u001b[38;5;124m\"\u001b[39m: PromptManager(),\n\u001b[1;32m     35\u001b[0m             })\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Phase 3 grid size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(PHASE_3_GRID)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m PHASE_3_TOP_CONFIGS \u001b[38;5;241m=\u001b[39m \u001b[43mrun_phase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPHASE 3 — Lexical Hyperparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPHASE_3_GRID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 42\u001b[0m, in \u001b[0;36mrun_phase\u001b[0;34m(phase_name, grid, validation_data, top_k)\u001b[0m\n\u001b[1;32m     39\u001b[0m key \u001b[38;5;241m=\u001b[39m generate_config_key(retrieval_mgr, prompt_mgr)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pending)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrieval_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretrieval_mgr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_mgr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m save_results_to_csv(result, key, EXPERIMENT_LOG_PATH)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(name, df_data, retrieval_manager, prompt_manager, max_questions, batch_size, verbose)\u001b[0m\n\u001b[1;32m     30\u001b[0m question \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m qid \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m contexts \u001b[38;5;241m=\u001b[39m \u001b[43mretrieval_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m batch_questions\u001b[38;5;241m.\u001b[39mappend(question)\n\u001b[1;32m     35\u001b[0m batch_qids\u001b[38;5;241m.\u001b[39mappend(qid)\n",
      "Cell \u001b[0;32mIn[10], line 112\u001b[0m, in \u001b[0;36mRetrievalManager.retrieve_context\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content:\n\u001b[1;32m    110\u001b[0m         passages\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_passages(content))\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassages\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 76\u001b[0m, in \u001b[0;36mRetrievalManager.rerank\u001b[0;34m(self, query, passages)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Bi-encoder reranking\u001b[39;00m\n\u001b[1;32m     75\u001b[0m q_emb \u001b[38;5;241m=\u001b[39m bi_encoder\u001b[38;5;241m.\u001b[39mencode(query, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 76\u001b[0m p_embs \u001b[38;5;241m=\u001b[39m \u001b[43mbi_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_passages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m scores \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(q_emb, p_embs)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     79\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_passages, \u001b[38;5;28mlen\u001b[39m(unique_passages))\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1062\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1061\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m-> 1062\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1064\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1621\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[0;34m(self, texts, **kwargs)\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;124;03mTokenizes the texts.\u001b[39;00m\n\u001b[1;32m   1612\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;124;03m        \"attention_mask\", and \"token_type_ids\".\u001b[39;00m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtokenize(texts)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:320\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[0;34m(self, texts, padding)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case:\n\u001b[1;32m    317\u001b[0m     to_tokenize \u001b[38;5;241m=\u001b[39m [[s\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m to_tokenize]\n\u001b[1;32m    319\u001b[0m output\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mto_tokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongest_first\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3073\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3072\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3073\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3075\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3161\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3156\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3157\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3158\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3159\u001b[0m         )\n\u001b[1;32m   3160\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 3161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3180\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   3184\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3185\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3203\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3204\u001b[0m     )\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3362\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3352\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3353\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3354\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3355\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3359\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3360\u001b[0m )\n\u001b[0;32m-> 3362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3380\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3381\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3382\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:601\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msanitized_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:249\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    245\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:796\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    793\u001b[0m     value \u001b[38;5;241m=\u001b[39m [value]\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 796\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m tensor\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:741\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    742\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint64\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(value, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:96\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_arr \u001b[38;5;129;01min\u001b[39;00m arr:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m---> 96\u001b[0m         res\u001b[38;5;241m.\u001b[39mextend(\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_arr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m         res\u001b[38;5;241m.\u001b[39mappend(sub_arr)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:95\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arr) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sub_arr \u001b[38;5;129;01min\u001b[39;00m arr:\n\u001b[0;32m---> 95\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     96\u001b[0m             res\u001b[38;5;241m.\u001b[39mextend(flatten(sub_arr))\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3 — Lexical Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "PHASE_3_GRID = []\n",
    "\n",
    "BM25_PARAMS = [\n",
    "    {\"k1\": 0.6, \"b\": 0.3},\n",
    "    {\"k1\": 0.9, \"b\": 0.4},   # baseline\n",
    "    {\"k1\": 1.2, \"b\": 0.6},\n",
    "]\n",
    "\n",
    "QLD_PARAMS = [\n",
    "    {\"mu\": 500},\n",
    "    {\"mu\": 1000},           # baseline\n",
    "    {\"mu\": 2000},\n",
    "]\n",
    "\n",
    "for entry in PHASE_2_TOP_CONFIGS:\n",
    "    base_mgr = entry[\"retrieval_mgr\"]\n",
    "\n",
    "    for bm25 in BM25_PARAMS:\n",
    "        for qld in QLD_PARAMS:\n",
    "            PHASE_3_GRID.append({\n",
    "                \"retrieval_mgr\": RetrievalManager(\n",
    "                    k_docs=base_mgr.k_docs,\n",
    "                    k_passages=base_mgr.k_passages,\n",
    "                    window=base_mgr.window,\n",
    "                    overlap=base_mgr.overlap,\n",
    "                    k1=bm25[\"k1\"],\n",
    "                    b=bm25[\"b\"],\n",
    "                    mu=qld[\"mu\"],\n",
    "                ),\n",
    "                \"prompt_mgr\": PromptManager(),\n",
    "            })\n",
    "\n",
    "print(f\"✓ Phase 3 grid size: {len(PHASE_3_GRID)}\")\n",
    "\n",
    "PHASE_3_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 3 — Lexical Hyperparameters\",\n",
    "    grid=PHASE_3_GRID,\n",
    "    validation_data=validation_data,\n",
    "    top_k=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a904e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SELECTION — Top 3 configs on 1,000 TRAIN questions\n",
    "# ============================================================\n",
    "\n",
    "FINAL_SELECTION_SEED = 123\n",
    "FINAL_SELECTION_SIZE = 1000\n",
    "\n",
    "# Take top-3 configs from Phase 3 (already sorted by F1)\n",
    "TOP_3_CONFIGS = PHASE_3_TOP_CONFIGS[:3]\n",
    "\n",
    "final_validation_data = df_train.sample(\n",
    "    n=FINAL_SELECTION_SIZE,\n",
    "    random_state=FINAL_SELECTION_SEED,\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL MODEL SELECTION ON 1,000 TRAIN QUESTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "FINAL_SELECTION_RESULTS = []\n",
    "\n",
    "for i, entry in enumerate(TOP_3_CONFIGS, 1):\n",
    "    retrieval_mgr = entry[\"retrieval_mgr\"]\n",
    "\n",
    "    prompt_mgr = PromptManager(\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        user_prompt=DEFAULT_USER_PROMPT,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "        top_p=1.0,\n",
    "        prompt_id=\"final\",\n",
    "    )\n",
    "\n",
    "    config_key = generate_config_key(retrieval_mgr, prompt_mgr)\n",
    "\n",
    "    print(f\"\\n[{i}/3] Evaluating config: {config_key}\")\n",
    "\n",
    "    result = run_experiment(\n",
    "        name=f\"{config_key}_final_select\",\n",
    "        df_data=final_validation_data,\n",
    "        retrieval_manager=retrieval_mgr,\n",
    "        prompt_manager=prompt_mgr,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"✓ F1={result['f1_score']:.4f} | \"\n",
    "        f\"EM={result['exact_match']:.4f} | \"\n",
    "        f\"P={result['precision']:.4f} | \"\n",
    "        f\"R={result['recall']:.4f}\"\n",
    "    )\n",
    "\n",
    "    FINAL_SELECTION_RESULTS.append({\n",
    "        \"retrieval_mgr\": retrieval_mgr,\n",
    "        \"prompt_mgr\": prompt_mgr,\n",
    "        \"config_key\": config_key,\n",
    "        **result,\n",
    "    })\n",
    "\n",
    "# Select the best config by F1\n",
    "FINAL_SELECTION_RESULTS.sort(\n",
    "    key=lambda x: x[\"f1_score\"],\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "BEST_FINAL_CONFIG = FINAL_SELECTION_RESULTS[0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ BEST FINAL CONFIG SELECTED\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"{BEST_FINAL_CONFIG['config_key']} | \"\n",
    "    f\"F1={BEST_FINAL_CONFIG['f1_score']:.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KAGGLE SUBMISSION — Final system on TEST set\n",
    "# ============================================================\n",
    "\n",
    "BEST_RETRIEVAL_MGR = BEST_FINAL_CONFIG[\"retrieval_mgr\"]\n",
    "BEST_PROMPT_MGR = BEST_FINAL_CONFIG[\"prompt_mgr\"]\n",
    "\n",
    "FINAL_CONFIG_KEY = BEST_FINAL_CONFIG[\"config_key\"]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KAGGLE SUBMISSION GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Using final config: {FINAL_CONFIG_KEY}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run inference only (no labels needed)\n",
    "test_questions = df_test[\"question\"].tolist()\n",
    "\n",
    "print(f\"Generating answers for {len(test_questions)} test questions...\")\n",
    "\n",
    "test_contexts = [\n",
    "    BEST_RETRIEVAL_MGR.retrieve_context(q)\n",
    "    for q in test_questions\n",
    "]\n",
    "\n",
    "test_answers = BEST_PROMPT_MGR.batch_generate_answers(\n",
    "    questions=test_questions,\n",
    "    contexts_list=test_contexts,\n",
    ")\n",
    "\n",
    "# Build Kaggle submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"],\n",
    "    \"answer\": test_answers,\n",
    "})\n",
    "\n",
    "SUBMISSION_PATH = \"./results/kaggle_submission.csv\"\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "print(f\"✓ Kaggle submission file saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"✓ Total rows: {len(submission_df)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
