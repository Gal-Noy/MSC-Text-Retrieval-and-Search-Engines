{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80249d88",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660a3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Literal\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79df6",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38133d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b4cf0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"21.0.9\" 2025-10-21\n",
      "OpenJDK Runtime Environment (build 21.0.9+10-Ubuntu-122.04)\n",
      "OpenJDK 64-Bit Server VM (build 21.0.9+10-Ubuntu-122.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4783a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install accelerate\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff9235",
   "metadata": {},
   "source": [
    "### Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f1afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged into Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(os.getenv('HUGGING_FACE_TOKEN'))\n",
    "print(\"✓ Logged into Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a17cc",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb3486b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3778 questions\n",
      "Test set: 2032 questions\n",
      "\n",
      "Sample question: what is the name of justin bieber brother?\n",
      "Sample answers: ['Jazmyn Bieber', 'Jaxon Bieber']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(\"./data/train.csv\", converters={\"answers\": json.loads})\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(f\"Train set: {len(df_train)} questions\")\n",
    "print(f\"Test set: {len(df_test)} questions\")\n",
    "print(f\"\\nSample question: {df_train.iloc[0]['question']}\")\n",
    "print(f\"Sample answers: {df_train.iloc[0]['answers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14492e1",
   "metadata": {},
   "source": [
    "## 3. Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef07078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-12-14 20:29:53.657930216 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pyserini index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dec 14, 2025 8:29:54 PM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleSearcher class has been deprecated, please use LuceneSearcher from pyserini.search.lucene instead\n",
      "✓ Index loaded: 5903530 documents\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "# Load Pyserini index\n",
    "print(\"Loading Pyserini index...\")\n",
    "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n",
    "\n",
    "print(f\"✓ Index loaded: {index_reader.stats()['documents']} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d922144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bi-encoder...\n",
      "✓ Bi-encoder loaded\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load bi-encoder\n",
    "print(\"Loading bi-encoder...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bi_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "print(\"✓ Bi-encoder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d0537dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=5 | window=150, overlap=50\n",
      "1. Harry Potter Harry Potter is a series of fantasy novels written by British author J. K. Rowling. The...\n",
      "2. nine to eleven. On the eve of publishing, Rowling was asked by her publishers to adopt a more gender...\n",
      "3. time, justified\"), while \"The Guardian\" called it \"a richly textured novel given lift-off by an inve...\n",
      "4. suddenly \"fell into her head\". Rowling gives an account of the experience on her website saying: Row...\n",
      "5. 1999 Whitbread Awards. His overall view of the series was negative – \"the Potter saga was essentiall...\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from functools import lru_cache\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Cached document access\n",
    "# --------------------------------------------------\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_doc_content(docid: str) -> str:\n",
    "    \"\"\"Return cached raw document text.\"\"\"\n",
    "    try:\n",
    "        doc = searcher.doc(docid)\n",
    "        return json.loads(doc.raw()).get(\"contents\", \"\").replace(\"\\n\", \" \")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Retrieval Manager\n",
    "# --------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class RetrievalManager:\n",
    "    \"\"\"\n",
    "    Hybrid retrieval with:\n",
    "      - QLD document retrieval\n",
    "      - BM25 document retrieval\n",
    "      - Passage extraction\n",
    "      - Dense passage reranking\n",
    "      - 3-way RRF on passages\n",
    "    \"\"\"\n",
    "    k_docs: int = 10\n",
    "    k_passages: int = 5\n",
    "    rrf_k: int = 60\n",
    "\n",
    "    mu: int = 1000\n",
    "    k1: float = 0.9\n",
    "    b: float = 0.4\n",
    "\n",
    "    window: int = 150\n",
    "    overlap: int = 50\n",
    "    min_passage_words: int = 30\n",
    "    max_dense_passages: int = 100\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Retrieval(RRF_k={self.rrf_k}, μ={self.mu}, \"\n",
    "            f\"k1={self.k1}, b={self.b}) | \"\n",
    "            f\"k_docs={self.k_docs}, k_passages={self.k_passages} | \"\n",
    "            f\"window={self.window}, overlap={self.overlap}\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Passage extraction\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    def extract_passages(self, text: str) -> List[str]:\n",
    "        \"\"\"Split document text into overlapping word windows.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        words = text.split()\n",
    "        if len(words) < self.min_passage_words:\n",
    "            return []\n",
    "\n",
    "        step = max(1, self.window - self.overlap)\n",
    "        passages = []\n",
    "\n",
    "        for i in range(0, len(words), step):\n",
    "            chunk = words[i:i + self.window]\n",
    "            if len(chunk) < self.min_passage_words:\n",
    "                break\n",
    "            passages.append(\" \".join(chunk))\n",
    "\n",
    "        return passages\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Dense ranking\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    def dense_rank(self, query: str, passages: List[str]) -> List[str]:\n",
    "        \"\"\"Rank passages by bi-encoder cosine similarity.\"\"\"\n",
    "        if not passages:\n",
    "            return []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_emb = bi_encoder.encode(\n",
    "                query,\n",
    "                convert_to_tensor=True,\n",
    "                device=device\n",
    "            )\n",
    "            p_embs = bi_encoder.encode(\n",
    "                passages,\n",
    "                convert_to_tensor=True,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            scores = util.cos_sim(q_emb, p_embs).squeeze(0)\n",
    "            order = torch.argsort(scores, descending=True).tolist()\n",
    "\n",
    "        return [passages[i] for i in order]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Main retrieval\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    def retrieve_context(self, query: str) -> List[str]:\n",
    "        \"\"\"Return top passages using 3-way RRF (QLD, BM25, Dense).\"\"\"\n",
    "\n",
    "        # ---------- Lexical retrieval ----------\n",
    "        searcher.set_qld(self.mu)\n",
    "        qld_docids = [h.docid for h in searcher.search(query, self.k_docs)]\n",
    "\n",
    "        searcher.set_bm25(self.k1, self.b)\n",
    "        bm25_docids = [h.docid for h in searcher.search(query, self.k_docs)]\n",
    "\n",
    "        # ---------- Extract passages per document ----------\n",
    "        doc_passages: Dict[str, List[str]] = {}\n",
    "        all_passages: List[str] = []\n",
    "\n",
    "        for docid in dict.fromkeys(qld_docids + bm25_docids):\n",
    "            content = get_doc_content(docid)\n",
    "            if not content:\n",
    "                continue\n",
    "\n",
    "            passages = self.extract_passages(content)\n",
    "            if passages:\n",
    "                doc_passages[docid] = passages\n",
    "                all_passages.extend(passages)\n",
    "\n",
    "        # Deduplicate passages\n",
    "        all_passages = list(dict.fromkeys(all_passages))\n",
    "\n",
    "        # ---------- Dense reranking (limited) ----------\n",
    "        dense_candidates = all_passages[:self.max_dense_passages]\n",
    "        dense_ranked = self.dense_rank(query, dense_candidates)\n",
    "\n",
    "        # ---------- RRF scoring on passages ----------\n",
    "        scores: Dict[str, float] = {}\n",
    "\n",
    "        # QLD → passages\n",
    "        for rank, docid in enumerate(qld_docids):\n",
    "            for p in doc_passages.get(docid, []):\n",
    "                scores[p] = scores.get(p, 0.0) + 1.0 / (self.rrf_k + rank + 1)\n",
    "\n",
    "        # BM25 → passages\n",
    "        for rank, docid in enumerate(bm25_docids):\n",
    "            for p in doc_passages.get(docid, []):\n",
    "                scores[p] = scores.get(p, 0.0) + 1.0 / (self.rrf_k + rank + 1)\n",
    "\n",
    "        # Dense → passages\n",
    "        for rank, p in enumerate(dense_ranked):\n",
    "            scores[p] = scores.get(p, 0.0) + 1.0 / (self.rrf_k + rank + 1)\n",
    "\n",
    "        # ---------- Final ranking ----------\n",
    "        ranked_passages = sorted(\n",
    "            scores,\n",
    "            key=scores.get,\n",
    "            reverse=True\n",
    "        )[:self.k_passages]\n",
    "\n",
    "        return ranked_passages\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Quick sanity check\n",
    "# --------------------------------------------------\n",
    "\n",
    "query = \"Who wrote Harry Potter?\"\n",
    "\n",
    "rm = RetrievalManager()\n",
    "print(rm)\n",
    "\n",
    "passages = rm.retrieve_context(query)\n",
    "for i, p in enumerate(passages, 1):\n",
    "    print(f\"{i}. {p[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e849f6",
   "metadata": {},
   "source": [
    "## 4. LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4b616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model...\n",
      "✓ Model loaded on: GPU\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Suppress transformers warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Loading LLM model...\")\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    device=0\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Set pad_token for batch processing\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "\n",
    "print(f\"✓ Model loaded on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: temp=0.0, top_p=1.0, max_tokens=256\n",
      "✓ Generated answer: 'J. K. Rowling'\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"Answer using the provided documents.\"\n",
    "    \"Prefer short factual answers.\"\n",
    "    \"If the answer cannot be reasonably inferred from the documents, return 'unknown'.\"\n",
    "    \"Do not explain your reasoning.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT = (\n",
    "    \"Answer the question using ONLY a short phrase or named entity found verbatim in the documents.\\n\"\n",
    "    \"If the answer is not explicitly stated, output: unknown.\\n\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class PromptManager:\n",
    "    \"\"\"Manages prompt generation and LLM answer generation.\"\"\"\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = USER_PROMPT\n",
    "    temperature: float = 0.0\n",
    "    top_p: float = 1.0\n",
    "    max_new_tokens: int = 256\n",
    "    do_sample: bool = False\n",
    "    prompt_id: str = \"default\"  # For later use in prompt tuning\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"temp={self.temperature}, top_p={self.top_p}, max_tokens={self.max_new_tokens}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_answer(answer: str) -> str:\n",
    "        \"\"\"Clean and standardize the generated answer.\"\"\"\n",
    "        answer = re.sub(r'^(Answer|The answer is|Based on the .*?,):?\\s*', '', answer, flags=re.I)\n",
    "        answer = answer.rstrip('.')\n",
    "        if any(phrase in answer.lower() for phrase in [\"dont know\", \"don't know\", \"do not know\", \"unknown\"]):\n",
    "            return \"unknown\"\n",
    "        return answer.strip()\n",
    "\n",
    "    def create_messages(self, question: str, contexts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Create messages for the LLM based on the question and contexts.\"\"\"\n",
    "        if not contexts:\n",
    "            context_str = \"No relevant documents found.\"\n",
    "        else:\n",
    "            context_str = '\\n\\n'.join([f\"Document {i+1}: {ctx}\" for i, ctx in enumerate(contexts)])\n",
    "        \n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": self.user_prompt.format(context=context_str, question=question)}\n",
    "        ]\n",
    "\n",
    "    def generate_answer(self, question: str, contexts: List[str]) -> str:\n",
    "        \"\"\"Generate an answer using the LLM based on the question and contexts.\"\"\"\n",
    "        messages = self.create_messages(question, contexts)\n",
    "        \n",
    "        outputs = pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=self.do_sample,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p\n",
    "        )\n",
    "        \n",
    "        answer = outputs[0][\"generated_text\"][-1].get('content', '')\n",
    "        return self.clean_answer(answer)\n",
    "\n",
    "    def batch_generate_answers(self, questions: List[str], contexts_list: List[List[str]]) -> List[str]:\n",
    "        \"\"\"Generate answers for multiple questions in batch.\"\"\"\n",
    "        # Create messages for all questions\n",
    "        batch_messages = [self.create_messages(q, ctx) for q, ctx in zip(questions, contexts_list)]\n",
    "        \n",
    "        # Process batch through pipeline\n",
    "        outputs = pipeline(\n",
    "            batch_messages,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=self.do_sample,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            batch_size=len(questions)\n",
    "        )\n",
    "        \n",
    "        # Extract and clean answers\n",
    "        answers = []\n",
    "        for output in outputs:\n",
    "            answer = output[0][\"generated_text\"][-1].get('content', '')\n",
    "            answers.append(self.clean_answer(answer))\n",
    "        \n",
    "        return answers\n",
    "\n",
    "\n",
    "# Test the PromptManager\n",
    "test_prompt_manager = PromptManager()\n",
    "print(f\"Testing: {test_prompt_manager}\")\n",
    "test_answer = test_prompt_manager.generate_answer(query, passages)\n",
    "print(f\"✓ Generated answer: '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873c1ef",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9b4c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation test: F1=66.67, P=66.67, R=66.67, EM=66.67\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer for comparison\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_token_metrics(prediction: str, ground_truth: str) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score for token-level comparison.\n",
    "    Returns: (precision, recall, f1)\n",
    "    \"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    # Handle empty cases\n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        match = int(pred_tokens == gt_tokens)\n",
    "        return match, match, match\n",
    "    \n",
    "    # Compute overlap\n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate_predictions(df_gold: pd.DataFrame, predictions: Dict[int, str]) -> Dict:\n",
    "    \"\"\"Evaluate predictions against ground truth.\"\"\"\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    exact_matches = []\n",
    "    \n",
    "    for _, row in df_gold.iterrows():\n",
    "        qid = row['id']\n",
    "        \n",
    "        # Handle missing predictions\n",
    "        if qid not in predictions:\n",
    "            f1_scores.append(0.0)\n",
    "            precision_scores.append(0.0)\n",
    "            recall_scores.append(0.0)\n",
    "            exact_matches.append(0)\n",
    "            continue\n",
    "        \n",
    "        prediction = predictions[qid]\n",
    "        ground_truths = row['answers']\n",
    "        \n",
    "        # Normalize once\n",
    "        norm_prediction = normalize_answer(prediction)\n",
    "        \n",
    "        # Find best match across all ground truths\n",
    "        best_f1 = 0.0\n",
    "        best_precision = 0.0\n",
    "        best_recall = 0.0\n",
    "        is_exact = 0\n",
    "        \n",
    "        for gt in ground_truths:\n",
    "            norm_gt = normalize_answer(gt)\n",
    "            \n",
    "            # Compute metrics\n",
    "            prec, rec, f1 = compute_token_metrics(prediction, gt)\n",
    "            \n",
    "            # Track best scores\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_precision = prec\n",
    "                best_recall = rec\n",
    "            \n",
    "            # Check exact match\n",
    "            if norm_prediction == norm_gt:\n",
    "                is_exact = 1\n",
    "        \n",
    "        f1_scores.append(best_f1)\n",
    "        precision_scores.append(best_precision)\n",
    "        recall_scores.append(best_recall)\n",
    "        exact_matches.append(is_exact)\n",
    "    \n",
    "    return {\n",
    "        'f1': 100.0 * sum(f1_scores) / len(f1_scores) if f1_scores else 0.0,\n",
    "        'precision': 100.0 * sum(precision_scores) / len(precision_scores) if precision_scores else 0.0,\n",
    "        'recall': 100.0 * sum(recall_scores) / len(recall_scores) if recall_scores else 0.0,\n",
    "        'exact_match': 100.0 * sum(exact_matches) / len(exact_matches) if exact_matches else 0.0,\n",
    "        'f1_scores': f1_scores,\n",
    "        'precision_scores': precision_scores,\n",
    "        'recall_scores': recall_scores,\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "test_predictions = {1: \"J.K. Rowling\", 2: \"Paris\", 3: \"Shakespeare\"}\n",
    "test_gold = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'answers': [[\"J.K. Rowling\", \"Rowling\"], [\"Earth\"], [\"William Shakespeare\", \"Shakespeare\"]]\n",
    "})\n",
    "\n",
    "test_metrics = evaluate_predictions(test_gold, test_predictions)\n",
    "print(f\"✓ Evaluation test: F1={test_metrics['f1']:.2f}, P={test_metrics['precision']:.2f}, R={test_metrics['recall']:.2f}, EM={test_metrics['exact_match']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956e446",
   "metadata": {},
   "source": [
    "## 6. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "721292c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing experiment with:\n",
      "  Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=20, k_passages=7 | window=150, overlap=50\n",
      "  Prompt: temp=0.0, top_p=1.0, max_tokens=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quick Test: 100%|██████████| 100/100 [01:07<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick Test\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=20, k_passages=7 | window=150, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=25.69 | P=31.19 | R=26.47 | EM=13.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ Experiment framework ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    name: str,\n",
    "    df_data: pd.DataFrame,\n",
    "    retrieval_manager: RetrievalManager,\n",
    "    prompt_manager: PromptManager,\n",
    "    max_questions: Optional[int] = None,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    if max_questions:\n",
    "        df_data = df_data.head(max_questions)\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    iterator = tqdm(df_data.iterrows(), total=len(df_data), desc=name) if verbose else df_data.iterrows()\n",
    "\n",
    "    for _, row in iterator:\n",
    "        question = row['question']\n",
    "        qid = row['id']\n",
    "\n",
    "        contexts = retrieval_manager.retrieve_context(question)\n",
    "        answer = prompt_manager.generate_answer(question, contexts)\n",
    "\n",
    "        predictions[qid] = answer\n",
    "\n",
    "    metrics = evaluate_predictions(df_data, predictions)\n",
    "\n",
    "    result = {\n",
    "        'name': name,\n",
    "        'retrieval': retrieval_manager,\n",
    "        'prompt': prompt_manager,\n",
    "        'f1_score': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'exact_match': metrics['exact_match'],\n",
    "        'num_questions': len(df_data),\n",
    "        'predictions': predictions,\n",
    "        'f1_scores': metrics['f1_scores'],\n",
    "        'precision_scores': metrics['precision_scores'],\n",
    "        'recall_scores': metrics['recall_scores'],\n",
    "        'exact_matches': metrics['exact_matches']\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"   Retrieval: {retrieval_manager}\")\n",
    "        print(f\"   Prompt: {prompt_manager}\")\n",
    "        print(\n",
    "            f\"   F1={metrics['f1']:.2f} | \"\n",
    "            f\"P={metrics['precision']:.2f} | \"\n",
    "            f\"R={metrics['recall']:.2f} | \"\n",
    "            f\"EM={metrics['exact_match']:.2f}\"\n",
    "        )\n",
    "        print(f\"   Questions: {len(df_data)}\\n\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test experiment\n",
    "test_retrieval = RetrievalManager(\n",
    "    k_docs=20,\n",
    "    k_passages=7\n",
    ")\n",
    "test_prompt = PromptManager()\n",
    "print(f\"Testing experiment with:\")\n",
    "print(f\"  Retrieval: {test_retrieval}\")\n",
    "print(f\"  Prompt: {test_prompt}\")\n",
    "\n",
    "test_exp = run_experiment(\n",
    "    \"Quick Test\",\n",
    "    df_train.head(100),\n",
    "    test_retrieval,\n",
    "    test_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Experiment framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78eead",
   "metadata": {},
   "source": [
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522c28f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Quick Test',\n",
       " 'retrieval': RetrievalManager(k_docs=20, k_passages=7, rrf_k=60, mu=1000, k1=0.9, b=0.4, window=150, overlap=50, min_passage_words=30, max_dense_passages=100),\n",
       " 'prompt': PromptManager(system_prompt=\"Answer using the provided documents.Prefer short factual answers.If the answer cannot be reasonably inferred from the documents, return 'unknown'.Do not explain your reasoning.\", user_prompt='Answer the question using ONLY a short phrase or named entity found verbatim in the documents.\\nIf the answer is not explicitly stated, output: unknown.\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:', temperature=0.0, top_p=1.0, max_new_tokens=256, do_sample=False, prompt_id='default'),\n",
       " 'f1_score': 34.885714285714286,\n",
       " 'precision': 40.99999999999999,\n",
       " 'recall': 35.733333333333334,\n",
       " 'exact_match': 16.0,\n",
       " 'num_questions': 25,\n",
       " 'predictions': {1: 'unknown',\n",
       "  2: 'Padmé',\n",
       "  3: 'Texas',\n",
       "  4: 'unknown',\n",
       "  5: 'unknown',\n",
       "  6: 'Gimli',\n",
       "  7: 'Memphis Grizzlies',\n",
       "  8: 'Washington',\n",
       "  9: 'unknown',\n",
       "  10: 'unknown',\n",
       "  11: 'Germany and the Soviet Union',\n",
       "  12: 'Eastern Time Zone',\n",
       "  13: 'Astoria Greengrass',\n",
       "  14: 'Canada / United States',\n",
       "  15: 'Italy',\n",
       "  16: 'Bulgaria',\n",
       "  17: 'Denmark',\n",
       "  18: 'John F. Kennedy',\n",
       "  19: 'Ramona',\n",
       "  20: 'United States',\n",
       "  21: 'unknown',\n",
       "  22: 'Abiy Ahmed',\n",
       "  23: 'Red Rock State Park',\n",
       "  24: 'Mission High School',\n",
       "  25: 'democratic'},\n",
       " 'f1_scores': [0.0,\n",
       "  0.6666666666666666,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.6666666666666666,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.4,\n",
       "  0.7499999999999999,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.6666666666666666,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.5714285714285715,\n",
       "  0.0],\n",
       " 'precision_scores': [0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.3333333333333333,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.6666666666666666,\n",
       "  0.0],\n",
       " 'recall_scores': [0.0,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.6,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.3333333333333333,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.0],\n",
       " 'exact_matches': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54f379",
   "metadata": {},
   "source": [
    "### Experiments global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8c7605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASED RETRIEVAL + GENERATION EXPERIMENT FRAMEWORK\n",
      "================================================================================\n",
      "Validation questions per config: 100\n",
      "Random seed: 42\n",
      "Results cache: ./results/grid_search_results_q100.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_SEED = 42\n",
    "EXPERIMENT_QUESTIONS = 100\n",
    "\n",
    "EXPERIMENT_LOG_PATH = (\n",
    "    f\"./results/grid_search_results_q{EXPERIMENT_QUESTIONS}.csv\"\n",
    ")\n",
    "\n",
    "validation_data = df_train.sample(\n",
    "    n=EXPERIMENT_QUESTIONS,\n",
    "    random_state=EXPERIMENT_SEED\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASED RETRIEVAL + GENERATION EXPERIMENT FRAMEWORK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Validation questions per config: {EXPERIMENT_QUESTIONS}\")\n",
    "print(f\"Random seed: {EXPERIMENT_SEED}\")\n",
    "print(f\"Results cache: {EXPERIMENT_LOG_PATH}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9a89e",
   "metadata": {},
   "source": [
    "### Experiments utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4d8b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_retrieval_manager(base: dict, override: dict) -> RetrievalManager:\n",
    "    \"\"\"Build RetrievalManager safely.\"\"\"\n",
    "    return RetrievalManager(**{**base, **override})\n",
    "\n",
    "\n",
    "def generate_config_key(\n",
    "    retrieval_mgr: RetrievalManager,\n",
    "    prompt_mgr: PromptManager,\n",
    ") -> str:\n",
    "    \"\"\"Generate unique config key for RRF-based retrieval.\"\"\"\n",
    "    return (\n",
    "        f\"RRF_k{retrieval_mgr.rrf_k}_\"\n",
    "        f\"mu{retrieval_mgr.mu}_\"\n",
    "        f\"k1{retrieval_mgr.k1}_b{retrieval_mgr.b}_\"\n",
    "        f\"kdocs{retrieval_mgr.k_docs}_\"\n",
    "        f\"kpass{retrieval_mgr.k_passages}_\"\n",
    "        f\"win{retrieval_mgr.window}_ovl{retrieval_mgr.overlap}_\"\n",
    "        f\"prompt{prompt_mgr.prompt_id}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def save_results_to_csv(result: dict, key: str, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    row = {\n",
    "        \"config_key\": key,\n",
    "        \"f1\": result[\"f1_score\"],\n",
    "        \"precision\": result[\"precision\"],\n",
    "        \"recall\": result[\"recall\"],\n",
    "        \"exact_match\": result[\"exact_match\"],\n",
    "        \"num_questions\": result[\"num_questions\"],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([row])\n",
    "    if not os.path.exists(path):\n",
    "        df.to_csv(path, index=False)\n",
    "    else:\n",
    "        df.to_csv(path, mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "def load_completed_configs(path: str) -> set[str]:\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    return set(pd.read_csv(path)[\"config_key\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb074e07",
   "metadata": {},
   "source": [
    "### Best-config selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "987015b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k_configs(\n",
    "    retrieval_managers: list[RetrievalManager],\n",
    "    prompt_managers: list[PromptManager],\n",
    "    *,\n",
    "    top_k: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return the top-K configurations by validation F1 score.\n",
    "    Always sorted by descending F1.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    scored_entries = []\n",
    "\n",
    "    for r_mgr, p_mgr in zip(retrieval_managers, prompt_managers):\n",
    "        key = generate_config_key(r_mgr, p_mgr)\n",
    "        row = df[df[\"config_key\"] == key]\n",
    "        if row.empty:\n",
    "            continue\n",
    "\n",
    "        scored_entries.append({\n",
    "            \"retrieval_mgr\": r_mgr,\n",
    "            \"prompt_mgr\": p_mgr,\n",
    "            \"f1\": float(row.iloc[0][\"f1\"]),\n",
    "            \"config_key\": key,\n",
    "        })\n",
    "\n",
    "    # Sort before slicing\n",
    "    scored_entries.sort(\n",
    "        key=lambda x: (x[\"f1\"], x[\"config_key\"]),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    return scored_entries[:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f416f",
   "metadata": {},
   "source": [
    "### Phase runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "827db996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phase(\n",
    "    *,\n",
    "    phase_name: str,\n",
    "    grid: list[dict],\n",
    "    validation_data,\n",
    "    top_k: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a single experiment phase.\n",
    "\n",
    "    Each grid item must contain:\n",
    "      - retrieval_mgr: RetrievalManager\n",
    "      - prompt_mgr: PromptManager\n",
    "\n",
    "    Returns:\n",
    "      - top-K configs sorted by F1 (if top_k is provided)\n",
    "      - otherwise, the full grid sorted by F1\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(phase_name)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    completed = load_completed_configs(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    pending = [\n",
    "        g for g in grid\n",
    "        if generate_config_key(g[\"retrieval_mgr\"], g[\"prompt_mgr\"]) not in completed\n",
    "    ]\n",
    "\n",
    "    print(f\"Total configs: {len(grid)}\")\n",
    "    print(f\"Completed configs: {len(grid) - len(pending)}\")\n",
    "    print(f\"Pending configs: {len(pending)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, entry in enumerate(pending, start=1):\n",
    "        retrieval_mgr = entry[\"retrieval_mgr\"]\n",
    "        prompt_mgr = entry[\"prompt_mgr\"]\n",
    "\n",
    "        key = generate_config_key(retrieval_mgr, prompt_mgr)\n",
    "        print(f\"[{i}/{len(pending)}] Running: {key}\")\n",
    "\n",
    "        result = run_experiment(\n",
    "            name=key,\n",
    "            df_data=validation_data,\n",
    "            retrieval_manager=retrieval_mgr,\n",
    "            prompt_manager=prompt_mgr,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        save_results_to_csv(result, key, EXPERIMENT_LOG_PATH)\n",
    "        print(f\"✓ F1={result['f1_score']:.4f}\")\n",
    "\n",
    "    # Load results once for consistent sorting\n",
    "    df = pd.read_csv(EXPERIMENT_LOG_PATH)\n",
    "\n",
    "    def get_f1(entry):\n",
    "        key = generate_config_key(entry[\"retrieval_mgr\"], entry[\"prompt_mgr\"])\n",
    "        row = df[df[\"config_key\"] == key]\n",
    "        return float(row.iloc[0][\"f1\"]) if not row.empty else -1.0\n",
    "\n",
    "    # Sort full grid by F1\n",
    "    sorted_grid = sorted(\n",
    "        grid,\n",
    "        key=lambda g: (get_f1(g), generate_config_key(g[\"retrieval_mgr\"], g[\"prompt_mgr\"])),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    if top_k is None:\n",
    "        return sorted_grid\n",
    "\n",
    "    top_configs = select_top_k_configs(\n",
    "        [g[\"retrieval_mgr\"] for g in sorted_grid],\n",
    "        [g[\"prompt_mgr\"] for g in sorted_grid],\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop configs selected:\")\n",
    "    for i, entry in enumerate(top_configs, 1):\n",
    "        print(\n",
    "            f\"{i}. {entry['config_key']} | \"\n",
    "            f\"F1={entry['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "    return top_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cdf5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 1 grid size: 10\n",
      "\n",
      "================================================================================\n",
      "PHASE 1 — Retrieval Capacity\n",
      "================================================================================\n",
      "Total configs: 10\n",
      "Completed configs: 9\n",
      "Pending configs: 1\n",
      "--------------------------------------------------------------------------------\n",
      "[1/1] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass5_win150_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs100_kpass5_win150_ovl50_promptdefault:  22%|██▏       | 22/100 [00:19<01:01,  1.26it/s]"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 1 — Retrieval Capacity (paired k_docs, k_passages)\n",
    "# ============================================================\n",
    "\n",
    "PHASE_1_GRID = []\n",
    "\n",
    "BASE_RETRIEVAL_PARAMS = {\n",
    "    \"window\": 150,\n",
    "    \"overlap\": 50,\n",
    "    \"mu\": 1000,\n",
    "    \"k1\": 0.9,\n",
    "    \"b\": 0.4,\n",
    "}\n",
    "\n",
    "CAPACITY_PAIRS = [\n",
    "    (5, 3),\n",
    "    (5, 10),\n",
    "    (10, 5),\n",
    "    (15, 5),\n",
    "    (15, 7),\n",
    "    (20, 7),\n",
    "    (20, 10),\n",
    "    (30, 10),\n",
    "    (50, 5),\n",
    "    (100, 5)\n",
    "]\n",
    "\n",
    "for k_docs, k_passages in CAPACITY_PAIRS:\n",
    "    PHASE_1_GRID.append({\n",
    "        \"retrieval_mgr\": RetrievalManager(\n",
    "            k_docs=k_docs,\n",
    "            k_passages=k_passages,\n",
    "            **BASE_RETRIEVAL_PARAMS,\n",
    "        ),\n",
    "        \"prompt_mgr\": PromptManager(),\n",
    "    })\n",
    "\n",
    "print(f\"✓ Phase 1 grid size: {len(PHASE_1_GRID)}\")\n",
    "\n",
    "PHASE_1_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 1 — Retrieval Capacity\",\n",
    "    grid=PHASE_1_GRID,\n",
    "    validation_data=validation_data,\n",
    "    top_k=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e95ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 2 grid size: 12\n",
      "\n",
      "================================================================================\n",
      "PHASE 2 — Passage Segmentation\n",
      "================================================================================\n",
      "Total configs: 12\n",
      "Completed configs: 3\n",
      "Pending configs: 9\n",
      "--------------------------------------------------------------------------------\n",
      "[1/9] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win100_ovl30_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win100_ovl30_promptdefault: 100%|██████████| 100/100 [00:43<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win100_ovl30_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=5 | window=100, overlap=30\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=17.60 | P=18.82 | R=18.75 | EM=11.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=17.6016\n",
      "[2/9] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win200_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win200_ovl50_promptdefault: 100%|██████████| 100/100 [01:07<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win200_ovl50_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=5 | window=200, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=21.26 | P=25.43 | R=21.87 | EM=11.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=21.2564\n",
      "[3/9] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win250_ovl60_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win250_ovl60_promptdefault: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win250_ovl60_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=10, k_passages=5 | window=250, overlap=60\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=16.37 | P=19.98 | R=17.95 | EM=8.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=16.3718\n",
      "[4/9] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win100_ovl30_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win100_ovl30_promptdefault: 100%|██████████| 100/100 [00:44<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win100_ovl30_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=5 | window=100, overlap=30\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=17.60 | P=18.82 | R=18.75 | EM=11.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=17.6016\n",
      "[5/9] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win200_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win200_ovl50_promptdefault: 100%|██████████| 100/100 [01:08<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win200_ovl50_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=5 | window=200, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=21.92 | P=26.43 | R=22.37 | EM=11.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=21.9231\n",
      "[6/9] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win250_ovl60_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win250_ovl60_promptdefault: 100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win250_ovl60_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=5 | window=250, overlap=60\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=17.21 | P=20.98 | R=18.62 | EM=8.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=17.2051\n",
      "[7/9] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win100_ovl30_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win100_ovl30_promptdefault: 100%|██████████| 100/100 [01:04<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win100_ovl30_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=7 | window=100, overlap=30\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=22.66 | P=25.60 | R=25.23 | EM=12.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=22.6645\n",
      "[8/9] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win200_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win200_ovl50_promptdefault: 100%|██████████| 100/100 [01:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win200_ovl50_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=7 | window=200, overlap=50\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=18.06 | P=20.44 | R=21.37 | EM=7.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=18.0610\n",
      "[9/9] Running: RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win250_ovl60_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win250_ovl60_promptdefault: 100%|██████████| 100/100 [01:25<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win250_ovl60_promptdefault\n",
      "   Retrieval: Retrieval(RRF_k=60, μ=1000, k1=0.9, b=0.4) | k_docs=15, k_passages=7 | window=250, overlap=60\n",
      "   Prompt: temp=0.0, top_p=1.0, max_tokens=256\n",
      "   F1=11.23 | P=12.62 | R=12.33 | EM=7.00\n",
      "   Questions: 100\n",
      "\n",
      "✓ F1=11.2334\n",
      "\n",
      "Top configs selected:\n",
      "1. RRF_k60_mu1000_k10.9_b0.4_kdocs10_kpass5_win150_ovl50_promptdefault | F1=26.4648\n",
      "2. RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass5_win150_ovl50_promptdefault | F1=24.9648\n",
      "3. RRF_k60_mu1000_k10.9_b0.4_kdocs15_kpass7_win100_ovl30_promptdefault | F1=22.6645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 2 — Passage Segmentation (window / overlap pairs)\n",
    "# ============================================================\n",
    "\n",
    "PHASE_2_GRID = []\n",
    "\n",
    "# Base + proportional variants\n",
    "WINDOW_OVERLAP_PAIRS = [\n",
    "    (100, 30),\n",
    "    (150, 50),  # Base\n",
    "    (200, 50),\n",
    "    (250, 60),\n",
    "]\n",
    "\n",
    "for entry in PHASE_1_TOP_CONFIGS:\n",
    "    base_mgr = entry[\"retrieval_mgr\"]\n",
    "\n",
    "    for window, overlap in WINDOW_OVERLAP_PAIRS:\n",
    "        PHASE_2_GRID.append({\n",
    "            \"retrieval_mgr\": RetrievalManager(\n",
    "                k_docs=base_mgr.k_docs,\n",
    "                k_passages=base_mgr.k_passages,\n",
    "                window=window,\n",
    "                overlap=overlap,\n",
    "                mu=base_mgr.mu,\n",
    "                k1=base_mgr.k1,\n",
    "                b=base_mgr.b,\n",
    "            ),\n",
    "            \"prompt_mgr\": PromptManager(),\n",
    "        })\n",
    "\n",
    "print(f\"✓ Phase 2 grid size: {len(PHASE_2_GRID)}\")\n",
    "\n",
    "PHASE_2_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 2 — Passage Segmentation\",\n",
    "    grid=PHASE_2_GRID,\n",
    "    validation_data=validation_data,\n",
    "    top_k=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ad0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 3 grid size: 27\n",
      "\n",
      "================================================================================\n",
      "PHASE 3 — Lexical Hyperparameters\n",
      "================================================================================\n",
      "Total configs: 27\n",
      "Completed configs: 7\n",
      "Pending configs: 20\n",
      "--------------------------------------------------------------------------------\n",
      "[1/20] Running: RRF_k60_mu2000_k10.9_b0.4_kdocs15_kpass5_win150_ovl50_promptdefault\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF_k60_mu2000_k10.9_b0.4_kdocs15_kpass5_win150_ovl50_promptdefault:  12%|█▏        | 3/25 [00:24<02:57,  8.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 39\u001b[0m\n\u001b[1;32m     24\u001b[0m             PHASE_3_GRID\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     25\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieval_mgr\u001b[39m\u001b[38;5;124m\"\u001b[39m: RetrievalManager(\n\u001b[1;32m     26\u001b[0m                     k_docs\u001b[38;5;241m=\u001b[39mbase_mgr\u001b[38;5;241m.\u001b[39mk_docs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_mgr\u001b[39m\u001b[38;5;124m\"\u001b[39m: PromptManager(),\n\u001b[1;32m     35\u001b[0m             })\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Phase 3 grid size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(PHASE_3_GRID)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m PHASE_3_TOP_CONFIGS \u001b[38;5;241m=\u001b[39m \u001b[43mrun_phase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPHASE 3 — Lexical Hyperparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPHASE_3_GRID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 42\u001b[0m, in \u001b[0;36mrun_phase\u001b[0;34m(phase_name, grid, validation_data, top_k)\u001b[0m\n\u001b[1;32m     39\u001b[0m key \u001b[38;5;241m=\u001b[39m generate_config_key(retrieval_mgr, prompt_mgr)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pending)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrieval_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretrieval_mgr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_mgr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m save_results_to_csv(result, key, EXPERIMENT_LOG_PATH)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(name, df_data, retrieval_manager, prompt_manager, max_questions, batch_size, verbose)\u001b[0m\n\u001b[1;32m     30\u001b[0m question \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m qid \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m contexts \u001b[38;5;241m=\u001b[39m \u001b[43mretrieval_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m batch_questions\u001b[38;5;241m.\u001b[39mappend(question)\n\u001b[1;32m     35\u001b[0m batch_qids\u001b[38;5;241m.\u001b[39mappend(qid)\n",
      "Cell \u001b[0;32mIn[9], line 80\u001b[0m, in \u001b[0;36mRetrievalManager.retrieve_context\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     77\u001b[0m         passages\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_passages(content))\n\u001b[1;32m     79\u001b[0m passages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(passages))\n\u001b[0;32m---> 80\u001b[0m dense_ranked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# RRF from lexical document rankings\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 60\u001b[0m, in \u001b[0;36mRetrievalManager.dense_rank\u001b[0;34m(self, query, passages)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m     59\u001b[0m q_emb \u001b[38;5;241m=\u001b[39m bi_encoder\u001b[38;5;241m.\u001b[39mencode(query, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 60\u001b[0m p_embs \u001b[38;5;241m=\u001b[39m \u001b[43mbi_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpassages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m scores \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(q_emb, p_embs)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     62\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(scores, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1062\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1061\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m-> 1062\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1064\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1621\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[0;34m(self, texts, **kwargs)\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;124;03mTokenizes the texts.\u001b[39;00m\n\u001b[1;32m   1612\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;124;03m        \"attention_mask\", and \"token_type_ids\".\u001b[39;00m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtokenize(texts)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:320\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[0;34m(self, texts, padding)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case:\n\u001b[1;32m    317\u001b[0m     to_tokenize \u001b[38;5;241m=\u001b[39m [[s\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m to_tokenize]\n\u001b[1;32m    319\u001b[0m output\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mto_tokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongest_first\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3073\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3072\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3073\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3075\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3161\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3156\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3157\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3158\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3159\u001b[0m         )\n\u001b[1;32m   3160\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 3161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3180\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   3184\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3185\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3203\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3204\u001b[0m     )\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3362\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3352\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3353\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3354\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3355\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3359\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3360\u001b[0m )\n\u001b[0;32m-> 3362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3380\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3381\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3382\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:553\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 553\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: tuple[\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m#                       list[EncodingFast]\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    565\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    567\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    577\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3 — Lexical Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "PHASE_3_GRID = []\n",
    "\n",
    "BM25_PARAMS = [\n",
    "    {\"k1\": 0.6, \"b\": 0.3},\n",
    "    {\"k1\": 0.9, \"b\": 0.4},   # baseline\n",
    "    {\"k1\": 1.2, \"b\": 0.6},\n",
    "]\n",
    "\n",
    "QLD_PARAMS = [\n",
    "    {\"mu\": 1000},           # baseline\n",
    "    {\"mu\": 2000},\n",
    "]\n",
    "\n",
    "for entry in PHASE_2_TOP_CONFIGS:\n",
    "    base_mgr = entry[\"retrieval_mgr\"]\n",
    "\n",
    "    for bm25 in BM25_PARAMS:\n",
    "        for qld in QLD_PARAMS:\n",
    "            PHASE_3_GRID.append({\n",
    "                \"retrieval_mgr\": RetrievalManager(\n",
    "                    k_docs=base_mgr.k_docs,\n",
    "                    k_passages=base_mgr.k_passages,\n",
    "                    window=base_mgr.window,\n",
    "                    overlap=base_mgr.overlap,\n",
    "                    k1=bm25[\"k1\"],\n",
    "                    b=bm25[\"b\"],\n",
    "                    mu=qld[\"mu\"],\n",
    "                ),\n",
    "                \"prompt_mgr\": PromptManager(),\n",
    "            })\n",
    "\n",
    "print(f\"✓ Phase 3 grid size: {len(PHASE_3_GRID)}\")\n",
    "\n",
    "PHASE_3_TOP_CONFIGS = run_phase(\n",
    "    phase_name=\"PHASE 3 — Lexical Hyperparameters\",\n",
    "    grid=PHASE_3_GRID,\n",
    "    validation_data=validation_data,\n",
    "    top_k=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a904e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SELECTION — Top 3 configs on 1,000 TRAIN questions\n",
    "# ============================================================\n",
    "\n",
    "FINAL_SELECTION_SEED = 123\n",
    "FINAL_SELECTION_SIZE = 1000\n",
    "\n",
    "# Take top-3 configs from Phase 3 (already sorted by F1)\n",
    "TOP_3_CONFIGS = PHASE_3_TOP_CONFIGS[:3]\n",
    "\n",
    "final_validation_data = df_train.sample(\n",
    "    n=FINAL_SELECTION_SIZE,\n",
    "    random_state=FINAL_SELECTION_SEED,\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL MODEL SELECTION ON 1,000 TRAIN QUESTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "FINAL_SELECTION_RESULTS = []\n",
    "\n",
    "for i, entry in enumerate(TOP_3_CONFIGS, 1):\n",
    "    retrieval_mgr = entry[\"retrieval_mgr\"]\n",
    "\n",
    "    prompt_mgr = PromptManager(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        user_prompt=USER_PROMPT,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "        top_p=1.0,\n",
    "        prompt_id=\"final\",\n",
    "    )\n",
    "\n",
    "    config_key = generate_config_key(retrieval_mgr, prompt_mgr)\n",
    "\n",
    "    print(f\"\\n[{i}/3] Evaluating config: {config_key}\")\n",
    "\n",
    "    result = run_experiment(\n",
    "        name=f\"{config_key}_final_select\",\n",
    "        df_data=final_validation_data,\n",
    "        retrieval_manager=retrieval_mgr,\n",
    "        prompt_manager=prompt_mgr,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"✓ F1={result['f1_score']:.4f} | \"\n",
    "        f\"EM={result['exact_match']:.4f} | \"\n",
    "        f\"P={result['precision']:.4f} | \"\n",
    "        f\"R={result['recall']:.4f}\"\n",
    "    )\n",
    "\n",
    "    FINAL_SELECTION_RESULTS.append({\n",
    "        \"retrieval_mgr\": retrieval_mgr,\n",
    "        \"prompt_mgr\": prompt_mgr,\n",
    "        \"config_key\": config_key,\n",
    "        **result,\n",
    "    })\n",
    "\n",
    "# Select the best config by F1\n",
    "FINAL_SELECTION_RESULTS.sort(\n",
    "    key=lambda x: x[\"f1_score\"],\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "BEST_FINAL_CONFIG = FINAL_SELECTION_RESULTS[0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ BEST FINAL CONFIG SELECTED\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"{BEST_FINAL_CONFIG['config_key']} | \"\n",
    "    f\"F1={BEST_FINAL_CONFIG['f1_score']:.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KAGGLE SUBMISSION — Final system on TEST set\n",
    "# ============================================================\n",
    "\n",
    "BEST_RETRIEVAL_MGR = BEST_FINAL_CONFIG[\"retrieval_mgr\"]\n",
    "BEST_PROMPT_MGR = BEST_FINAL_CONFIG[\"prompt_mgr\"]\n",
    "\n",
    "FINAL_CONFIG_KEY = BEST_FINAL_CONFIG[\"config_key\"]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KAGGLE SUBMISSION GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Using final config: {FINAL_CONFIG_KEY}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run inference only (no labels needed)\n",
    "test_questions = df_test[\"question\"].tolist()\n",
    "\n",
    "print(f\"Generating answers for {len(test_questions)} test questions...\")\n",
    "\n",
    "test_contexts = [\n",
    "    BEST_RETRIEVAL_MGR.retrieve_context(q)\n",
    "    for q in test_questions\n",
    "]\n",
    "\n",
    "test_answers = BEST_PROMPT_MGR.batch_generate_answers(\n",
    "    questions=test_questions,\n",
    "    contexts_list=test_contexts,\n",
    ")\n",
    "\n",
    "# Build Kaggle submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"],\n",
    "    \"answer\": test_answers,\n",
    "})\n",
    "\n",
    "SUBMISSION_PATH = \"./results/kaggle_submission.csv\"\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "print(f\"✓ Kaggle submission file saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"✓ Total rows: {len(submission_df)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
