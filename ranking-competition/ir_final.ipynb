{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b40988",
   "metadata": {},
   "source": [
    "# Robust04: 3-Model Neural Retrieval Pipeline\n",
    "\n",
    "**Gal Noy** · 209346486\n",
    "\n",
    "**Dataset**: TREC Robust04 - 249 queries (50 train with qrels, 199 test), ~528K documents\n",
    "\n",
    "**Pipeline**:\n",
    "1. **Model 1 - BM25**: Baseline with k1, b tuning\n",
    "2. **Model 2 - RM3**: Query expansion (fb_terms, fb_docs, original_weight)\n",
    "3. **Model 3 - Neural**: Multi-branch (RM3+SPLADE) → RRF fusion → Neural PRF+MiniLM → Score blending\n",
    "\n",
    "**Method**: Grid search for optimal parameters at each model using MAP on training queries\n",
    "\n",
    "**Output**: Three TREC runs (run_1.res, run_2.res, run_3.res) for test queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ec658",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4888f",
   "metadata": {},
   "source": [
    "### 1.1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04b58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install -y openjdk-21-jdk\n",
    "# !update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-21-openjdk-amd64/bin/java 1\n",
    "# !update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac 1\n",
    "# !update-alternatives --set java /usr/lib/jvm/java-21-openjdk-amd64/bin/java\n",
    "# !update-alternatives --set javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6da3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install transformers\n",
    "# !pip install sentence-transformers\n",
    "# !pip install pytrec_eval\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install faiss-cpu --no-cache\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff9e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[0;93m2026-01-17 10:30:24.467606034 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✓ Dependencies imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Tuple, Optional, Iterable\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import product\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from pyserini.search.lucene import LuceneSearcher, LuceneImpactSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "from pyserini.encode import SpladeQueryEncoder\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec6c3c",
   "metadata": {},
   "source": [
    "### 1.2. Load Pyserini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d89b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: robust04\n",
      "Total documents: 528,030\n",
      "Total terms: 174,540,872\n",
      "✓ Pyserini index loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jan 17, 2026 10:30:25 AM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"robust04\"\n",
    "\n",
    "index_reader = IndexReader.from_prebuilt_index(INDEX_NAME)\n",
    "\n",
    "print(f\"Index: {INDEX_NAME}\")\n",
    "print(f\"Total documents: {index_reader.stats()['documents']:,}\")\n",
    "print(f\"Total terms: {index_reader.stats()['total_terms']:,}\")\n",
    "print(\"✓ Pyserini index loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b2621",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5a950",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note**: The 50 queries with qrels are used for training/tuning (parameter optimization via grid search). The remaining 199 queries without qrels are used for generating final submission files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5e6b1",
   "metadata": {},
   "source": [
    "### 2.1. Load Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac88eb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries loaded: 249\n",
      "\n",
      "Sample queries:\n",
      "  301: international organized crime\n",
      "  302: poliomyelitis post polio\n",
      "  303: hubble telescope achievements\n",
      "  304: endangered species mammals\n",
      "  305: dangerous vehicles\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "def load_queries(filepath: str) -> Dict[str, str]:\n",
    "    \"\"\"Load queries from file. Format: qid<tab>query_text\"\"\"\n",
    "    queries = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                qid, text = parts\n",
    "                queries[qid] = text\n",
    "    return queries\n",
    "\n",
    "all_queries = load_queries(os.path.join(DATA_DIR, \"queriesROBUST.txt\"))\n",
    "\n",
    "print(f\"Total queries loaded: {len(all_queries)}\")\n",
    "print(f\"\\nSample queries:\")\n",
    "for qid, text in list(all_queries.items())[:5]:\n",
    "    print(f\"  {qid}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16000c0a",
   "metadata": {},
   "source": [
    "### 2.2. Load Relevance Judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a1a9a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries with relevance judgments: 50\n",
      "Total judgments: 61,511\n",
      "\n",
      "Sample qrels for query 301:\n",
      "  FBIS3-10082: 1\n",
      "  FBIS3-10169: 0\n",
      "  FBIS3-10243: 1\n",
      "  FBIS3-10319: 0\n",
      "  FBIS3-10397: 1\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(filepath: str) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"Load qrels. Format: qid 0 docid relevance\"\"\"\n",
    "    qrels = defaultdict(dict)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 4:\n",
    "                qid, _, docid, rel = parts[:4]\n",
    "                qrels[qid][docid] = int(rel)\n",
    "    return dict(qrels)\n",
    "\n",
    "qrels = load_qrels(os.path.join(DATA_DIR, \"qrels_50_Queries\"))\n",
    "\n",
    "print(f\"Queries with relevance judgments: {len(qrels)}\")\n",
    "print(f\"Total judgments: {sum(len(v) for v in qrels.values()):,}\")\n",
    "print(f\"\\nSample qrels for query 301:\")\n",
    "sample_rels = list(qrels.get(\"301\", {}).items())[:5]\n",
    "for docid, rel in sample_rels:\n",
    "    print(f\"  {docid}: {rel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95332a",
   "metadata": {},
   "source": [
    "### 2.3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5779bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training queries: 50 (with qrels)\n",
      "Test queries: 199 (no qrels)\n",
      "\n",
      "Train QIDs: ['301', '302', '303', '304', '305', '306', '307', '308', '309', '310']...\n",
      "Test QIDs: ['351', '352', '353', '354', '355', '356', '357', '358', '359', '360']...\n"
     ]
    }
   ],
   "source": [
    "train_qids = sorted(qrels.keys())\n",
    "test_qids = [qid for qid in all_queries.keys() if qid not in train_qids]\n",
    "\n",
    "train_queries = {qid: all_queries[qid] for qid in train_qids}\n",
    "test_queries = {qid: all_queries[qid] for qid in test_qids}\n",
    "\n",
    "print(f\"Training queries: {len(train_queries)} (with qrels)\")\n",
    "print(f\"Test queries: {len(test_queries)} (no qrels)\")\n",
    "print(f\"\\nTrain QIDs: {train_qids[:10]}...\")\n",
    "print(f\"Test QIDs: {test_qids[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2aa8a3",
   "metadata": {},
   "source": [
    "## 3. Evaluation Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b429060",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Evaluation Strategy**: All experiments use MAP (Mean Average Precision) computed with pytrec_eval. Grid search explores parameter combinations, caching results to avoid redundant computation. Best parameters from each model are carried forward to the next model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c0234",
   "metadata": {},
   "source": [
    "### 3.1. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab176d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def compute_map(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    qrels: Dict[str, Dict[str, int]]\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute MAP using pytrec_eval (TREC standard).\n",
    "\n",
    "    Returns:\n",
    "        map_score: float\n",
    "        per_query_ap: Dict[qid, AP]\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert run to pytrec_eval format: {qid: {docid: score}}\n",
    "    run_dict = {\n",
    "        qid: {docid: score for docid, score in docs}\n",
    "        for qid, docs in run.items()\n",
    "    }\n",
    "\n",
    "    # Convert qrels to pytrec_eval format: {qid: {docid: relevance}}\n",
    "    qrels_dict = {\n",
    "        qid: dict(docs)\n",
    "        for qid, docs in qrels.items()\n",
    "    }\n",
    "\n",
    "    missing_qids = set(qrels_dict) - set(run_dict)\n",
    "    assert not missing_qids, (\n",
    "        f\"Missing queries in run: {sorted(list(missing_qids))[:10]}\"\n",
    "    )\n",
    "\n",
    "    for qid, docs in run.items():\n",
    "        docids = [d for d, _ in docs]\n",
    "        assert len(docids) == len(set(docids)), (\n",
    "            f\"Duplicate docIDs in run for query {qid}\"\n",
    "        )\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels_dict, {\"map\"})\n",
    "    results = evaluator.evaluate(run_dict)\n",
    "\n",
    "    per_query_ap = {qid: m[\"map\"] for qid, m in results.items()}\n",
    "\n",
    "    map_score = pytrec_eval.compute_aggregated_measure(\n",
    "        \"map\",\n",
    "        list(per_query_ap.values())\n",
    "    )\n",
    "\n",
    "    return map_score, per_query_ap\n",
    "\n",
    "\n",
    "def evaluate_run(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    qrels: Dict[str, Dict[str, int]],\n",
    "    run_name: str = \"run\"\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate a run using pytrec_eval and return metrics.\"\"\"\n",
    "    map_score, per_query_ap = compute_map(run, qrels)\n",
    "    \n",
    "    return {\n",
    "        \"run_name\": run_name,\n",
    "        \"map\": map_score,\n",
    "        \"num_queries\": len(per_query_ap),\n",
    "        \"per_query_ap\": per_query_ap\n",
    "    }\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1ce08",
   "metadata": {},
   "source": [
    "### 3.2. Caching Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357dddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Caching utilities defined\n"
     ]
    }
   ],
   "source": [
    "def get_results_csv(model_num: int = None) -> str:\n",
    "    \"\"\"Get CSV filename for a specific model.\"\"\"\n",
    "    if model_num is None:\n",
    "        return \"./experiments.csv\"\n",
    "    return f\"./experiments_model{model_num}.csv\"\n",
    "\n",
    "\n",
    "def generate_config_key(method: str, params: Dict) -> str:\n",
    "    \"\"\"Generate unique config key for caching.\"\"\"\n",
    "    parts = [method]\n",
    "    for k, v in sorted(params.items()):\n",
    "        parts.append(f\"{k}={v}\")\n",
    "    return \"__\".join(parts)\n",
    "\n",
    "\n",
    "def load_cached_result(config_key: str, model_num: int = None) -> Optional[Dict]:\n",
    "    \"\"\"Load cached result if exists.\"\"\"\n",
    "    results_csv = get_results_csv(model_num)\n",
    "    \n",
    "    if not os.path.exists(results_csv):\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(results_csv)\n",
    "    row = df[df[\"config_key\"] == config_key]\n",
    "    \n",
    "    if row.empty:\n",
    "        return None\n",
    "    \n",
    "    return row.iloc[0].to_dict()\n",
    "\n",
    "\n",
    "def save_experiment_result(result: Dict, model_num: int = None):\n",
    "    \"\"\"Save experiment result to model-specific cache.\"\"\"\n",
    "    results_csv = get_results_csv(model_num)\n",
    "    df_row = pd.DataFrame([result])\n",
    "    \n",
    "    if not os.path.exists(results_csv):\n",
    "        df_row.to_csv(results_csv, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(results_csv, mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "def load_completed_configs(model_num: int = None) -> set:\n",
    "    \"\"\"Load set of completed experiment config keys from CSV.\"\"\"\n",
    "    results_csv = get_results_csv(model_num)\n",
    "    if not os.path.exists(results_csv):\n",
    "        return set()\n",
    "    df = pd.read_csv(results_csv)\n",
    "    return set(df[\"config_key\"])\n",
    "\n",
    "\n",
    "print(\"✓ Caching utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634949d",
   "metadata": {},
   "source": [
    "### 3.3. Base Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea6a3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BaseRetriever class defined\n"
     ]
    }
   ],
   "source": [
    "class BaseRetriever(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all retrieval models.\n",
    "    All models must implement search() and get_params().\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, index_name: str = \"robust04\"):\n",
    "        self.index_name = index_name\n",
    "    \n",
    "    @abstractmethod\n",
    "    def search(\n",
    "        self,\n",
    "        queries: Dict[str, str],\n",
    "        k: int = 1000\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        \"\"\"Search for all queries and return ranked results.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_params(self) -> Dict:\n",
    "        \"\"\"Return model parameters for logging.\"\"\"\n",
    "        return {}\n",
    "\n",
    "\n",
    "print(\"✓ BaseRetriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023aa3c",
   "metadata": {},
   "source": [
    "### 3.4. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b40836a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Experiment framework defined\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    config_key: str,\n",
    "    model_name: str,\n",
    "    model_class: type,\n",
    "    model_params: Dict,\n",
    "    queries: Dict[str, str],\n",
    "    qrels: Dict[str, Dict[str, int]]\n",
    ") -> Dict:\n",
    "    \"\"\"Run a single experiment using a retriever class.\"\"\"\n",
    "    model = model_class(**model_params)\n",
    "    run = model.search(queries, k=1000)\n",
    "    metrics = evaluate_run(run, qrels, config_key)\n",
    "    \n",
    "    return {\n",
    "        \"config_key\": config_key,\n",
    "        \"method\": model_name,\n",
    "        **model.get_params(),\n",
    "        \"map\": metrics[\"map\"],\n",
    "        \"num_queries\": metrics[\"num_queries\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def run_grid_search(\n",
    "    method_name: str,\n",
    "    model_class: type,\n",
    "    param_grid: Dict[str, list],\n",
    "    queries: Dict[str, str],\n",
    "    qrels: Dict[str, Dict[str, int]],\n",
    "    model_num: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Grid search for retriever classes with model-specific caching.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Print header\n",
    "    model_str = f\"MODEL {model_num}: \" if model_num else \"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{model_str}Tuning {method_name} parameters\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    \n",
    "    for combo in product(*param_values):\n",
    "        params = dict(zip(param_names, combo))\n",
    "        config_key = generate_config_key(method_name, params)\n",
    "        \n",
    "        # Check model-specific cache\n",
    "        cached = load_cached_result(config_key, model_num)\n",
    "        if cached is not None:\n",
    "            param_str = \", \".join([f\"{k}={v}\" for k, v in params.items()])\n",
    "            print(f\"{param_str} -> MAP={cached['map']:.4f} [CACHED]\")\n",
    "            results.append(cached)\n",
    "            continue\n",
    "        \n",
    "        # Run experiment\n",
    "        param_str = \", \".join([f\"{k}={v}\" for k, v in params.items()])\n",
    "        print(f\"{param_str} -> Running...\", end=\" \")\n",
    "        \n",
    "        result = run_experiment(\n",
    "            config_key=config_key,\n",
    "            model_name=method_name,\n",
    "            model_class=model_class,\n",
    "            model_params=params,\n",
    "            queries=queries,\n",
    "            qrels=qrels\n",
    "        )\n",
    "        \n",
    "        # Save to model-specific cache\n",
    "        save_experiment_result(result, model_num)\n",
    "        print(f\"MAP={result['map']:.4f}\")\n",
    "        results.append(result)\n",
    "    \n",
    "    # Create and sort results dataframe\n",
    "    df = pd.DataFrame(results).sort_values(\"map\", ascending=False)\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{method_name} Tuning Results (Top 5):\")\n",
    "    print(\"=\"*60)\n",
    "    display(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Experiment framework defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e034cf",
   "metadata": {},
   "source": [
    "## 4. MODEL 1: BM25 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efee42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Model 1 Overview**: BM25 is a probabilistic ranking function based on term frequency and document length normalization. We tune k1 (term frequency saturation) and b (length normalization) to find optimal retrieval parameters for Robust04."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ad949",
   "metadata": {},
   "source": [
    "### 4.1. BM25 Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be810bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BM25Retriever class defined\n"
     ]
    }
   ],
   "source": [
    "class BM25Retriever(BaseRetriever):\n",
    "    \"\"\"BM25 retrieval model.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        k1: float = 0.9,\n",
    "        b: float = 0.4,\n",
    "        index_name: str = \"robust04\"\n",
    "    ):\n",
    "        super().__init__(index_name)\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self._searcher = None\n",
    "    \n",
    "    @property\n",
    "    def searcher(self) -> LuceneSearcher:\n",
    "        if self._searcher is None:\n",
    "            self._searcher = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "            self._searcher.set_bm25(k1=self.k1, b=self.b)\n",
    "        return self._searcher\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        queries: Dict[str, str],\n",
    "        k: int = 1000\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        results = {}\n",
    "        for qid, query_text in tqdm(queries.items(), desc=\"BM25 Search\"):\n",
    "            hits = self.searcher.search(query_text, k=k)\n",
    "            results[qid] = [(hit.docid, hit.score) for hit in hits]\n",
    "        return results\n",
    "    \n",
    "    def get_params(self) -> Dict:\n",
    "        return {\"k1\": self.k1, \"b\": self.b}\n",
    "\n",
    "\n",
    "print(\"✓ BM25Retriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882ee79",
   "metadata": {},
   "source": [
    "### 4.2. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df83f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL 1: Tuning BM25 parameters\n",
      "============================================================\n",
      "\n",
      "k1=0.6, b=0.3 -> MAP=0.2463 [CACHED]\n",
      "k1=0.6, b=0.4 -> MAP=0.2475 [CACHED]\n",
      "k1=0.6, b=0.5 -> MAP=0.2453 [CACHED]\n",
      "k1=0.6, b=0.6 -> MAP=0.2422 [CACHED]\n",
      "k1=0.6, b=0.75 -> MAP=0.2392 [CACHED]\n",
      "k1=0.9, b=0.3 -> MAP=0.2450 [CACHED]\n",
      "k1=0.9, b=0.4 -> MAP=0.2455 [CACHED]\n",
      "k1=0.9, b=0.5 -> MAP=0.2442 [CACHED]\n",
      "k1=0.9, b=0.6 -> MAP=0.2415 [CACHED]\n",
      "k1=0.9, b=0.75 -> MAP=0.2374 [CACHED]\n",
      "k1=1.2, b=0.3 -> MAP=0.2426 [CACHED]\n",
      "k1=1.2, b=0.4 -> MAP=0.2427 [CACHED]\n",
      "k1=1.2, b=0.5 -> MAP=0.2424 [CACHED]\n",
      "k1=1.2, b=0.6 -> MAP=0.2400 [CACHED]\n",
      "k1=1.2, b=0.75 -> MAP=0.2334 [CACHED]\n",
      "k1=1.5, b=0.3 -> MAP=0.2392 [CACHED]\n",
      "k1=1.5, b=0.4 -> MAP=0.2396 [CACHED]\n",
      "k1=1.5, b=0.5 -> MAP=0.2384 [CACHED]\n",
      "k1=1.5, b=0.6 -> MAP=0.2360 [CACHED]\n",
      "k1=1.5, b=0.75 -> MAP=0.2307 [CACHED]\n",
      "k1=2.0, b=0.3 -> MAP=0.2320 [CACHED]\n",
      "k1=2.0, b=0.4 -> MAP=0.2330 [CACHED]\n",
      "k1=2.0, b=0.5 -> MAP=0.2324 [CACHED]\n",
      "k1=2.0, b=0.6 -> MAP=0.2292 [CACHED]\n",
      "k1=2.0, b=0.75 -> MAP=0.2250 [CACHED]\n",
      "\n",
      "============================================================\n",
      "BM25 Tuning Results (Top 5):\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>method</th>\n",
       "      <th>k1</th>\n",
       "      <th>b</th>\n",
       "      <th>map</th>\n",
       "      <th>num_queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25__b=0.4__k1=0.6</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.247464</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25__b=0.3__k1=0.6</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.246328</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BM25__b=0.4__k1=0.9</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.245466</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BM25__b=0.5__k1=0.6</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.245285</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BM25__b=0.3__k1=0.9</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.244954</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            config_key method   k1    b       map  num_queries\n",
       "1  BM25__b=0.4__k1=0.6   BM25  0.6  0.4  0.247464           50\n",
       "0  BM25__b=0.3__k1=0.6   BM25  0.6  0.3  0.246328           50\n",
       "6  BM25__b=0.4__k1=0.9   BM25  0.9  0.4  0.245466           50\n",
       "2  BM25__b=0.5__k1=0.6   BM25  0.6  0.5  0.245285           50\n",
       "5  BM25__b=0.3__k1=0.9   BM25  0.9  0.3  0.244954           50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BM25_PARAM_GRID = {\n",
    "    \"k1\": [0.6, 0.9, 1.2, 1.5, 2.0],\n",
    "    \"b\": [0.3, 0.4, 0.5, 0.6, 0.75]\n",
    "}\n",
    "\n",
    "# Run grid search with BM25Retriever class\n",
    "bm25_df = run_grid_search(\n",
    "    method_name=\"BM25\",\n",
    "    model_class=BM25Retriever,\n",
    "    param_grid=BM25_PARAM_GRID,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    model_num=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522b7f3",
   "metadata": {},
   "source": [
    "### 4.3. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f82722cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL 1 COMPLETE: Best BM25 Parameters\n",
      "============================================================\n",
      "  k1 = 0.6\n",
      "  b = 0.4\n",
      "  MAP = 0.2475\n",
      "============================================================\n",
      "\n",
      "✓ Best BM25 model ready for Model 2\n"
     ]
    }
   ],
   "source": [
    "best_bm25 = bm25_df.iloc[0]\n",
    "best_k1, best_b = best_bm25[\"k1\"], best_bm25[\"b\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 1 COMPLETE: Best BM25 Parameters\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  k1 = {best_k1}\")\n",
    "print(f\"  b = {best_b}\")\n",
    "print(f\"  MAP = {best_bm25['map']:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create best BM25 model\n",
    "bm25_model = BM25Retriever(k1=best_k1, b=best_b)\n",
    "\n",
    "print(\"\\n✓ Best BM25 model ready for Model 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90683dc7",
   "metadata": {},
   "source": [
    "## 5. MODEL 2: BM25 + RM3 (Query Expansion)\n",
    "\n",
    "Using best BM25 parameters from Model 1 to tune RM3 hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9d2ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Model 2 Overview**: RM3 (Relevance Model 3) improves retrieval through pseudo-relevance feedback. It expands the original query with terms from top-ranked documents, then re-retrieves with the expanded query. Parameters: fb_terms (expansion terms), fb_docs (feedback documents), original_weight (balance between original and expanded query)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af424f9",
   "metadata": {},
   "source": [
    "### 5.1. RM3 Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "420d3e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RM3Retriever class defined\n"
     ]
    }
   ],
   "source": [
    "class RM3Retriever(BaseRetriever):\n",
    "    \"\"\"BM25 + RM3 pseudo-relevance feedback.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        k1: float = 0.9,\n",
    "        b: float = 0.4,\n",
    "        fb_terms: int = 10,\n",
    "        fb_docs: int = 10,\n",
    "        original_weight: float = 0.5,\n",
    "        index_name: str = \"robust04\"\n",
    "    ):\n",
    "        super().__init__(index_name)\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.fb_terms = fb_terms\n",
    "        self.fb_docs = fb_docs\n",
    "        self.original_weight = original_weight\n",
    "        self._searcher = None\n",
    "    \n",
    "    @property\n",
    "    def searcher(self) -> LuceneSearcher:\n",
    "        if self._searcher is None:\n",
    "            self._searcher = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "            self._searcher.set_bm25(k1=self.k1, b=self.b)\n",
    "            self._searcher.set_rm3(\n",
    "                fb_terms=self.fb_terms,\n",
    "                fb_docs=self.fb_docs,\n",
    "                original_query_weight=self.original_weight\n",
    "            )\n",
    "        return self._searcher\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        queries: Dict[str, str],\n",
    "        k: int = 1000\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        results = {}\n",
    "        for qid, query_text in tqdm(queries.items(), desc=\"RM3 Search\"):\n",
    "            hits = self.searcher.search(query_text, k=k)\n",
    "            results[qid] = [(hit.docid, hit.score) for hit in hits]\n",
    "        return results\n",
    "    \n",
    "    def get_params(self) -> Dict:\n",
    "        return {\n",
    "            \"k1\": self.k1,\n",
    "            \"b\": self.b,\n",
    "            \"fb_terms\": self.fb_terms,\n",
    "            \"fb_docs\": self.fb_docs,\n",
    "            \"original_weight\": self.original_weight\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✓ RM3Retriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0426d107",
   "metadata": {},
   "source": [
    "### 5.2. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91071833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best BM25 params: k1=0.6, b=0.4\n",
      "\n",
      "============================================================\n",
      "MODEL 2: Tuning RM3 parameters\n",
      "============================================================\n",
      "\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=5, original_weight=0.4 -> MAP=0.2591 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=5, original_weight=0.5 -> MAP=0.2654 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=5, original_weight=0.6 -> MAP=0.2660 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=10, original_weight=0.4 -> MAP=0.2520 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=10, original_weight=0.5 -> MAP=0.2570 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=10, original_weight=0.6 -> MAP=0.2599 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=15, original_weight=0.4 -> MAP=0.2548 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=15, original_weight=0.5 -> MAP=0.2597 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=15, original_weight=0.6 -> MAP=0.2615 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=5, original_weight=0.4 -> MAP=0.2699 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=5, original_weight=0.5 -> MAP=0.2709 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=5, original_weight=0.6 -> MAP=0.2702 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=10, original_weight=0.4 -> MAP=0.2631 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=10, original_weight=0.5 -> MAP=0.2666 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=10, original_weight=0.6 -> MAP=0.2641 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=15, original_weight=0.4 -> MAP=0.2688 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=15, original_weight=0.5 -> MAP=0.2738 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=15, original_weight=0.6 -> MAP=0.2698 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=5, original_weight=0.4 -> MAP=0.2722 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=5, original_weight=0.5 -> MAP=0.2713 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=5, original_weight=0.6 -> MAP=0.2685 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=10, original_weight=0.4 -> MAP=0.2633 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=10, original_weight=0.5 -> MAP=0.2657 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=10, original_weight=0.6 -> MAP=0.2659 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=15, original_weight=0.4 -> MAP=0.2737 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=15, original_weight=0.5 -> MAP=0.2717 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=15, original_weight=0.6 -> MAP=0.2680 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=5, original_weight=0.4 -> MAP=0.2725 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=5, original_weight=0.5 -> MAP=0.2719 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=5, original_weight=0.6 -> MAP=0.2675 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=10, original_weight=0.4 -> MAP=0.2625 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=10, original_weight=0.5 -> MAP=0.2647 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=10, original_weight=0.6 -> MAP=0.2670 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=15, original_weight=0.4 -> MAP=0.2699 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=15, original_weight=0.5 -> MAP=0.2709 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=15, original_weight=0.6 -> MAP=0.2695 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=5, original_weight=0.4 -> MAP=0.2767 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=5, original_weight=0.5 -> MAP=0.2727 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=5, original_weight=0.6 -> MAP=0.2671 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=10, original_weight=0.4 -> MAP=0.2665 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=10, original_weight=0.5 -> MAP=0.2655 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=10, original_weight=0.6 -> MAP=0.2660 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=15, original_weight=0.4 -> MAP=0.2698 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=15, original_weight=0.5 -> MAP=0.2745 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=15, original_weight=0.6 -> MAP=0.2686 [CACHED]\n",
      "\n",
      "============================================================\n",
      "RM3 Tuning Results (Top 5):\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>method</th>\n",
       "      <th>k1</th>\n",
       "      <th>b</th>\n",
       "      <th>fb_terms</th>\n",
       "      <th>fb_docs</th>\n",
       "      <th>original_weight</th>\n",
       "      <th>map</th>\n",
       "      <th>num_queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.276720</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RM3__b=0.4__fb_docs=15__fb_terms=100__k1=0.6__...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.274451</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RM3__b=0.4__fb_docs=15__fb_terms=25__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.273836</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RM3__b=0.4__fb_docs=15__fb_terms=50__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.273685</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.272742</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           config_key method   k1    b  \\\n",
       "36  RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...    RM3  0.6  0.4   \n",
       "43  RM3__b=0.4__fb_docs=15__fb_terms=100__k1=0.6__...    RM3  0.6  0.4   \n",
       "16  RM3__b=0.4__fb_docs=15__fb_terms=25__k1=0.6__o...    RM3  0.6  0.4   \n",
       "24  RM3__b=0.4__fb_docs=15__fb_terms=50__k1=0.6__o...    RM3  0.6  0.4   \n",
       "37  RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...    RM3  0.6  0.4   \n",
       "\n",
       "    fb_terms  fb_docs  original_weight       map  num_queries  \n",
       "36       100        5              0.4  0.276720           50  \n",
       "43       100       15              0.5  0.274451           50  \n",
       "16        25       15              0.5  0.273836           50  \n",
       "24        50       15              0.4  0.273685           50  \n",
       "37       100        5              0.5  0.272742           50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RM3_PARAM_GRID = {\n",
    "    \"k1\": [best_k1],        # fixed from BM25 tuning\n",
    "    \"b\": [best_b],          # fixed from BM25 tuning\n",
    "    \"fb_terms\": [10, 25, 50, 75, 100],\n",
    "    \"fb_docs\": [5, 10, 15],\n",
    "    \"original_weight\": [0.4, 0.5, 0.6],\n",
    "}\n",
    "\n",
    "# Run grid search with RM3Retriever class\n",
    "print(f\"Using best BM25 params: k1={best_k1}, b={best_b}\\n\")\n",
    "\n",
    "rm3_df = run_grid_search(\n",
    "    method_name=\"RM3\",\n",
    "    model_class=RM3Retriever,\n",
    "    param_grid=RM3_PARAM_GRID,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    model_num=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71ccf2",
   "metadata": {},
   "source": [
    "### 5.3. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aa11dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL 2 COMPLETE: Best RM3 Parameters\n",
      "============================================================\n",
      "  BM25 k1 = 0.6\n",
      "  BM25 b = 0.4\n",
      "  fb_terms = 100\n",
      "  fb_docs = 5\n",
      "  original_weight = 0.4\n",
      "  MAP = 0.2767\n",
      "\n",
      "  Improvement over BM25: +11.82%\n",
      "============================================================\n",
      "\n",
      "✓ Best RM3 model ready for Model 3\n"
     ]
    }
   ],
   "source": [
    "best_rm3 = rm3_df.iloc[0]\n",
    "best_fb_terms = int(best_rm3[\"fb_terms\"])\n",
    "best_fb_docs = int(best_rm3[\"fb_docs\"])\n",
    "best_orig_w = best_rm3[\"original_weight\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 2 COMPLETE: Best RM3 Parameters\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  BM25 k1 = {best_k1}\")\n",
    "print(f\"  BM25 b = {best_b}\")\n",
    "print(f\"  fb_terms = {best_fb_terms}\")\n",
    "print(f\"  fb_docs = {best_fb_docs}\")\n",
    "print(f\"  original_weight = {best_orig_w}\")\n",
    "print(f\"  MAP = {best_rm3['map']:.4f}\")\n",
    "\n",
    "improvement = (best_rm3['map'] - best_bm25['map']) / best_bm25['map'] * 100\n",
    "print(f\"\\n  Improvement over BM25: {improvement:+.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create best RM3 model\n",
    "rm3_model = RM3Retriever(\n",
    "    k1=best_k1, b=best_b,\n",
    "    fb_terms=best_fb_terms, fb_docs=best_fb_docs, original_weight=best_orig_w\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Best RM3 model ready for Model 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece92b96",
   "metadata": {},
   "source": [
    "## 6. MODEL 3: Advanced Neural Reranking\n",
    "\n",
    "Using best BM25/RM3 parameters from Models 1-2 as baseline for advanced neural reranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5ddf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Model 3 Architecture** (4-Stage Pipeline):\n",
    "\n",
    "1. **Multi-Branch Generation**: RM3 + SPLADE → initial candidates\n",
    "2. **RRF Fusion**: Merge RM3 + SPLADE with rrf_k parameter\n",
    "3. **Neural PRF + MiniLM MaxP**: \n",
    "   - Extract top passages from feedback docs using cross-encoder\n",
    "   - Expand query with selected passages (respects token budget)\n",
    "   - Rerank documents by best-passage score\n",
    "4. **Score Blending**: Normalize and interpolate RRF + MiniLM with alpha parameter\n",
    "\n",
    "**Model**: cross-encoder/ms-marco-MiniLM-L-6-v2 (passage ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6917a4b",
   "metadata": {},
   "source": [
    "### 6.1. Model 3 Architecture Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2017ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MultiBranchRetriever ready\n"
     ]
    }
   ],
   "source": [
    "class MultiBranchRetriever(BaseRetriever):\n",
    "    \"\"\"Robust04-optimized multi-stage retrieval with controlled neural PRF.\"\"\"\n",
    "\n",
    "    TEXT_RE = re.compile(r\"<TEXT>(.*?)</TEXT>\", re.DOTALL | re.IGNORECASE)\n",
    "    TAG_RE  = re.compile(r\"<[^>]+>\")\n",
    "    TOK_RE  = re.compile(r\"[A-Za-z]+(?:-[A-Za-z]+)?\")\n",
    "\n",
    "    STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        k1: float = 0.9,\n",
    "        b: float = 0.4,\n",
    "        fb_terms: int = 80,\n",
    "        fb_docs: int = 10,\n",
    "        original_weight: float = 0.3,\n",
    "        rrf_k: int = 20,\n",
    "        prf_fb_docs: int = 5,\n",
    "        prf_top_passages: int = 10,\n",
    "        passage_size: int = 192,\n",
    "        passage_stride: int = 128,\n",
    "        expansion_terms: int = 12,\n",
    "        expansion_token_budget: int = 40,\n",
    "        rerank_k: int = 200,\n",
    "        batch_size: int = 64,\n",
    "        alpha: float = 0.35,\n",
    "        index_name: str = \"robust04\",\n",
    "        splade_index_name: str = \"beir-v1.0.0-robust04.splade-pp-ed\",\n",
    "        model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "        device: str = DEVICE,\n",
    "    ):\n",
    "        super().__init__(index_name)\n",
    "        \n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.fb_terms = fb_terms\n",
    "        self.fb_docs = fb_docs\n",
    "        self.original_weight = original_weight\n",
    "        self.rrf_k = rrf_k\n",
    "\n",
    "        self.prf_fb_docs = prf_fb_docs\n",
    "        self.prf_top_passages = prf_top_passages\n",
    "\n",
    "        self.passage_size = passage_size\n",
    "        self.passage_stride = passage_stride\n",
    "\n",
    "        self.expansion_terms = expansion_terms\n",
    "        self.expansion_token_budget = expansion_token_budget\n",
    "\n",
    "        self.rerank_k = rerank_k\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.splade_index_name = splade_index_name\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "\n",
    "        self._rm3 = self._splade = self._doc_searcher = self._reranker = None\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Searchers\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    @property\n",
    "    def rm3(self):\n",
    "        if self._rm3 is None:\n",
    "            s = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "            s.set_bm25(self.k1, self.b)\n",
    "            s.set_rm3(self.fb_terms, self.fb_docs, self.original_weight)\n",
    "            self._rm3 = s\n",
    "        return self._rm3\n",
    "\n",
    "    @property\n",
    "    def splade(self):\n",
    "        if self._splade is None:\n",
    "            self._splade = LuceneImpactSearcher.from_prebuilt_index(\n",
    "                self.splade_index_name,\n",
    "                SpladeQueryEncoder(\"naver/splade-cocondenser-ensembledistil\", device=self.device),\n",
    "            )\n",
    "        return self._splade\n",
    "\n",
    "    @property\n",
    "    def doc_searcher(self):\n",
    "        if self._doc_searcher is None:\n",
    "            self._doc_searcher = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "        return self._doc_searcher\n",
    "\n",
    "    @property\n",
    "    def reranker(self):\n",
    "        if self._reranker is None:\n",
    "            self._reranker = CrossEncoder(self.model_name, max_length=512, device=self.device)\n",
    "        return self._reranker\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Document & Passage Utilities\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    @lru_cache(maxsize=10000)\n",
    "    def _doc_text(self, docid: str) -> str:\n",
    "        doc = self.doc_searcher.doc(docid)\n",
    "        if not doc:\n",
    "            return \"\"\n",
    "        raw = doc.raw() or \"\"\n",
    "        text = \" \".join(self.TEXT_RE.findall(raw)) or self.TAG_RE.sub(\" \", raw)\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    @lru_cache(maxsize=10000)\n",
    "    def _doc_passages(self, docid: str) -> List[str]:\n",
    "        words = self._doc_text(docid).split()\n",
    "        return [\n",
    "            \" \".join(words[i:i + self.passage_size])\n",
    "            for i in range(0, len(words), self.passage_stride)\n",
    "            if len(words[i:i + self.passage_size]) >= 30\n",
    "        ]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Tokenization & PRF\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def _tokens(self, text: str) -> List[str]:\n",
    "        return [\n",
    "            t.lower() for t in self.TOK_RE.findall(text)\n",
    "            if 3 <= len(t) <= 22 and t.lower() not in self.STOPWORDS\n",
    "        ]\n",
    "\n",
    "    def _limit_query(self, q: str, expansion: str) -> str:\n",
    "        base = self._tokens(q)\n",
    "        extra = [t for t in self._tokens(expansion) if t not in base]\n",
    "        kept = base + extra[: max(0, self.expansion_token_budget - len(base))]\n",
    "        return q + (\" \" + \" \".join(kept[len(base):]) if len(kept) > len(base) else \"\")\n",
    "\n",
    "    def _prf_terms(self, query: str, docids: List[str]) -> str:\n",
    "        pairs, passages = [], []\n",
    "\n",
    "        for d in docids[: self.prf_fb_docs]:\n",
    "            for p in self._doc_passages(d):\n",
    "                pairs.append([query, p])\n",
    "                passages.append(p)\n",
    "\n",
    "        if not pairs:\n",
    "            return \"\"\n",
    "\n",
    "        scores = self.reranker.predict(pairs, batch_size=self.batch_size, show_progress_bar=False)\n",
    "        top_passages = [p for p, _ in sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)\n",
    "                        [: self.prf_top_passages]]\n",
    "\n",
    "        tf = Counter()\n",
    "        for p in top_passages:\n",
    "            tf.update(self._tokens(p))\n",
    "\n",
    "        return \" \".join(t for t, _ in tf.most_common(self.expansion_terms))\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Fusion Helpers\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def _rrf(self, runs: List[Dict[str, List[Tuple[str, float]]]], k: int):\n",
    "        fused = defaultdict(lambda: defaultdict(float))\n",
    "        for run in runs:\n",
    "            for qid, ranked in run.items():\n",
    "                for r, (d, _) in enumerate(ranked, 1):\n",
    "                    fused[qid][d] += 1 / (self.rrf_k + r)\n",
    "        return {q: sorted(v.items(), key=lambda x: x[1], reverse=True)[:k] for q, v in fused.items()}\n",
    "\n",
    "    def _maxp(self, scores: Iterable[float], docids: Iterable[str]):\n",
    "        out = defaultdict(lambda: -1e9)\n",
    "        for s, d in zip(scores, docids):\n",
    "            out[d] = max(out[d], float(s))\n",
    "        return out\n",
    "\n",
    "    def _norm(self, xs: Dict[str, float]):\n",
    "        if not xs:\n",
    "            return {}\n",
    "        lo, hi = min(xs.values()), max(xs.values())\n",
    "        return {k: 0.0 if hi == lo else (v - lo) / (hi - lo) for k, v in xs.items()}\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Main Search\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def search(self, queries: Dict[str, str], k: int = 1000):\n",
    "        rm3 = {q: [(h.docid, h.score) for h in self.rm3.search(t, k)]\n",
    "               for q, t in tqdm(queries.items(), desc=\"RM3\")}\n",
    "\n",
    "        spl = {q: [(h.docid, h.score) for h in self.splade.search(t, k)]\n",
    "               for q, t in tqdm(queries.items(), desc=\"SPLADE\")}\n",
    "\n",
    "        fused = self._rrf([rm3, spl], k)\n",
    "        fused_scores = {q: dict(v) for q, v in fused.items()}\n",
    "\n",
    "        neural_scores = {}\n",
    "\n",
    "        for qid, q in tqdm(queries.items(), desc=\"PRF + MiniLM\"):\n",
    "            docs = [d for d, _ in fused[qid]]\n",
    "            exp = self._prf_terms(q, docs)\n",
    "            rq = self._limit_query(q, exp)\n",
    "\n",
    "            pairs, meta = [], []\n",
    "            for d in docs[: self.rerank_k]:\n",
    "                for p in self._doc_passages(d):\n",
    "                    pairs.append([rq, p])\n",
    "                    meta.append(d)\n",
    "\n",
    "            if pairs:\n",
    "                s = self.reranker.predict(pairs, batch_size=self.batch_size, show_progress_bar=False)\n",
    "                neural_scores[qid] = self._maxp(s, meta)\n",
    "            else:\n",
    "                neural_scores[qid] = {}\n",
    "\n",
    "        final = {}\n",
    "        for q in queries:\n",
    "            b, n = self._norm(fused_scores.get(q, {})), self._norm(neural_scores.get(q, {}))\n",
    "            final[q] = sorted(\n",
    "                {d: (1 - self.alpha) * b.get(d, 0) + self.alpha * n.get(d, 0)\n",
    "                 for d in set(b) | set(n)}.items(),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:k]\n",
    "\n",
    "        return final\n",
    "\n",
    "    def get_params(self) -> Dict:\n",
    "        return {k: getattr(self, k) for k in [\n",
    "            \"k1\",\"b\",\"fb_terms\",\"fb_docs\",\"original_weight\",\"rrf_k\",\n",
    "            \"prf_fb_docs\",\"prf_top_passages\",\"passage_size\",\"passage_stride\",\n",
    "            \"expansion_terms\",\"expansion_token_budget\",\n",
    "            \"rerank_k\",\"batch_size\",\"alpha\"\n",
    "        ]}\n",
    "\n",
    "\n",
    "print(\"✓ MultiBranchRetriever ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfde70e",
   "metadata": {},
   "source": [
    "### 6.2. 3-Phase Fine-Tuning Strategy\n",
    "\n",
    "**Phase 1 - Passage Configuration**: Optimize passage size, stride, and reranking depth\n",
    "\n",
    "**Phase 2 - Neural PRF**: Optimize feedback docs, top passages, and expansion terms  \n",
    "\n",
    "**Phase 3 - Score Blending**: Optimize RRF constant and final blend weight (alpha)\n",
    "\n",
    "Each phase uses best configurations from the previous phase, reducing search space progressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9fb6c3",
   "metadata": {},
   "source": [
    "#### 6.2.1. Phase Running Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4e28361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase runner ready\n"
     ]
    }
   ],
   "source": [
    "def _generate_configs(param_grid: Dict, base_config: Dict, prev_best: Optional[List[Dict]]) -> List[Dict]:\n",
    "    \"\"\"Generate all configurations for this phase.\"\"\"\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    \n",
    "    if prev_best is None:\n",
    "        # Phase 1: expand param_grid directly on top of base_config\n",
    "        return [{**base_config, **dict(zip(param_names, combo))}\n",
    "                for combo in product(*param_values)]\n",
    "    else:\n",
    "        # Phase 2/3: expand from previous best configs while keeping base_config defaults\n",
    "        return [{**base_config,\n",
    "                 **{k: v for k, v in prev_config.items() if k != \"map\"},\n",
    "                 **dict(zip(param_names, combo))}\n",
    "                for prev_config in prev_best\n",
    "                for combo in product(*param_values)]\n",
    "\n",
    "\n",
    "def _reconstruct_config(row: pd.Series, base_config: Dict) -> Dict:\n",
    "    \"\"\"Reconstruct config dict from DataFrame row with proper types.\"\"\"\n",
    "    # Start with base_config so fixed values persist across phases\n",
    "    config = {**base_config, **{k: row[k] for k in row.index\n",
    "                                if k not in [\"config_key\", \"map\", \"method\", \"num_queries\"]\n",
    "                                and pd.notna(row[k])}}\n",
    "    \n",
    "    # Cast to correct types using base_config as reference\n",
    "    for k, v in list(config.items()):\n",
    "        if k in base_config:\n",
    "            config[k] = type(base_config[k])(v)\n",
    "    \n",
    "    # Store map separately for sorting, not in config\n",
    "    config[\"map\"] = row[\"map\"]\n",
    "    return config\n",
    "\n",
    "\n",
    "def run_phase(\n",
    "    phase_name: str,\n",
    "    phase_num: int,\n",
    "    param_grid: Dict[str, list],\n",
    "    base_config: Dict,\n",
    "    prev_best_configs: Optional[List[Dict]] = None,\n",
    "    model_class: type = None,\n",
    "    queries: Dict[str, str] = None,\n",
    "    qrels: Dict[str, Dict[str, int]] = None,\n",
    "    top_k: int = 3\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run a Model 3 fine-tuning phase with grid search and caching.\n",
    "    \n",
    "    Returns: List of top-k configurations with their MAP scores\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\\n{phase_name}\\n{'=' * 80}\")\n",
    "    \n",
    "    # Single results file across phases\n",
    "    results_csv_path = get_results_csv(model_num=3)\n",
    "\n",
    "    # Generate all configs\n",
    "    configs = _generate_configs(param_grid, base_config, prev_best_configs)\n",
    "    \n",
    "    print(f\"Total configurations: {len(configs)}\")\n",
    "    if prev_best_configs:\n",
    "        print(f\"Expanding from {len(prev_best_configs)} previous configs\")\n",
    "    print(f\"Tuning parameters: {list(param_grid.keys())}\\n{'-' * 80}\")\n",
    "    \n",
    "    # Check cache across all phases; skip any config already computed\n",
    "    completed = load_completed_configs(model_num=3)\n",
    "    pending = [c for c in configs if generate_config_key(\"MultiBranch\", c) not in completed]\n",
    "    \n",
    "    print(f\"Completed: {len(configs) - len(pending)} | Pending: {len(pending)}\\n{'-' * 80}\")\n",
    "    \n",
    "    # Run pending experiments\n",
    "    for i, config in enumerate(pending, 1):\n",
    "        config_key = generate_config_key(\"MultiBranch\", config)\n",
    "        print(f\"[{i}/{len(pending)}] {config_key[:60]}...\", end=\" \")\n",
    "        \n",
    "        model = model_class(**config)\n",
    "        run = model.search(queries, k=1000)\n",
    "        metrics = evaluate_run(run, qrels, config_key)\n",
    "        \n",
    "        # Save tuned params + base params (no phase column)\n",
    "        tuned_keys = set(base_config.keys()) | set(param_grid.keys())\n",
    "        if prev_best_configs:\n",
    "            tuned_keys |= {k for k in prev_best_configs[0].keys() if k != \"map\"}\n",
    "        \n",
    "        result = {\n",
    "            \"config_key\": config_key,\n",
    "            **{k: v for k, v in config.items() if k in tuned_keys},\n",
    "            \"map\": metrics[\"map\"]\n",
    "        }\n",
    "        \n",
    "        save_experiment_result(result, model_num=3)\n",
    "        print(f\"MAP={metrics['map']:.4f}\")\n",
    "    \n",
    "    # Load and display results for this phase (filtered by generated configs)\n",
    "    if not os.path.exists(results_csv_path):\n",
    "        return []\n",
    "    \n",
    "    df = pd.read_csv(results_csv_path)\n",
    "    config_keys = [generate_config_key(\"MultiBranch\", c) for c in configs]\n",
    "    df = df[df[\"config_key\"].isin(config_keys)].sort_values(\"map\", ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\\n{phase_name} - Top {min(5, len(df))} Results:\\n{'=' * 80}\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Return top-k with proper types\n",
    "    top_configs = [_reconstruct_config(row, base_config) \n",
    "                   for _, row in df.head(top_k).iterrows()]\n",
    "    \n",
    "    print(f\"\\n✓ Selected top {len(top_configs)} configs for next phase\\n\")\n",
    "    return top_configs\n",
    "\n",
    "\n",
    "print(\"✓ Phase runner ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399b32b",
   "metadata": {},
   "source": [
    "#### 6.2.2. PHASE 1: Passage Configuration & Reranking Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9740f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 3 - PHASE 1: Passage Configuration & Reranking Depth\n",
      "================================================================================\n",
      "Total configurations: 24\n",
      "Tuning parameters: ['passage_size', 'passage_stride', 'rerank_k']\n",
      "--------------------------------------------------------------------------------\n",
      "Completed: 24 | Pending: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "MODEL 3 - PHASE 1: Passage Configuration & Reranking Depth - Top 5 Results:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>k1</th>\n",
       "      <th>b</th>\n",
       "      <th>fb_terms</th>\n",
       "      <th>fb_docs</th>\n",
       "      <th>original_weight</th>\n",
       "      <th>rrf_k</th>\n",
       "      <th>prf_fb_docs</th>\n",
       "      <th>prf_top_passages</th>\n",
       "      <th>expansion_terms</th>\n",
       "      <th>expansion_token_budget</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>alpha</th>\n",
       "      <th>passage_size</th>\n",
       "      <th>passage_stride</th>\n",
       "      <th>rerank_k</th>\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.327163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.325184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>200</td>\n",
       "      <td>0.322310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>192</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.320701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>200</td>\n",
       "      <td>0.320562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           config_key   k1    b  fb_terms  \\\n",
       "8   MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "19  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "2   MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "11  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "18  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "\n",
       "    fb_docs  original_weight  rrf_k  prf_fb_docs  prf_top_passages  \\\n",
       "8         5              0.4     20            5                10   \n",
       "19        5              0.4     20            5                10   \n",
       "2         5              0.4     20            5                10   \n",
       "11        5              0.4     20            5                10   \n",
       "18        5              0.4     20            5                10   \n",
       "\n",
       "    expansion_terms  expansion_token_budget  batch_size  alpha  passage_size  \\\n",
       "8                12                      40           8   0.35           128   \n",
       "19               12                      40           8   0.35           256   \n",
       "2                12                      40           8   0.35           128   \n",
       "11               12                      40           8   0.35           192   \n",
       "18               12                      40           8   0.35           256   \n",
       "\n",
       "    passage_stride  rerank_k       map  \n",
       "8               64       500  0.327163  \n",
       "19              64       500  0.325184  \n",
       "2               64       200  0.322310  \n",
       "11              64       500  0.320701  \n",
       "18              64       200  0.320562  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Selected top 2 configs for next phase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PHASE_1_GRID = {\n",
    "    \"passage_size\": [128, 192, 256],\n",
    "    \"passage_stride\": [64, 128],\n",
    "    \"rerank_k\": [50, 100, 200, 500],\n",
    "}\n",
    "\n",
    "PHASE_1_FIXED = {\n",
    "    \"k1\": best_k1,\n",
    "    \"b\": best_b,\n",
    "    \"fb_terms\": best_fb_terms,\n",
    "    \"fb_docs\": best_fb_docs,\n",
    "    \"original_weight\": best_orig_w,\n",
    "    \"rrf_k\": 20,\n",
    "    \"prf_fb_docs\": 5,\n",
    "    \"prf_top_passages\": 10,\n",
    "    \"expansion_terms\": 12,\n",
    "    \"expansion_token_budget\": 40,\n",
    "    \"batch_size\": 8,\n",
    "    \"alpha\": 0.35,\n",
    "}\n",
    "\n",
    "phase_1_best = run_phase(\n",
    "    phase_name=\"MODEL 3 - PHASE 1: Passage Configuration & Reranking Depth\",\n",
    "    phase_num=1,\n",
    "    param_grid=PHASE_1_GRID,\n",
    "    base_config=PHASE_1_FIXED,\n",
    "    prev_best_configs=None,\n",
    "    model_class=MultiBranchRetriever,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65617d2f",
   "metadata": {},
   "source": [
    "#### 6.2.3. PHASE 2: Neural PRF Capacity & Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab9c8f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 3 - PHASE 2: Neural PRF Capacity & Query Expansion\n",
      "================================================================================\n",
      "Total configurations: 36\n",
      "Expanding from 2 previous configs\n",
      "Tuning parameters: ['prf_fb_docs', 'prf_top_passages', 'expansion_terms']\n",
      "--------------------------------------------------------------------------------\n",
      "Completed: 36 | Pending: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "MODEL 3 - PHASE 2: Neural PRF Capacity & Query Expansion - Top 5 Results:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>k1</th>\n",
       "      <th>b</th>\n",
       "      <th>fb_terms</th>\n",
       "      <th>fb_docs</th>\n",
       "      <th>original_weight</th>\n",
       "      <th>rrf_k</th>\n",
       "      <th>prf_fb_docs</th>\n",
       "      <th>prf_top_passages</th>\n",
       "      <th>expansion_terms</th>\n",
       "      <th>expansion_token_budget</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>alpha</th>\n",
       "      <th>passage_size</th>\n",
       "      <th>passage_stride</th>\n",
       "      <th>rerank_k</th>\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.331545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.329912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.328240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.327612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.327227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           config_key   k1    b  fb_terms  \\\n",
       "54  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "52  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "47  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "63  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "35  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "\n",
       "    fb_docs  original_weight  rrf_k  prf_fb_docs  prf_top_passages  \\\n",
       "54        5              0.4     20           10                 5   \n",
       "52        5              0.4     20           10                 5   \n",
       "47        5              0.4     20            5                 5   \n",
       "63        5              0.4     20           10                10   \n",
       "35        5              0.4     20           10                 5   \n",
       "\n",
       "    expansion_terms  expansion_token_budget  batch_size  alpha  passage_size  \\\n",
       "54               12                      40           8   0.35           256   \n",
       "52               24                      40           8   0.35           256   \n",
       "47               12                      40           8   0.35           256   \n",
       "63               24                      40           8   0.35           256   \n",
       "35               12                      40           8   0.35           128   \n",
       "\n",
       "    passage_stride  rerank_k       map  \n",
       "54              64       500  0.331545  \n",
       "52              64       500  0.329912  \n",
       "47              64       500  0.328240  \n",
       "63              64       500  0.327612  \n",
       "35              64       500  0.327227  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Selected top 1 configs for next phase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PHASE_2_GRID = {\n",
    "    \"prf_fb_docs\": [3, 5, 10],\n",
    "    \"prf_top_passages\": [5, 10, 20],\n",
    "    \"expansion_terms\": [12, 24],\n",
    "}\n",
    "\n",
    "phase_2_best = run_phase(\n",
    "    phase_name=\"MODEL 3 - PHASE 2: Neural PRF Capacity & Query Expansion\",\n",
    "    phase_num=2,\n",
    "    param_grid=PHASE_2_GRID,\n",
    "    base_config=PHASE_1_FIXED,\n",
    "    prev_best_configs=phase_1_best,\n",
    "    model_class=MultiBranchRetriever,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    top_k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc064f26",
   "metadata": {},
   "source": [
    "#### 6.2.4. PHASE 3: Score Blending & RRF Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "864e964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 3 - PHASE 3: Score Blending & RRF Fusion\n",
      "================================================================================\n",
      "Total configurations: 21\n",
      "Expanding from 1 previous configs\n",
      "Tuning parameters: ['rrf_k', 'alpha']\n",
      "--------------------------------------------------------------------------------\n",
      "Completed: 21 | Pending: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "MODEL 3 - PHASE 3: Score Blending & RRF Fusion - Top 5 Results:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>k1</th>\n",
       "      <th>b</th>\n",
       "      <th>fb_terms</th>\n",
       "      <th>fb_docs</th>\n",
       "      <th>original_weight</th>\n",
       "      <th>rrf_k</th>\n",
       "      <th>prf_fb_docs</th>\n",
       "      <th>prf_top_passages</th>\n",
       "      <th>expansion_terms</th>\n",
       "      <th>expansion_token_budget</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>alpha</th>\n",
       "      <th>passage_size</th>\n",
       "      <th>passage_stride</th>\n",
       "      <th>rerank_k</th>\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>MultiBranch__alpha=0.6__b=0.4__batch_size=8__e...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.342872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>MultiBranch__alpha=0.7__b=0.4__batch_size=8__e...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.342085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>MultiBranch__alpha=0.8__b=0.4__batch_size=8__e...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.341984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>MultiBranch__alpha=0.7__b=0.4__batch_size=8__e...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.339893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>MultiBranch__alpha=0.6__b=0.4__batch_size=8__e...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.339811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           config_key   k1    b  fb_terms  \\\n",
       "60  MultiBranch__alpha=0.6__b=0.4__batch_size=8__e...  0.6  0.4       100   \n",
       "69  MultiBranch__alpha=0.7__b=0.4__batch_size=8__e...  0.6  0.4       100   \n",
       "70  MultiBranch__alpha=0.8__b=0.4__batch_size=8__e...  0.6  0.4       100   \n",
       "74  MultiBranch__alpha=0.7__b=0.4__batch_size=8__e...  0.6  0.4       100   \n",
       "73  MultiBranch__alpha=0.6__b=0.4__batch_size=8__e...  0.6  0.4       100   \n",
       "\n",
       "    fb_docs  original_weight  rrf_k  prf_fb_docs  prf_top_passages  \\\n",
       "60        5              0.4     20           10                 5   \n",
       "69        5              0.4     20           10                 5   \n",
       "70        5              0.4     20           10                 5   \n",
       "74        5              0.4     40           10                 5   \n",
       "73        5              0.4     40           10                 5   \n",
       "\n",
       "    expansion_terms  expansion_token_budget  batch_size  alpha  passage_size  \\\n",
       "60               12                      40           8    0.6           256   \n",
       "69               12                      40           8    0.7           256   \n",
       "70               12                      40           8    0.8           256   \n",
       "74               12                      40           8    0.7           256   \n",
       "73               12                      40           8    0.6           256   \n",
       "\n",
       "    passage_stride  rerank_k       map  \n",
       "60              64       500  0.342872  \n",
       "69              64       500  0.342085  \n",
       "70              64       500  0.341984  \n",
       "74              64       500  0.339893  \n",
       "73              64       500  0.339811  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Selected top 1 configs for next phase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PHASE_3_GRID = {\n",
    "    \"rrf_k\": [20, 40, 60],\n",
    "    \"alpha\": [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "}\n",
    "\n",
    "phase_3_best = run_phase(\n",
    "    phase_name=\"MODEL 3 - PHASE 3: Score Blending & RRF Fusion\",\n",
    "    phase_num=3,\n",
    "    param_grid=PHASE_3_GRID,\n",
    "    base_config=PHASE_1_FIXED,\n",
    "    prev_best_configs=phase_2_best,\n",
    "    model_class=MultiBranchRetriever,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    top_k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912eecdb",
   "metadata": {},
   "source": [
    "#### 6.2.5. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55fe1f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 3 COMPLETE: Best Configuration (After 3-Phase Fine-Tuning)\n",
      "================================================================================\n",
      "  passage_size=256\n",
      "  passage_stride=64\n",
      "  rerank_k=500\n",
      "  prf_fb_docs=10\n",
      "  prf_top_passages=5\n",
      "  expansion_terms=12\n",
      "  rrf_k=20\n",
      "  alpha=0.6\n",
      "  MAP=0.3429\n",
      "================================================================================\n",
      "\n",
      "✓ Final model ready for test inference\n"
     ]
    }
   ],
   "source": [
    "# Best overall config from Phase 3\n",
    "best_phase_3 = phase_3_best[0]\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 3 COMPLETE: Best Configuration (After 3-Phase Fine-Tuning)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  passage_size={best_phase_3['passage_size']}\")\n",
    "print(f\"  passage_stride={best_phase_3['passage_stride']}\")\n",
    "print(f\"  rerank_k={best_phase_3['rerank_k']}\")\n",
    "print(f\"  prf_fb_docs={best_phase_3['prf_fb_docs']}\")\n",
    "print(f\"  prf_top_passages={best_phase_3['prf_top_passages']}\")\n",
    "print(f\"  expansion_terms={best_phase_3['expansion_terms']}\")\n",
    "print(f\"  rrf_k={best_phase_3['rrf_k']}\")\n",
    "print(f\"  alpha={best_phase_3['alpha']}\")\n",
    "print(f\"  MAP={best_phase_3['map']:.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create final best model\n",
    "multi_branch_model = MultiBranchRetriever(\n",
    "    passage_size=int(best_phase_3['passage_size']),\n",
    "    passage_stride=int(best_phase_3['passage_stride']),\n",
    "    rerank_k=int(best_phase_3['rerank_k']),\n",
    "    prf_fb_docs=int(best_phase_3['prf_fb_docs']),\n",
    "    prf_top_passages=int(best_phase_3['prf_top_passages']),\n",
    "    expansion_terms=int(best_phase_3['expansion_terms']),\n",
    "    rrf_k=int(best_phase_3['rrf_k']),\n",
    "    alpha=float(best_phase_3['alpha']),\n",
    "    k1=best_k1,\n",
    "    b=best_b,\n",
    "    fb_terms=best_fb_terms,\n",
    "    fb_docs=best_fb_docs,\n",
    "    original_weight=best_orig_w,\n",
    "    expansion_token_budget=40,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Final model ready for test inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58802f78",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527a6da",
   "metadata": {},
   "source": [
    "### 7.1. Training Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1a86717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "3-MODEL PIPELINE TRAINING PERFORMANCE (50 queries with qrels)\n",
      "====================================================================================================\n",
      "    Method      MAP Improvement\n",
      "      BM25 0.247464    baseline\n",
      "BM25 + RM3 0.276720     +11.82%\n",
      "    Neural 0.342872     +38.55%\n",
      "====================================================================================================\n",
      "\n",
      "Best configurations:\n",
      "  Model 1 (BM25): k1=0.6, b=0.4\n",
      "  Model 2 (RM3): fb_terms=100, fb_docs=5, original_weight=0.4\n",
      "\n",
      "  Model 3 (Neural - 3-Phase Fine-tuned):\n",
      "    Passage: size=256, stride=64\n",
      "    PRF: fb_docs=10, top_passages=5\n",
      "    Expansion: terms=12\n",
      "    Reranking: rerank_k=500\n",
      "    Fusion: rrf_k=20, alpha=0.60\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        \"Method\": \"BM25\",\n",
    "        \"MAP\": best_bm25[\"map\"],\n",
    "        \"Improvement\": \"baseline\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"BM25 + RM3\",\n",
    "        \"MAP\": best_rm3[\"map\"],\n",
    "        \"Improvement\": f\"+{(best_rm3['map'] - best_bm25['map']) / best_bm25['map'] * 100:.2f}%\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"Neural\",\n",
    "        \"MAP\": best_phase_3['map'],\n",
    "        \"Improvement\": f\"+{(best_phase_3['map'] - best_bm25['map']) / best_bm25['map'] * 100:.2f}%\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"3-MODEL PIPELINE TRAINING PERFORMANCE (50 queries with qrels)\")\n",
    "print(\"=\" * 100)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\nBest configurations:\")\n",
    "print(f\"  Model 1 (BM25): k1={best_k1}, b={best_b}\")\n",
    "print(f\"  Model 2 (RM3): fb_terms={best_fb_terms}, fb_docs={best_fb_docs}, original_weight={best_orig_w}\")\n",
    "print(f\"\\n  Model 3 (Neural - 3-Phase Fine-tuned):\")\n",
    "print(f\"    Passage: size={int(best_phase_3['passage_size'])}, stride={int(best_phase_3['passage_stride'])}\")\n",
    "print(f\"    PRF: fb_docs={int(best_phase_3['prf_fb_docs'])}, top_passages={int(best_phase_3['prf_top_passages'])}\")\n",
    "print(f\"    Expansion: terms={int(best_phase_3['expansion_terms'])}\")\n",
    "print(f\"    Reranking: rerank_k={int(best_phase_3['rerank_k'])}\")\n",
    "print(f\"    Fusion: rrf_k={int(best_phase_3['rrf_k'])}, alpha={best_phase_3['alpha']:.2f}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47c634",
   "metadata": {},
   "source": [
    "### 7.2. Visualization & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f817abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ All models tuned and evaluated on training queries\n",
      "\n",
      "→ Ready to generate submission files for test queries\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "methods = summary[\"Method\"].tolist()\n",
    "maps = summary[\"MAP\"].tolist()\n",
    "colors = [\"#3498db\", \"#2ecc71\", \"#e74c3c\"]\n",
    "\n",
    "bars = ax.bar(methods, maps, color=colors, edgecolor=\"black\")\n",
    "ax.set_ylabel(\"MAP\")\n",
    "ax.set_title(\"Training Performance Comparison (50 queries)\")\n",
    "ax.set_ylim(0, max(maps) * 1.15)\n",
    "\n",
    "for bar, m in zip(bars, maps):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f\"{m:.4f}\", ha=\"center\", va=\"bottom\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ All models tuned and evaluated on training queries\")\n",
    "print(\"\\n→ Ready to generate submission files for test queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493be89",
   "metadata": {},
   "source": [
    "## 8. Generate Submission Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc2b33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Final Step**: Using the best configurations from each model, we generate predictions for the 199 test queries (without relevance judgments)\n",
    "\n",
    "Each model produces a TREC-formatted run file with 1000 ranked documents per query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7959369",
   "metadata": {},
   "source": [
    "### 8.1. Run Inference on Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b8eee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING SUBMISSION FILES: 199 test queries (no qrels)\n",
      "================================================================================\n",
      "\n",
      "Run 1: Model 1 - BM25 Baseline\n",
      "  k1=0.6, b=0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25 Search: 100%|██████████| 199/199 [00:17<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 199 queries processed\n",
      "\n",
      "Run 2: Model 2 - BM25 + RM3 Query Expansion\n",
      "  k1=0.6, b=0.4, fb_terms=100, fb_docs=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RM3 Search: 100%|██████████| 199/199 [00:21<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 199 queries processed\n",
      "\n",
      "Run 3: Model 3 - Neural Reranking\n",
      "  Passage: size=256, stride=64\n",
      "  PRF: fb_docs=10, top_passages=5\n",
      "  Expansion: terms=12\n",
      "  Reranking: rerank_k=500\n",
      "  Fusion: rrf_k=20, alpha=0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RM3: 100%|██████████| 199/199 [00:17<00:00, 11.29it/s]\n",
      "SPLADE:   0%|          | 0/199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize pre-built index beir-v1.0.0-robust04.splade-pp-ed.\n",
      "/home/galnoy/.cache/pyserini/indexes/lucene-inverted.beir-v1.0.0-robust04.splade-pp-ed.20231124.a66f86f.c1a6fd094bb9e34e69e10040d9b0ad2a already exists, skipping download.\n",
      "Initializing beir-v1.0.0-robust04.splade-pp-ed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPLADE: 100%|██████████| 199/199 [00:18<00:00, 11.02it/s]\n",
      "PRF + MiniLM: 100%|██████████| 199/199 [2:03:14<00:00, 37.16s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 199 queries processed\n",
      "\n",
      "✓ All test queries processed\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"GENERATING SUBMISSION FILES: {len(test_queries)} test queries (no qrels)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Run 1: Model 1 - BM25 Baseline\n",
    "print(\"Run 1: Model 1 - BM25 Baseline\")\n",
    "print(f\"  k1={best_k1}, b={best_b}\")\n",
    "run_1 = bm25_model.search(test_queries, k=1000)\n",
    "print(f\"  ✓ {len(run_1)} queries processed\")\n",
    "\n",
    "# Run 2: Model 2 - BM25 + RM3\n",
    "print(\"\\nRun 2: Model 2 - BM25 + RM3 Query Expansion\")\n",
    "print(f\"  k1={best_k1}, b={best_b}, fb_terms={best_fb_terms}, fb_docs={best_fb_docs}\")\n",
    "run_2 = rm3_model.search(test_queries, k=1000)\n",
    "print(f\"  ✓ {len(run_2)} queries processed\")\n",
    "\n",
    "# Run 3: Model 3 - Neural\n",
    "print(\"\\nRun 3: Model 3 - Neural Reranking\")\n",
    "print(f\"  Passage: size={int(best_phase_3['passage_size'])}, stride={int(best_phase_3['passage_stride'])}\")\n",
    "print(f\"  PRF: fb_docs={int(best_phase_3['prf_fb_docs'])}, top_passages={int(best_phase_3['prf_top_passages'])}\")\n",
    "print(f\"  Expansion: terms={int(best_phase_3['expansion_terms'])}\")\n",
    "print(f\"  Reranking: rerank_k={int(best_phase_3['rerank_k'])}\")\n",
    "print(f\"  Fusion: rrf_k={int(best_phase_3['rrf_k'])}, alpha={best_phase_3['alpha']:.2f}\")\n",
    "run_3 = multi_branch_model.search(test_queries, k=1000)\n",
    "print(f\"  ✓ {len(run_3)} queries processed\")\n",
    "\n",
    "print(\"\\n✓ All test queries processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f94a8",
   "metadata": {},
   "source": [
    "### 8.2. Export TREC Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2789317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission files...\n",
      "\n",
      "✓ Written: run_1.res\n",
      "✓ Written: run_2.res\n",
      "✓ Written: run_3.res\n",
      "\n",
      "✓ All submission files written\n"
     ]
    }
   ],
   "source": [
    "def write_trec_run(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    filepath: str,\n",
    "    run_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Write run to TREC format.\n",
    "    Format: topic_id Q0 doc_id rank score run_name\n",
    "    \"\"\"\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for qid in sorted(run.keys(), key=lambda x: int(x)):\n",
    "            results = run[qid]\n",
    "            # Sort by score descending\n",
    "            sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "            for rank, (docid, score) in enumerate(sorted_results[:1000], start=1):\n",
    "                f.write(f\"{qid} Q0 {docid} {rank} {score:.6f} {run_name}\\n\")\n",
    "    \n",
    "    print(f\"✓ Written: {filepath}\")\n",
    "\n",
    "\n",
    "print(\"Writing submission files...\\n\")\n",
    "\n",
    "write_trec_run(run_1, \"run_1.res\", \"run_1\")\n",
    "write_trec_run(run_2, \"run_2.res\", \"run_2\")\n",
    "write_trec_run(run_3, \"run_3.res\", \"run_3\")\n",
    "\n",
    "print(\"\\n✓ All submission files written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38715c",
   "metadata": {},
   "source": [
    "### 8.3. Validate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da5991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating submission files...\n",
      "\n",
      "✓ run_1.res: 199 queries validated (≤ 1000 docs per query)\n",
      "✓ run_2.res: 199 queries validated (≤ 1000 docs per query)\n",
      "✓ run_3.res: 199 queries validated (≤ 1000 docs per query)\n",
      "\n",
      "============================================================\n",
      "SUBMISSION FILES READY\n",
      "============================================================\n",
      "Files: run_1.res, run_2.res, run_3.res\n",
      "Format: TREC 6-column\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "def validate_run_file(\n",
    "    filepath: str,\n",
    "    expected_queries: int = 199,\n",
    "    max_docs_per_query: int = 1000,\n",
    "    expected_tag: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Validate a TREC run file in a MAP-safe way.\n",
    "\n",
    "    Enforces:\n",
    "      - Proper 6-column TREC format\n",
    "      - All expected queries appear exactly once\n",
    "      - 1 ≤ docs per query ≤ max_docs_per_query\n",
    "      - Ranks are contiguous starting at 1\n",
    "      - Scores are non-increasing by rank\n",
    "      - No duplicate docids per query\n",
    "      - Optional: consistent run tag\n",
    "    \"\"\"\n",
    "    query_rows: Dict[str, List[Tuple[int, str, float, str]]] = defaultdict(list)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Parse & basic format checks\n",
    "    # ----------------------------\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) != 6:\n",
    "                raise AssertionError(\n",
    "                    f\"{filepath}: line {ln} expected 6 columns, got {len(parts)}\"\n",
    "                )\n",
    "\n",
    "            qid, q0, docid, rank_s, score_s, tag = parts\n",
    "\n",
    "            if q0 != \"Q0\":\n",
    "                raise AssertionError(\n",
    "                    f\"{filepath}: line {ln} column 2 expected 'Q0', got '{q0}'\"\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                rank = int(rank_s)\n",
    "            except Exception:\n",
    "                raise AssertionError(\n",
    "                    f\"{filepath}: line {ln} rank not int: '{rank_s}'\"\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                score = float(score_s)\n",
    "            except Exception:\n",
    "                raise AssertionError(\n",
    "                    f\"{filepath}: line {ln} score not float: '{score_s}'\"\n",
    "                )\n",
    "\n",
    "            query_rows[qid].append((rank, docid, score, tag))\n",
    "\n",
    "    # ----------------------------\n",
    "    # Query count check\n",
    "    # ----------------------------\n",
    "    if len(query_rows) != expected_queries:\n",
    "        raise AssertionError(\n",
    "            f\"{filepath}: expected {expected_queries} queries, got {len(query_rows)}\"\n",
    "        )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Per-query validation\n",
    "    # ----------------------------\n",
    "    for qid, rows in query_rows.items():\n",
    "        n = len(rows)\n",
    "\n",
    "        # Allow fewer than max_docs_per_query (TREC-correct)\n",
    "        if not (1 <= n <= max_docs_per_query):\n",
    "            raise AssertionError(\n",
    "                f\"{filepath}: qid {qid} has {n} docs, expected ≤ {max_docs_per_query}\"\n",
    "            )\n",
    "\n",
    "        # Ranks: contiguous starting at 1\n",
    "        ranks = [r for r, _, _, _ in rows]\n",
    "        if len(set(ranks)) != len(ranks):\n",
    "            raise AssertionError(f\"{filepath}: qid {qid} has duplicate ranks\")\n",
    "\n",
    "        if set(ranks) != set(range(1, n + 1)):\n",
    "            raise AssertionError(f\"{filepath}: qid {qid} ranks not contiguous\")\n",
    "\n",
    "        # Docids: no duplicates\n",
    "        docids = [d for _, d, _, _ in rows]\n",
    "        if len(set(docids)) != len(docids):\n",
    "            raise AssertionError(f\"{filepath}: qid {qid} has duplicate docids\")\n",
    "\n",
    "        # Scores: non-increasing by rank\n",
    "        scores = [s for _, _, s, _ in sorted(rows, key=lambda x: x[0])]\n",
    "        if scores != sorted(scores, reverse=True):\n",
    "            raise AssertionError(\n",
    "                f\"{filepath}: qid {qid} scores not non-increasing\"\n",
    "            )\n",
    "\n",
    "        # Optional: run tag consistency\n",
    "        if expected_tag is not None:\n",
    "            tags = {t for _, _, _, t in rows}\n",
    "            if tags != {expected_tag}:\n",
    "                raise AssertionError(\n",
    "                    f\"{filepath}: qid {qid} unexpected tags {sorted(tags)}\"\n",
    "                )\n",
    "\n",
    "    print(\n",
    "        f\"✓ {filepath}: {len(query_rows)} queries validated \"\n",
    "        f\"(≤ {max_docs_per_query} docs per query)\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Validating submission files...\\n\")\n",
    "\n",
    "validate_run_file(\"run_1.res\", expected_tag=\"run_1\")\n",
    "validate_run_file(\"run_2.res\", expected_tag=\"run_2\")\n",
    "validate_run_file(\"run_3.res\", expected_tag=\"run_3\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUBMISSION FILES READY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Files: run_1.res, run_2.res, run_3.res\")\n",
    "print(\"Format: TREC 6-column\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915397d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- **ROBUST04 / TREC Robust Track**\n",
    "  - Voorhees, E. M. (2004). *Overview of the TREC 2004 Robust Retrieval Track.*  \n",
    "    Proceedings of the Thirteenth Text REtrieval Conference (TREC 2004).\n",
    "\n",
    "- **BM25 Ranking Function**\n",
    "  - Robertson, S., Walker, S., Jones, S., Hancock-Beaulieu, M., & Gatford, M. (1994).  \n",
    "    *Okapi at TREC-3.* Proceedings of the Third Text REtrieval Conference (TREC-3).\n",
    "\n",
    "- **Relevance Model 3 (RM3) Query Expansion**\n",
    "  - Lavrenko, V., & Croft, W. B. (2001).  \n",
    "    *Relevance-Based Language Models.*  \n",
    "    Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.\n",
    "\n",
    "- **Reciprocal Rank Fusion (RRF)**\n",
    "  - Cormack, G. V., Clarke, C. L. A., & Buettcher, S. (2009).  \n",
    "    *Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods.*  \n",
    "    Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval.\n",
    "\n",
    "- **SPLADE (Sparse Learned Representations)**\n",
    "  - Formal, T., Lassance, C., Piwowarski, B., & Clinchant, S. (2021).  \n",
    "    *SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking.*  \n",
    "    Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval.\n",
    "\n",
    "- **Neural Cross-Encoder Reranking (MiniLM)**\n",
    "  - Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., & Zhou, M. (2020).  \n",
    "    *MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers.*  \n",
    "    Advances in Neural Information Processing Systems (NeurIPS).\n",
    "\n",
    "- **Evaluation Framework**\n",
    "  - Van Gysel, C., de Rijke, M., & Kanoulas, E. (2018).  \n",
    "    *pytrec_eval: An Extremely Fast Python Interface to trec_eval.*  \n",
    "    Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
