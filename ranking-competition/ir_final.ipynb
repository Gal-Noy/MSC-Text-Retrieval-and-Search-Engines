{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b40988",
   "metadata": {},
   "source": [
    "# Robust04: 3-Model Neural Retrieval Pipeline\n",
    "\n",
    "**Gal Noy** · 209346486\n",
    "\n",
    "**Dataset**: TREC Robust04 - 249 queries (50 train with qrels, 199 test), ~528K documents\n",
    "\n",
    "**Pipeline**:\n",
    "1. **Model 1 - BM25**: Baseline with k1, b tuning\n",
    "2. **Model 2 - RM3**: Query expansion (fb_terms, fb_docs, original_weight)\n",
    "3. **Model 3 - Neural**: Multi-branch (RM3+SPLADE) → RRF fusion → Neural PRF+MiniLM → Score blending\n",
    "\n",
    "**Method**: Grid search for optimal parameters at each model using MAP on training queries\n",
    "\n",
    "**Output**: Three TREC runs (run_1.res, run_2.res, run_3.res) for test queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ec658",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4888f",
   "metadata": {},
   "source": [
    "### 1.1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04b58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install -y openjdk-21-jdk\n",
    "# !update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-21-openjdk-amd64/bin/java 1\n",
    "# !update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac 1\n",
    "# !update-alternatives --set java /usr/lib/jvm/java-21-openjdk-amd64/bin/java\n",
    "# !update-alternatives --set javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6da3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install transformers\n",
    "# !pip install sentence-transformers\n",
    "# !pip install pytrec_eval\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install faiss-cpu --no-cache\n",
    "# !pip install pyserini==0.36.0\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cff9e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[0;93m2026-01-17 10:17:45.207227847 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✓ Dependencies imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Tuple, Optional, Iterable\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import product\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from pyserini.search.lucene import LuceneSearcher, LuceneImpactSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "from pyserini.encode import SpladeQueryEncoder\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(\"✓ Dependencies imported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec6c3c",
   "metadata": {},
   "source": [
    "### 1.2. Load Pyserini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d89b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: robust04\n",
      "Total documents: 528,030\n",
      "Total terms: 174,540,872\n",
      "✓ Pyserini index loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jan 17, 2026 10:17:46 AM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"robust04\"\n",
    "\n",
    "index_reader = IndexReader.from_prebuilt_index(INDEX_NAME)\n",
    "\n",
    "print(f\"Index: {INDEX_NAME}\")\n",
    "print(f\"Total documents: {index_reader.stats()['documents']:,}\")\n",
    "print(f\"Total terms: {index_reader.stats()['total_terms']:,}\")\n",
    "print(\"✓ Pyserini index loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b2621",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5a950",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note**: The 50 queries with qrels are used for training/tuning (parameter optimization via grid search). The remaining 199 queries without qrels are used for generating final submission files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5e6b1",
   "metadata": {},
   "source": [
    "### 2.1. Load Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac88eb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries loaded: 249\n",
      "\n",
      "Sample queries:\n",
      "  301: international organized crime\n",
      "  302: poliomyelitis post polio\n",
      "  303: hubble telescope achievements\n",
      "  304: endangered species mammals\n",
      "  305: dangerous vehicles\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "def load_queries(filepath: str) -> Dict[str, str]:\n",
    "    \"\"\"Load queries from file. Format: qid<tab>query_text\"\"\"\n",
    "    queries = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                qid, text = parts\n",
    "                queries[qid] = text\n",
    "    return queries\n",
    "\n",
    "all_queries = load_queries(os.path.join(DATA_DIR, \"queriesROBUST.txt\"))\n",
    "\n",
    "print(f\"Total queries loaded: {len(all_queries)}\")\n",
    "print(f\"\\nSample queries:\")\n",
    "for qid, text in list(all_queries.items())[:5]:\n",
    "    print(f\"  {qid}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16000c0a",
   "metadata": {},
   "source": [
    "### 2.2. Load Relevance Judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a1a9a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries with relevance judgments: 50\n",
      "Total judgments: 61,511\n",
      "\n",
      "Sample qrels for query 301:\n",
      "  FBIS3-10082: 1\n",
      "  FBIS3-10169: 0\n",
      "  FBIS3-10243: 1\n",
      "  FBIS3-10319: 0\n",
      "  FBIS3-10397: 1\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(filepath: str) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"Load qrels. Format: qid 0 docid relevance\"\"\"\n",
    "    qrels = defaultdict(dict)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 4:\n",
    "                qid, _, docid, rel = parts[:4]\n",
    "                qrels[qid][docid] = int(rel)\n",
    "    return dict(qrels)\n",
    "\n",
    "qrels = load_qrels(os.path.join(DATA_DIR, \"qrels_50_Queries\"))\n",
    "\n",
    "print(f\"Queries with relevance judgments: {len(qrels)}\")\n",
    "print(f\"Total judgments: {sum(len(v) for v in qrels.values()):,}\")\n",
    "print(f\"\\nSample qrels for query 301:\")\n",
    "sample_rels = list(qrels.get(\"301\", {}).items())[:5]\n",
    "for docid, rel in sample_rels:\n",
    "    print(f\"  {docid}: {rel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95332a",
   "metadata": {},
   "source": [
    "### 2.3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5779bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training queries: 50 (with qrels)\n",
      "Test queries: 199 (no qrels)\n",
      "\n",
      "Train QIDs: ['301', '302', '303', '304', '305', '306', '307', '308', '309', '310']...\n",
      "Test QIDs: ['351', '352', '353', '354', '355', '356', '357', '358', '359', '360']...\n"
     ]
    }
   ],
   "source": [
    "train_qids = sorted(qrels.keys())\n",
    "test_qids = [qid for qid in all_queries.keys() if qid not in train_qids]\n",
    "\n",
    "train_queries = {qid: all_queries[qid] for qid in train_qids}\n",
    "test_queries = {qid: all_queries[qid] for qid in test_qids}\n",
    "\n",
    "print(f\"Training queries: {len(train_queries)} (with qrels)\")\n",
    "print(f\"Test queries: {len(test_queries)} (no qrels)\")\n",
    "print(f\"\\nTrain QIDs: {train_qids[:10]}...\")\n",
    "print(f\"Test QIDs: {test_qids[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2aa8a3",
   "metadata": {},
   "source": [
    "## 3. Evaluation Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b429060",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Evaluation Strategy**: All experiments use MAP (Mean Average Precision) computed with pytrec_eval. Grid search explores parameter combinations, caching results to avoid redundant computation. Best parameters from each model are carried forward to the next model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c0234",
   "metadata": {},
   "source": [
    "### 3.1. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab176d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def compute_map(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    qrels: Dict[str, Dict[str, int]]\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute MAP using pytrec_eval (TREC standard).\n",
    "\n",
    "    Returns:\n",
    "        map_score: float\n",
    "        per_query_ap: Dict[qid, AP]\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert run to pytrec_eval format: {qid: {docid: score}}\n",
    "    run_dict = {\n",
    "        qid: {docid: score for docid, score in docs}\n",
    "        for qid, docs in run.items()\n",
    "    }\n",
    "\n",
    "    # Convert qrels to pytrec_eval format: {qid: {docid: relevance}}\n",
    "    qrels_dict = {\n",
    "        qid: dict(docs)\n",
    "        for qid, docs in qrels.items()\n",
    "    }\n",
    "\n",
    "    missing_qids = set(qrels_dict) - set(run_dict)\n",
    "    assert not missing_qids, (\n",
    "        f\"Missing queries in run: {sorted(list(missing_qids))[:10]}\"\n",
    "    )\n",
    "\n",
    "    for qid, docs in run.items():\n",
    "        docids = [d for d, _ in docs]\n",
    "        assert len(docids) == len(set(docids)), (\n",
    "            f\"Duplicate docIDs in run for query {qid}\"\n",
    "        )\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels_dict, {\"map\"})\n",
    "    results = evaluator.evaluate(run_dict)\n",
    "\n",
    "    per_query_ap = {qid: m[\"map\"] for qid, m in results.items()}\n",
    "\n",
    "    map_score = pytrec_eval.compute_aggregated_measure(\n",
    "        \"map\",\n",
    "        list(per_query_ap.values())\n",
    "    )\n",
    "\n",
    "    return map_score, per_query_ap\n",
    "\n",
    "\n",
    "def evaluate_run(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    qrels: Dict[str, Dict[str, int]],\n",
    "    run_name: str = \"run\"\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate a run using pytrec_eval and return metrics.\"\"\"\n",
    "    map_score, per_query_ap = compute_map(run, qrels)\n",
    "    \n",
    "    return {\n",
    "        \"run_name\": run_name,\n",
    "        \"map\": map_score,\n",
    "        \"num_queries\": len(per_query_ap),\n",
    "        \"per_query_ap\": per_query_ap\n",
    "    }\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1ce08",
   "metadata": {},
   "source": [
    "### 3.2. Caching Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357dddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Caching utilities defined\n"
     ]
    }
   ],
   "source": [
    "def get_results_csv(model_num: int = None) -> str:\n",
    "    \"\"\"Get CSV filename for a specific model.\"\"\"\n",
    "    if model_num is None:\n",
    "        return \"./experiments.csv\"\n",
    "    return f\"./experiments_model{model_num}.csv\"\n",
    "\n",
    "\n",
    "def generate_config_key(method: str, params: Dict) -> str:\n",
    "    \"\"\"Generate unique config key for caching.\"\"\"\n",
    "    parts = [method]\n",
    "    for k, v in sorted(params.items()):\n",
    "        parts.append(f\"{k}={v}\")\n",
    "    return \"__\".join(parts)\n",
    "\n",
    "\n",
    "def load_cached_result(config_key: str, model_num: int = None) -> Optional[Dict]:\n",
    "    \"\"\"Load cached result if exists.\"\"\"\n",
    "    results_csv = get_results_csv(model_num)\n",
    "    \n",
    "    if not os.path.exists(results_csv):\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(results_csv)\n",
    "    row = df[df[\"config_key\"] == config_key]\n",
    "    \n",
    "    if row.empty:\n",
    "        return None\n",
    "    \n",
    "    return row.iloc[0].to_dict()\n",
    "\n",
    "\n",
    "def save_experiment_result(result: Dict, model_num: int = None):\n",
    "    \"\"\"Save experiment result to model-specific cache.\"\"\"\n",
    "    results_csv = get_results_csv(model_num)\n",
    "    df_row = pd.DataFrame([result])\n",
    "    \n",
    "    if not os.path.exists(results_csv):\n",
    "        df_row.to_csv(results_csv, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(results_csv, mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "def load_completed_configs(model_num: int = None) -> set:\n",
    "    \"\"\"Load set of completed experiment config keys from CSV.\"\"\"\n",
    "    results_csv = get_results_csv(model_num)\n",
    "    if not os.path.exists(results_csv):\n",
    "        return set()\n",
    "    df = pd.read_csv(results_csv)\n",
    "    return set(df[\"config_key\"])\n",
    "\n",
    "\n",
    "print(\"✓ Caching utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634949d",
   "metadata": {},
   "source": [
    "### 3.3. Base Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea6a3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BaseRetriever class defined\n"
     ]
    }
   ],
   "source": [
    "class BaseRetriever(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all retrieval models.\n",
    "    All models must implement search() and get_params().\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, index_name: str = \"robust04\"):\n",
    "        self.index_name = index_name\n",
    "    \n",
    "    @abstractmethod\n",
    "    def search(\n",
    "        self,\n",
    "        queries: Dict[str, str],\n",
    "        k: int = 1000\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        \"\"\"Search for all queries and return ranked results.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_params(self) -> Dict:\n",
    "        \"\"\"Return model parameters for logging.\"\"\"\n",
    "        return {}\n",
    "\n",
    "\n",
    "print(\"✓ BaseRetriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023aa3c",
   "metadata": {},
   "source": [
    "### 3.4. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b40836a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Experiment framework defined\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    config_key: str,\n",
    "    model_name: str,\n",
    "    model_class: type,\n",
    "    model_params: Dict,\n",
    "    queries: Dict[str, str],\n",
    "    qrels: Dict[str, Dict[str, int]]\n",
    ") -> Dict:\n",
    "    \"\"\"Run a single experiment using a retriever class.\"\"\"\n",
    "    model = model_class(**model_params)\n",
    "    run = model.search(queries, k=1000)\n",
    "    metrics = evaluate_run(run, qrels, config_key)\n",
    "    \n",
    "    return {\n",
    "        \"config_key\": config_key,\n",
    "        \"method\": model_name,\n",
    "        **model.get_params(),\n",
    "        \"map\": metrics[\"map\"],\n",
    "        \"num_queries\": metrics[\"num_queries\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def run_grid_search(\n",
    "    method_name: str,\n",
    "    model_class: type,\n",
    "    param_grid: Dict[str, list],\n",
    "    queries: Dict[str, str],\n",
    "    qrels: Dict[str, Dict[str, int]],\n",
    "    model_num: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Grid search for retriever classes with model-specific caching.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Print header\n",
    "    model_str = f\"MODEL {model_num}: \" if model_num else \"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{model_str}Tuning {method_name} parameters\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    \n",
    "    for combo in product(*param_values):\n",
    "        params = dict(zip(param_names, combo))\n",
    "        config_key = generate_config_key(method_name, params)\n",
    "        \n",
    "        # Check model-specific cache\n",
    "        cached = load_cached_result(config_key, model_num)\n",
    "        if cached is not None:\n",
    "            param_str = \", \".join([f\"{k}={v}\" for k, v in params.items()])\n",
    "            print(f\"{param_str} -> MAP={cached['map']:.4f} [CACHED]\")\n",
    "            results.append(cached)\n",
    "            continue\n",
    "        \n",
    "        # Run experiment\n",
    "        param_str = \", \".join([f\"{k}={v}\" for k, v in params.items()])\n",
    "        print(f\"{param_str} -> Running...\", end=\" \")\n",
    "        \n",
    "        result = run_experiment(\n",
    "            config_key=config_key,\n",
    "            model_name=method_name,\n",
    "            model_class=model_class,\n",
    "            model_params=params,\n",
    "            queries=queries,\n",
    "            qrels=qrels\n",
    "        )\n",
    "        \n",
    "        # Save to model-specific cache\n",
    "        save_experiment_result(result, model_num)\n",
    "        print(f\"MAP={result['map']:.4f}\")\n",
    "        results.append(result)\n",
    "    \n",
    "    # Create and sort results dataframe\n",
    "    df = pd.DataFrame(results).sort_values(\"map\", ascending=False)\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{method_name} Tuning Results (Top 5):\")\n",
    "    print(\"=\"*60)\n",
    "    display(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Experiment framework defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e034cf",
   "metadata": {},
   "source": [
    "## 4. MODEL 1: BM25 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efee42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Model 1 Overview**: BM25 is a probabilistic ranking function based on term frequency and document length normalization. We tune k1 (term frequency saturation) and b (length normalization) to find optimal retrieval parameters for Robust04."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ad949",
   "metadata": {},
   "source": [
    "### 4.1. BM25 Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be810bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BM25Retriever class defined\n"
     ]
    }
   ],
   "source": [
    "class BM25Retriever(BaseRetriever):\n",
    "    \"\"\"BM25 retrieval model.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        k1: float = 0.9,\n",
    "        b: float = 0.4,\n",
    "        index_name: str = \"robust04\"\n",
    "    ):\n",
    "        super().__init__(index_name)\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self._searcher = None\n",
    "    \n",
    "    @property\n",
    "    def searcher(self) -> LuceneSearcher:\n",
    "        if self._searcher is None:\n",
    "            self._searcher = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "            self._searcher.set_bm25(k1=self.k1, b=self.b)\n",
    "        return self._searcher\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        queries: Dict[str, str],\n",
    "        k: int = 1000\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        results = {}\n",
    "        for qid, query_text in tqdm(queries.items(), desc=\"BM25 Search\"):\n",
    "            hits = self.searcher.search(query_text, k=k)\n",
    "            results[qid] = [(hit.docid, hit.score) for hit in hits]\n",
    "        return results\n",
    "    \n",
    "    def get_params(self) -> Dict:\n",
    "        return {\"k1\": self.k1, \"b\": self.b}\n",
    "\n",
    "\n",
    "print(\"✓ BM25Retriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882ee79",
   "metadata": {},
   "source": [
    "### 4.2. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df83f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL 1: Tuning BM25 parameters\n",
      "============================================================\n",
      "\n",
      "k1=0.6, b=0.3 -> MAP=0.2463 [CACHED]\n",
      "k1=0.6, b=0.4 -> MAP=0.2475 [CACHED]\n",
      "k1=0.6, b=0.5 -> MAP=0.2453 [CACHED]\n",
      "k1=0.6, b=0.6 -> MAP=0.2422 [CACHED]\n",
      "k1=0.6, b=0.75 -> MAP=0.2392 [CACHED]\n",
      "k1=0.9, b=0.3 -> MAP=0.2450 [CACHED]\n",
      "k1=0.9, b=0.4 -> MAP=0.2455 [CACHED]\n",
      "k1=0.9, b=0.5 -> MAP=0.2442 [CACHED]\n",
      "k1=0.9, b=0.6 -> MAP=0.2415 [CACHED]\n",
      "k1=0.9, b=0.75 -> MAP=0.2374 [CACHED]\n",
      "k1=1.2, b=0.3 -> MAP=0.2426 [CACHED]\n",
      "k1=1.2, b=0.4 -> MAP=0.2427 [CACHED]\n",
      "k1=1.2, b=0.5 -> MAP=0.2424 [CACHED]\n",
      "k1=1.2, b=0.6 -> MAP=0.2400 [CACHED]\n",
      "k1=1.2, b=0.75 -> MAP=0.2334 [CACHED]\n",
      "k1=1.5, b=0.3 -> MAP=0.2392 [CACHED]\n",
      "k1=1.5, b=0.4 -> MAP=0.2396 [CACHED]\n",
      "k1=1.5, b=0.5 -> MAP=0.2384 [CACHED]\n",
      "k1=1.5, b=0.6 -> MAP=0.2360 [CACHED]\n",
      "k1=1.5, b=0.75 -> MAP=0.2307 [CACHED]\n",
      "k1=2.0, b=0.3 -> MAP=0.2320 [CACHED]\n",
      "k1=2.0, b=0.4 -> MAP=0.2330 [CACHED]\n",
      "k1=2.0, b=0.5 -> MAP=0.2324 [CACHED]\n",
      "k1=2.0, b=0.6 -> MAP=0.2292 [CACHED]\n",
      "k1=2.0, b=0.75 -> MAP=0.2250 [CACHED]\n",
      "\n",
      "============================================================\n",
      "BM25 Tuning Results (Top 5):\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>method</th>\n",
       "      <th>k1</th>\n",
       "      <th>b</th>\n",
       "      <th>map</th>\n",
       "      <th>num_queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25__b=0.4__k1=0.6</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.247464</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25__b=0.3__k1=0.6</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.246328</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BM25__b=0.4__k1=0.9</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.245466</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BM25__b=0.5__k1=0.6</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.245285</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BM25__b=0.3__k1=0.9</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.244954</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            config_key method   k1    b       map  num_queries\n",
       "1  BM25__b=0.4__k1=0.6   BM25  0.6  0.4  0.247464           50\n",
       "0  BM25__b=0.3__k1=0.6   BM25  0.6  0.3  0.246328           50\n",
       "6  BM25__b=0.4__k1=0.9   BM25  0.9  0.4  0.245466           50\n",
       "2  BM25__b=0.5__k1=0.6   BM25  0.6  0.5  0.245285           50\n",
       "5  BM25__b=0.3__k1=0.9   BM25  0.9  0.3  0.244954           50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BM25_PARAM_GRID = {\n",
    "    \"k1\": [0.6, 0.9, 1.2, 1.5, 2.0],\n",
    "    \"b\": [0.3, 0.4, 0.5, 0.6, 0.75]\n",
    "}\n",
    "\n",
    "# Run grid search with BM25Retriever class\n",
    "bm25_df = run_grid_search(\n",
    "    method_name=\"BM25\",\n",
    "    model_class=BM25Retriever,\n",
    "    param_grid=BM25_PARAM_GRID,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    model_num=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522b7f3",
   "metadata": {},
   "source": [
    "### 4.3. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f82722cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL 1 COMPLETE: Best BM25 Parameters\n",
      "============================================================\n",
      "  k1 = 0.6\n",
      "  b = 0.4\n",
      "  MAP = 0.2475\n",
      "============================================================\n",
      "\n",
      "✓ Best BM25 model ready for Model 2\n"
     ]
    }
   ],
   "source": [
    "best_bm25 = bm25_df.iloc[0]\n",
    "best_k1, best_b = best_bm25[\"k1\"], best_bm25[\"b\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 1 COMPLETE: Best BM25 Parameters\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  k1 = {best_k1}\")\n",
    "print(f\"  b = {best_b}\")\n",
    "print(f\"  MAP = {best_bm25['map']:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create best BM25 model\n",
    "bm25_model = BM25Retriever(k1=best_k1, b=best_b)\n",
    "\n",
    "print(\"\\n✓ Best BM25 model ready for Model 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90683dc7",
   "metadata": {},
   "source": [
    "## 5. MODEL 2: BM25 + RM3 (Query Expansion)\n",
    "\n",
    "Using best BM25 parameters from Model 1 to tune RM3 hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9d2ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Model 2 Overview**: RM3 (Relevance Model 3) improves retrieval through pseudo-relevance feedback. It expands the original query with terms from top-ranked documents, then re-retrieves with the expanded query. Parameters: fb_terms (expansion terms), fb_docs (feedback documents), original_weight (balance between original and expanded query)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af424f9",
   "metadata": {},
   "source": [
    "### 5.1. RM3 Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "420d3e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RM3Retriever class defined\n"
     ]
    }
   ],
   "source": [
    "class RM3Retriever(BaseRetriever):\n",
    "    \"\"\"BM25 + RM3 pseudo-relevance feedback.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        k1: float = 0.9,\n",
    "        b: float = 0.4,\n",
    "        fb_terms: int = 10,\n",
    "        fb_docs: int = 10,\n",
    "        original_weight: float = 0.5,\n",
    "        index_name: str = \"robust04\"\n",
    "    ):\n",
    "        super().__init__(index_name)\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.fb_terms = fb_terms\n",
    "        self.fb_docs = fb_docs\n",
    "        self.original_weight = original_weight\n",
    "        self._searcher = None\n",
    "    \n",
    "    @property\n",
    "    def searcher(self) -> LuceneSearcher:\n",
    "        if self._searcher is None:\n",
    "            self._searcher = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "            self._searcher.set_bm25(k1=self.k1, b=self.b)\n",
    "            self._searcher.set_rm3(\n",
    "                fb_terms=self.fb_terms,\n",
    "                fb_docs=self.fb_docs,\n",
    "                original_query_weight=self.original_weight\n",
    "            )\n",
    "        return self._searcher\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        queries: Dict[str, str],\n",
    "        k: int = 1000\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        results = {}\n",
    "        for qid, query_text in tqdm(queries.items(), desc=\"RM3 Search\"):\n",
    "            hits = self.searcher.search(query_text, k=k)\n",
    "            results[qid] = [(hit.docid, hit.score) for hit in hits]\n",
    "        return results\n",
    "    \n",
    "    def get_params(self) -> Dict:\n",
    "        return {\n",
    "            \"k1\": self.k1,\n",
    "            \"b\": self.b,\n",
    "            \"fb_terms\": self.fb_terms,\n",
    "            \"fb_docs\": self.fb_docs,\n",
    "            \"original_weight\": self.original_weight\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✓ RM3Retriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0426d107",
   "metadata": {},
   "source": [
    "### 5.2. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91071833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best BM25 params: k1=0.6, b=0.4\n",
      "\n",
      "============================================================\n",
      "MODEL 2: Tuning RM3 parameters\n",
      "============================================================\n",
      "\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=5, original_weight=0.4 -> MAP=0.2591 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=5, original_weight=0.5 -> MAP=0.2654 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=5, original_weight=0.6 -> MAP=0.2660 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=10, original_weight=0.4 -> MAP=0.2520 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=10, original_weight=0.5 -> MAP=0.2570 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=10, original_weight=0.6 -> MAP=0.2599 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=15, original_weight=0.4 -> MAP=0.2548 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=15, original_weight=0.5 -> MAP=0.2597 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=15, original_weight=0.6 -> MAP=0.2615 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=5, original_weight=0.4 -> MAP=0.2699 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=5, original_weight=0.5 -> MAP=0.2709 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=5, original_weight=0.6 -> MAP=0.2702 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=10, original_weight=0.4 -> MAP=0.2631 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=10, original_weight=0.5 -> MAP=0.2666 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=10, original_weight=0.6 -> MAP=0.2641 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=15, original_weight=0.4 -> MAP=0.2688 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=15, original_weight=0.5 -> MAP=0.2738 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=15, original_weight=0.6 -> MAP=0.2698 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=5, original_weight=0.4 -> MAP=0.2722 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=5, original_weight=0.5 -> MAP=0.2713 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=5, original_weight=0.6 -> MAP=0.2685 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=10, original_weight=0.4 -> MAP=0.2633 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=10, original_weight=0.5 -> MAP=0.2657 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=10, original_weight=0.6 -> MAP=0.2659 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=15, original_weight=0.4 -> MAP=0.2737 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=15, original_weight=0.5 -> MAP=0.2717 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=15, original_weight=0.6 -> MAP=0.2680 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=5, original_weight=0.4 -> MAP=0.2725 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=5, original_weight=0.5 -> MAP=0.2719 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=5, original_weight=0.6 -> MAP=0.2675 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=10, original_weight=0.4 -> MAP=0.2625 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=10, original_weight=0.5 -> MAP=0.2647 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=10, original_weight=0.6 -> MAP=0.2670 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=15, original_weight=0.4 -> MAP=0.2699 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=15, original_weight=0.5 -> MAP=0.2709 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=15, original_weight=0.6 -> MAP=0.2695 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=5, original_weight=0.4 -> MAP=0.2767 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=5, original_weight=0.5 -> MAP=0.2727 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=5, original_weight=0.6 -> MAP=0.2671 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=10, original_weight=0.4 -> MAP=0.2665 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=10, original_weight=0.5 -> MAP=0.2655 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=10, original_weight=0.6 -> MAP=0.2660 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=15, original_weight=0.4 -> MAP=0.2698 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=15, original_weight=0.5 -> MAP=0.2745 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=15, original_weight=0.6 -> MAP=0.2686 [CACHED]\n",
      "\n",
      "============================================================\n",
      "RM3 Tuning Results (Top 5):\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>method</th>\n",
       "      <th>k1</th>\n",
       "      <th>b</th>\n",
       "      <th>fb_terms</th>\n",
       "      <th>fb_docs</th>\n",
       "      <th>original_weight</th>\n",
       "      <th>map</th>\n",
       "      <th>num_queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.276720</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RM3__b=0.4__fb_docs=15__fb_terms=100__k1=0.6__...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.274451</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RM3__b=0.4__fb_docs=15__fb_terms=25__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.273836</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RM3__b=0.4__fb_docs=15__fb_terms=50__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.273685</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.272742</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           config_key method   k1    b  \\\n",
       "36  RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...    RM3  0.6  0.4   \n",
       "43  RM3__b=0.4__fb_docs=15__fb_terms=100__k1=0.6__...    RM3  0.6  0.4   \n",
       "16  RM3__b=0.4__fb_docs=15__fb_terms=25__k1=0.6__o...    RM3  0.6  0.4   \n",
       "24  RM3__b=0.4__fb_docs=15__fb_terms=50__k1=0.6__o...    RM3  0.6  0.4   \n",
       "37  RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...    RM3  0.6  0.4   \n",
       "\n",
       "    fb_terms  fb_docs  original_weight       map  num_queries  \n",
       "36       100        5              0.4  0.276720           50  \n",
       "43       100       15              0.5  0.274451           50  \n",
       "16        25       15              0.5  0.273836           50  \n",
       "24        50       15              0.4  0.273685           50  \n",
       "37       100        5              0.5  0.272742           50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RM3_PARAM_GRID = {\n",
    "    \"k1\": [best_k1],        # fixed from BM25 tuning\n",
    "    \"b\": [best_b],          # fixed from BM25 tuning\n",
    "    \"fb_terms\": [10, 25, 50, 75, 100],\n",
    "    \"fb_docs\": [5, 10, 15],\n",
    "    \"original_weight\": [0.4, 0.5, 0.6],\n",
    "}\n",
    "\n",
    "# Run grid search with RM3Retriever class\n",
    "print(f\"Using best BM25 params: k1={best_k1}, b={best_b}\\n\")\n",
    "\n",
    "rm3_df = run_grid_search(\n",
    "    method_name=\"RM3\",\n",
    "    model_class=RM3Retriever,\n",
    "    param_grid=RM3_PARAM_GRID,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    model_num=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71ccf2",
   "metadata": {},
   "source": [
    "### 5.3. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aa11dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL 2 COMPLETE: Best RM3 Parameters\n",
      "============================================================\n",
      "  BM25 k1 = 0.6\n",
      "  BM25 b = 0.4\n",
      "  fb_terms = 100\n",
      "  fb_docs = 5\n",
      "  original_weight = 0.4\n",
      "  MAP = 0.2767\n",
      "\n",
      "  Improvement over BM25: +11.82%\n",
      "============================================================\n",
      "\n",
      "✓ Best RM3 model ready for Model 3\n"
     ]
    }
   ],
   "source": [
    "best_rm3 = rm3_df.iloc[0]\n",
    "best_fb_terms = int(best_rm3[\"fb_terms\"])\n",
    "best_fb_docs = int(best_rm3[\"fb_docs\"])\n",
    "best_orig_w = best_rm3[\"original_weight\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 2 COMPLETE: Best RM3 Parameters\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  BM25 k1 = {best_k1}\")\n",
    "print(f\"  BM25 b = {best_b}\")\n",
    "print(f\"  fb_terms = {best_fb_terms}\")\n",
    "print(f\"  fb_docs = {best_fb_docs}\")\n",
    "print(f\"  original_weight = {best_orig_w}\")\n",
    "print(f\"  MAP = {best_rm3['map']:.4f}\")\n",
    "\n",
    "improvement = (best_rm3['map'] - best_bm25['map']) / best_bm25['map'] * 100\n",
    "print(f\"\\n  Improvement over BM25: {improvement:+.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create best RM3 model\n",
    "rm3_model = RM3Retriever(\n",
    "    k1=best_k1, b=best_b,\n",
    "    fb_terms=best_fb_terms, fb_docs=best_fb_docs, original_weight=best_orig_w\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Best RM3 model ready for Model 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece92b96",
   "metadata": {},
   "source": [
    "## 6. MODEL 3: Advanced Neural Reranking\n",
    "\n",
    "Using best BM25/RM3 parameters from Models 1-2 as baseline for advanced neural reranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5ddf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Model 3 Architecture** (4-Stage Pipeline):\n",
    "\n",
    "1. **Multi-Branch Generation**: RM3 + SPLADE → initial candidates\n",
    "2. **RRF Fusion**: Merge RM3 + SPLADE with rrf_k parameter\n",
    "3. **Neural PRF + MiniLM MaxP**: \n",
    "   - Extract top passages from feedback docs using cross-encoder\n",
    "   - Expand query with selected passages (respects token budget)\n",
    "   - Rerank documents by best-passage score\n",
    "4. **Score Blending**: Normalize and interpolate RRF + MiniLM with alpha parameter\n",
    "\n",
    "**Model**: cross-encoder/ms-marco-MiniLM-L-6-v2 (passage ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6917a4b",
   "metadata": {},
   "source": [
    "### 6.1. Model 3 Architecture Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2017ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MultiBranchRetriever ready\n"
     ]
    }
   ],
   "source": [
    "class MultiBranchRetriever(BaseRetriever):\n",
    "    \"\"\"Robust04-optimized multi-stage retrieval with controlled neural PRF.\"\"\"\n",
    "\n",
    "    TEXT_RE = re.compile(r\"<TEXT>(.*?)</TEXT>\", re.DOTALL | re.IGNORECASE)\n",
    "    TAG_RE  = re.compile(r\"<[^>]+>\")\n",
    "    TOK_RE  = re.compile(r\"[A-Za-z]+(?:-[A-Za-z]+)?\")\n",
    "\n",
    "    STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        k1: float = 0.9,\n",
    "        b: float = 0.4,\n",
    "        fb_terms: int = 80,\n",
    "        fb_docs: int = 10,\n",
    "        original_weight: float = 0.3,\n",
    "        rrf_k: int = 20,\n",
    "        prf_fb_docs: int = 5,\n",
    "        prf_top_passages: int = 10,\n",
    "        passage_size: int = 192,\n",
    "        passage_stride: int = 128,\n",
    "        expansion_terms: int = 12,\n",
    "        expansion_token_budget: int = 40,\n",
    "        rerank_k: int = 200,\n",
    "        batch_size: int = 64,\n",
    "        alpha: float = 0.35,\n",
    "        index_name: str = \"robust04\",\n",
    "        splade_index_name: str = \"beir-v1.0.0-robust04.splade-pp-ed\",\n",
    "        model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "        device: str = DEVICE,\n",
    "    ):\n",
    "        super().__init__(index_name)\n",
    "        \n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.fb_terms = fb_terms\n",
    "        self.fb_docs = fb_docs\n",
    "        self.original_weight = original_weight\n",
    "        self.rrf_k = rrf_k\n",
    "\n",
    "        self.prf_fb_docs = prf_fb_docs\n",
    "        self.prf_top_passages = prf_top_passages\n",
    "\n",
    "        self.passage_size = passage_size\n",
    "        self.passage_stride = passage_stride\n",
    "\n",
    "        self.expansion_terms = expansion_terms\n",
    "        self.expansion_token_budget = expansion_token_budget\n",
    "\n",
    "        self.rerank_k = rerank_k\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.splade_index_name = splade_index_name\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "\n",
    "        self._rm3 = self._splade = self._doc_searcher = self._reranker = None\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Searchers\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    @property\n",
    "    def rm3(self):\n",
    "        if self._rm3 is None:\n",
    "            s = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "            s.set_bm25(self.k1, self.b)\n",
    "            s.set_rm3(self.fb_terms, self.fb_docs, self.original_weight)\n",
    "            self._rm3 = s\n",
    "        return self._rm3\n",
    "\n",
    "    @property\n",
    "    def splade(self):\n",
    "        if self._splade is None:\n",
    "            self._splade = LuceneImpactSearcher.from_prebuilt_index(\n",
    "                self.splade_index_name,\n",
    "                SpladeQueryEncoder(\"naver/splade-cocondenser-ensembledistil\", device=self.device),\n",
    "            )\n",
    "        return self._splade\n",
    "\n",
    "    @property\n",
    "    def doc_searcher(self):\n",
    "        if self._doc_searcher is None:\n",
    "            self._doc_searcher = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "        return self._doc_searcher\n",
    "\n",
    "    @property\n",
    "    def reranker(self):\n",
    "        if self._reranker is None:\n",
    "            self._reranker = CrossEncoder(self.model_name, max_length=512, device=self.device)\n",
    "        return self._reranker\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Document & Passage Utilities\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    @lru_cache(maxsize=10000)\n",
    "    def _doc_text(self, docid: str) -> str:\n",
    "        doc = self.doc_searcher.doc(docid)\n",
    "        if not doc:\n",
    "            return \"\"\n",
    "        raw = doc.raw() or \"\"\n",
    "        text = \" \".join(self.TEXT_RE.findall(raw)) or self.TAG_RE.sub(\" \", raw)\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    @lru_cache(maxsize=10000)\n",
    "    def _doc_passages(self, docid: str) -> List[str]:\n",
    "        words = self._doc_text(docid).split()\n",
    "        return [\n",
    "            \" \".join(words[i:i + self.passage_size])\n",
    "            for i in range(0, len(words), self.passage_stride)\n",
    "            if len(words[i:i + self.passage_size]) >= 30\n",
    "        ]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Tokenization & PRF\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def _tokens(self, text: str) -> List[str]:\n",
    "        return [\n",
    "            t.lower() for t in self.TOK_RE.findall(text)\n",
    "            if 3 <= len(t) <= 22 and t.lower() not in self.STOPWORDS\n",
    "        ]\n",
    "\n",
    "    def _limit_query(self, q: str, expansion: str) -> str:\n",
    "        base = self._tokens(q)\n",
    "        extra = [t for t in self._tokens(expansion) if t not in base]\n",
    "        kept = base + extra[: max(0, self.expansion_token_budget - len(base))]\n",
    "        return q + (\" \" + \" \".join(kept[len(base):]) if len(kept) > len(base) else \"\")\n",
    "\n",
    "    def _prf_terms(self, query: str, docids: List[str]) -> str:\n",
    "        pairs, passages = [], []\n",
    "\n",
    "        for d in docids[: self.prf_fb_docs]:\n",
    "            for p in self._doc_passages(d):\n",
    "                pairs.append([query, p])\n",
    "                passages.append(p)\n",
    "\n",
    "        if not pairs:\n",
    "            return \"\"\n",
    "\n",
    "        scores = self.reranker.predict(pairs, batch_size=self.batch_size, show_progress_bar=False)\n",
    "        top_passages = [p for p, _ in sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)\n",
    "                        [: self.prf_top_passages]]\n",
    "\n",
    "        tf = Counter()\n",
    "        for p in top_passages:\n",
    "            tf.update(self._tokens(p))\n",
    "\n",
    "        return \" \".join(t for t, _ in tf.most_common(self.expansion_terms))\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Fusion Helpers\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def _rrf(self, runs: List[Dict[str, List[Tuple[str, float]]]], k: int):\n",
    "        fused = defaultdict(lambda: defaultdict(float))\n",
    "        for run in runs:\n",
    "            for qid, ranked in run.items():\n",
    "                for r, (d, _) in enumerate(ranked, 1):\n",
    "                    fused[qid][d] += 1 / (self.rrf_k + r)\n",
    "        return {q: sorted(v.items(), key=lambda x: x[1], reverse=True)[:k] for q, v in fused.items()}\n",
    "\n",
    "    def _maxp(self, scores: Iterable[float], docids: Iterable[str]):\n",
    "        out = defaultdict(lambda: -1e9)\n",
    "        for s, d in zip(scores, docids):\n",
    "            out[d] = max(out[d], float(s))\n",
    "        return out\n",
    "\n",
    "    def _norm(self, xs: Dict[str, float]):\n",
    "        if not xs:\n",
    "            return {}\n",
    "        lo, hi = min(xs.values()), max(xs.values())\n",
    "        return {k: 0.0 if hi == lo else (v - lo) / (hi - lo) for k, v in xs.items()}\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Main Search\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def search(self, queries: Dict[str, str], k: int = 1000):\n",
    "        rm3 = {q: [(h.docid, h.score) for h in self.rm3.search(t, k)]\n",
    "               for q, t in tqdm(queries.items(), desc=\"RM3\")}\n",
    "\n",
    "        spl = {q: [(h.docid, h.score) for h in self.splade.search(t, k)]\n",
    "               for q, t in tqdm(queries.items(), desc=\"SPLADE\")}\n",
    "\n",
    "        fused = self._rrf([rm3, spl], k)\n",
    "        fused_scores = {q: dict(v) for q, v in fused.items()}\n",
    "\n",
    "        neural_scores = {}\n",
    "\n",
    "        for qid, q in tqdm(queries.items(), desc=\"PRF + MiniLM\"):\n",
    "            docs = [d for d, _ in fused[qid]]\n",
    "            exp = self._prf_terms(q, docs)\n",
    "            rq = self._limit_query(q, exp)\n",
    "\n",
    "            pairs, meta = [], []\n",
    "            for d in docs[: self.rerank_k]:\n",
    "                for p in self._doc_passages(d):\n",
    "                    pairs.append([rq, p])\n",
    "                    meta.append(d)\n",
    "\n",
    "            if pairs:\n",
    "                s = self.reranker.predict(pairs, batch_size=self.batch_size, show_progress_bar=False)\n",
    "                neural_scores[qid] = self._maxp(s, meta)\n",
    "            else:\n",
    "                neural_scores[qid] = {}\n",
    "\n",
    "        final = {}\n",
    "        for q in queries:\n",
    "            b, n = self._norm(fused_scores.get(q, {})), self._norm(neural_scores.get(q, {}))\n",
    "            final[q] = sorted(\n",
    "                {d: (1 - self.alpha) * b.get(d, 0) + self.alpha * n.get(d, 0)\n",
    "                 for d in set(b) | set(n)}.items(),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:k]\n",
    "\n",
    "        return final\n",
    "\n",
    "    def get_params(self) -> Dict:\n",
    "        return {k: getattr(self, k) for k in [\n",
    "            \"k1\",\"b\",\"fb_terms\",\"fb_docs\",\"original_weight\",\"rrf_k\",\n",
    "            \"prf_fb_docs\",\"prf_top_passages\",\"passage_size\",\"passage_stride\",\n",
    "            \"expansion_terms\",\"expansion_token_budget\",\n",
    "            \"rerank_k\",\"batch_size\",\"alpha\"\n",
    "        ]}\n",
    "\n",
    "\n",
    "print(\"✓ MultiBranchRetriever ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfde70e",
   "metadata": {},
   "source": [
    "### 6.2. 3-Phase Fine-Tuning Strategy\n",
    "\n",
    "**Phase 1 - Passage Configuration**: Optimize passage size, stride, and reranking depth\n",
    "\n",
    "**Phase 2 - Neural PRF**: Optimize feedback docs, top passages, and expansion terms  \n",
    "\n",
    "**Phase 3 - Score Blending**: Optimize RRF constant and final blend weight (alpha)\n",
    "\n",
    "Each phase uses best configurations from the previous phase, reducing search space progressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9fb6c3",
   "metadata": {},
   "source": [
    "#### 6.2.1. Phase Running Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4e28361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase runner ready\n"
     ]
    }
   ],
   "source": [
    "def _generate_configs(param_grid: Dict, base_config: Dict, prev_best: Optional[List[Dict]]) -> List[Dict]:\n",
    "    \"\"\"Generate all configurations for this phase.\"\"\"\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    \n",
    "    if prev_best is None:\n",
    "        # Phase 1: expand param_grid directly on top of base_config\n",
    "        return [{**base_config, **dict(zip(param_names, combo))}\n",
    "                for combo in product(*param_values)]\n",
    "    else:\n",
    "        # Phase 2/3: expand from previous best configs while keeping base_config defaults\n",
    "        return [{**base_config,\n",
    "                 **{k: v for k, v in prev_config.items() if k != \"map\"},\n",
    "                 **dict(zip(param_names, combo))}\n",
    "                for prev_config in prev_best\n",
    "                for combo in product(*param_values)]\n",
    "\n",
    "\n",
    "def _reconstruct_config(row: pd.Series, base_config: Dict) -> Dict:\n",
    "    \"\"\"Reconstruct config dict from DataFrame row with proper types.\"\"\"\n",
    "    # Start with base_config so fixed values persist across phases\n",
    "    config = {**base_config, **{k: row[k] for k in row.index\n",
    "                                if k not in [\"config_key\", \"map\", \"method\", \"num_queries\"]\n",
    "                                and pd.notna(row[k])}}\n",
    "    \n",
    "    # Cast to correct types using base_config as reference\n",
    "    for k, v in list(config.items()):\n",
    "        if k in base_config:\n",
    "            config[k] = type(base_config[k])(v)\n",
    "    \n",
    "    # Store map separately for sorting, not in config\n",
    "    config[\"map\"] = row[\"map\"]\n",
    "    return config\n",
    "\n",
    "\n",
    "def run_phase(\n",
    "    phase_name: str,\n",
    "    phase_num: int,\n",
    "    param_grid: Dict[str, list],\n",
    "    base_config: Dict,\n",
    "    prev_best_configs: Optional[List[Dict]] = None,\n",
    "    model_class: type = None,\n",
    "    queries: Dict[str, str] = None,\n",
    "    qrels: Dict[str, Dict[str, int]] = None,\n",
    "    top_k: int = 3\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run a Model 3 fine-tuning phase with grid search and caching.\n",
    "    \n",
    "    Returns: List of top-k configurations with their MAP scores\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\\n{phase_name}\\n{'=' * 80}\")\n",
    "    \n",
    "    # Single results file across phases\n",
    "    results_csv_path = get_results_csv(model_num=3)\n",
    "\n",
    "    # Generate all configs\n",
    "    configs = _generate_configs(param_grid, base_config, prev_best_configs)\n",
    "    \n",
    "    print(f\"Total configurations: {len(configs)}\")\n",
    "    if prev_best_configs:\n",
    "        print(f\"Expanding from {len(prev_best_configs)} previous configs\")\n",
    "    print(f\"Tuning parameters: {list(param_grid.keys())}\\n{'-' * 80}\")\n",
    "    \n",
    "    # Check cache across all phases; skip any config already computed\n",
    "    completed = load_completed_configs(model_num=3)\n",
    "    pending = [c for c in configs if generate_config_key(\"MultiBranch\", c) not in completed]\n",
    "    \n",
    "    print(f\"Completed: {len(configs) - len(pending)} | Pending: {len(pending)}\\n{'-' * 80}\")\n",
    "    \n",
    "    # Run pending experiments\n",
    "    for i, config in enumerate(pending, 1):\n",
    "        config_key = generate_config_key(\"MultiBranch\", config)\n",
    "        print(f\"[{i}/{len(pending)}] {config_key[:60]}...\", end=\" \")\n",
    "        \n",
    "        model = model_class(**config)\n",
    "        run = model.search(queries, k=1000)\n",
    "        metrics = evaluate_run(run, qrels, config_key)\n",
    "        \n",
    "        # Save tuned params + base params (no phase column)\n",
    "        tuned_keys = set(base_config.keys()) | set(param_grid.keys())\n",
    "        if prev_best_configs:\n",
    "            tuned_keys |= {k for k in prev_best_configs[0].keys() if k != \"map\"}\n",
    "        \n",
    "        result = {\n",
    "            \"config_key\": config_key,\n",
    "            **{k: v for k, v in config.items() if k in tuned_keys},\n",
    "            \"map\": metrics[\"map\"]\n",
    "        }\n",
    "        \n",
    "        save_experiment_result(result, model_num=3)\n",
    "        print(f\"MAP={metrics['map']:.4f}\")\n",
    "    \n",
    "    # Load and display results for this phase (filtered by generated configs)\n",
    "    if not os.path.exists(results_csv_path):\n",
    "        return []\n",
    "    \n",
    "    df = pd.read_csv(results_csv_path)\n",
    "    config_keys = [generate_config_key(\"MultiBranch\", c) for c in configs]\n",
    "    df = df[df[\"config_key\"].isin(config_keys)].sort_values(\"map\", ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\\n{phase_name} - Top {min(5, len(df))} Results:\\n{'=' * 80}\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Return top-k with proper types\n",
    "    top_configs = [_reconstruct_config(row, base_config) \n",
    "                   for _, row in df.head(top_k).iterrows()]\n",
    "    \n",
    "    print(f\"\\n✓ Selected top {len(top_configs)} configs for next phase\\n\")\n",
    "    return top_configs\n",
    "\n",
    "\n",
    "print(\"✓ Phase runner ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399b32b",
   "metadata": {},
   "source": [
    "#### 6.2.2. PHASE 1: Passage Configuration & Reranking Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9740f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 3 - PHASE 1: Passage Configuration & Reranking Depth\n",
      "================================================================================\n",
      "Total configurations: 24\n",
      "Tuning parameters: ['passage_size', 'passage_stride', 'rerank_k']\n",
      "--------------------------------------------------------------------------------\n",
      "Completed: 24 | Pending: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "MODEL 3 - PHASE 1: Passage Configuration & Reranking Depth - Top 5 Results:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>k1</th>\n",
       "      <th>b</th>\n",
       "      <th>fb_terms</th>\n",
       "      <th>fb_docs</th>\n",
       "      <th>original_weight</th>\n",
       "      <th>rrf_k</th>\n",
       "      <th>prf_fb_docs</th>\n",
       "      <th>prf_top_passages</th>\n",
       "      <th>expansion_terms</th>\n",
       "      <th>expansion_token_budget</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>alpha</th>\n",
       "      <th>passage_size</th>\n",
       "      <th>passage_stride</th>\n",
       "      <th>rerank_k</th>\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.327163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.325184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>200</td>\n",
       "      <td>0.322310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>192</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.320701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>200</td>\n",
       "      <td>0.320562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           config_key   k1    b  fb_terms  \\\n",
       "8   MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "19  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "2   MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "11  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "18  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "\n",
       "    fb_docs  original_weight  rrf_k  prf_fb_docs  prf_top_passages  \\\n",
       "8         5              0.4     20            5                10   \n",
       "19        5              0.4     20            5                10   \n",
       "2         5              0.4     20            5                10   \n",
       "11        5              0.4     20            5                10   \n",
       "18        5              0.4     20            5                10   \n",
       "\n",
       "    expansion_terms  expansion_token_budget  batch_size  alpha  passage_size  \\\n",
       "8                12                      40           8   0.35           128   \n",
       "19               12                      40           8   0.35           256   \n",
       "2                12                      40           8   0.35           128   \n",
       "11               12                      40           8   0.35           192   \n",
       "18               12                      40           8   0.35           256   \n",
       "\n",
       "    passage_stride  rerank_k       map  \n",
       "8               64       500  0.327163  \n",
       "19              64       500  0.325184  \n",
       "2               64       200  0.322310  \n",
       "11              64       500  0.320701  \n",
       "18              64       200  0.320562  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Selected top 2 configs for next phase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PHASE_1_GRID = {\n",
    "    \"passage_size\": [128, 192, 256],\n",
    "    \"passage_stride\": [64, 128],\n",
    "    \"rerank_k\": [50, 100, 200, 500],\n",
    "}\n",
    "\n",
    "PHASE_1_FIXED = {\n",
    "    \"k1\": best_k1,\n",
    "    \"b\": best_b,\n",
    "    \"fb_terms\": best_fb_terms,\n",
    "    \"fb_docs\": best_fb_docs,\n",
    "    \"original_weight\": best_orig_w,\n",
    "    \"rrf_k\": 20,\n",
    "    \"prf_fb_docs\": 5,\n",
    "    \"prf_top_passages\": 10,\n",
    "    \"expansion_terms\": 12,\n",
    "    \"expansion_token_budget\": 40,\n",
    "    \"batch_size\": 8,\n",
    "    \"alpha\": 0.35,\n",
    "}\n",
    "\n",
    "phase_1_best = run_phase(\n",
    "    phase_name=\"MODEL 3 - PHASE 1: Passage Configuration & Reranking Depth\",\n",
    "    phase_num=1,\n",
    "    param_grid=PHASE_1_GRID,\n",
    "    base_config=PHASE_1_FIXED,\n",
    "    prev_best_configs=None,\n",
    "    model_class=MultiBranchRetriever,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65617d2f",
   "metadata": {},
   "source": [
    "#### 6.2.3. PHASE 2: Neural PRF Capacity & Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab9c8f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 3 - PHASE 2: Neural PRF Capacity & Query Expansion\n",
      "================================================================================\n",
      "Total configurations: 36\n",
      "Expanding from 2 previous configs\n",
      "Tuning parameters: ['prf_fb_docs', 'prf_top_passages', 'expansion_terms']\n",
      "--------------------------------------------------------------------------------\n",
      "Completed: 36 | Pending: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "MODEL 3 - PHASE 2: Neural PRF Capacity & Query Expansion - Top 5 Results:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>k1</th>\n",
       "      <th>b</th>\n",
       "      <th>fb_terms</th>\n",
       "      <th>fb_docs</th>\n",
       "      <th>original_weight</th>\n",
       "      <th>rrf_k</th>\n",
       "      <th>prf_fb_docs</th>\n",
       "      <th>prf_top_passages</th>\n",
       "      <th>expansion_terms</th>\n",
       "      <th>expansion_token_budget</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>alpha</th>\n",
       "      <th>passage_size</th>\n",
       "      <th>passage_stride</th>\n",
       "      <th>rerank_k</th>\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.331545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.329912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.328240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.327612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MultiBranch__alpha=0.35__b=0.4__batch_size=8__...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.327227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           config_key   k1    b  fb_terms  \\\n",
       "54  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "52  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "47  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "63  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "35  MultiBranch__alpha=0.35__b=0.4__batch_size=8__...  0.6  0.4       100   \n",
       "\n",
       "    fb_docs  original_weight  rrf_k  prf_fb_docs  prf_top_passages  \\\n",
       "54        5              0.4     20           10                 5   \n",
       "52        5              0.4     20           10                 5   \n",
       "47        5              0.4     20            5                 5   \n",
       "63        5              0.4     20           10                10   \n",
       "35        5              0.4     20           10                 5   \n",
       "\n",
       "    expansion_terms  expansion_token_budget  batch_size  alpha  passage_size  \\\n",
       "54               12                      40           8   0.35           256   \n",
       "52               24                      40           8   0.35           256   \n",
       "47               12                      40           8   0.35           256   \n",
       "63               24                      40           8   0.35           256   \n",
       "35               12                      40           8   0.35           128   \n",
       "\n",
       "    passage_stride  rerank_k       map  \n",
       "54              64       500  0.331545  \n",
       "52              64       500  0.329912  \n",
       "47              64       500  0.328240  \n",
       "63              64       500  0.327612  \n",
       "35              64       500  0.327227  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Selected top 2 configs for next phase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PHASE_2_GRID = {\n",
    "    \"prf_fb_docs\": [3, 5, 10],\n",
    "    \"prf_top_passages\": [5, 10, 20],\n",
    "    \"expansion_terms\": [12, 24],\n",
    "}\n",
    "\n",
    "phase_2_best = run_phase(\n",
    "    phase_name=\"MODEL 3 - PHASE 2: Neural PRF Capacity & Query Expansion\",\n",
    "    phase_num=2,\n",
    "    param_grid=PHASE_2_GRID,\n",
    "    base_config=PHASE_1_FIXED,\n",
    "    prev_best_configs=phase_1_best,\n",
    "    model_class=MultiBranchRetriever,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc064f26",
   "metadata": {},
   "source": [
    "#### 6.2.4. PHASE 3: Score Blending & RRF Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "864e964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 3 - PHASE 3: Score Blending & RRF Fusion\n",
      "================================================================================\n",
      "Total configurations: 42\n",
      "Expanding from 2 previous configs\n",
      "Tuning parameters: ['rrf_k', 'alpha']\n",
      "--------------------------------------------------------------------------------\n",
      "Completed: 21 | Pending: 21\n",
      "--------------------------------------------------------------------------------\n",
      "[1/21] MultiBranch__alpha=0.2__b=0.4__batch_size=8__expansion_terms... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RM3: 100%|██████████| 50/50 [00:13<00:00,  3.71it/s]\n",
      "SPLADE:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize pre-built index beir-v1.0.0-robust04.splade-pp-ed.\n",
      "/home/galnoy/.cache/pyserini/indexes/lucene-inverted.beir-v1.0.0-robust04.splade-pp-ed.20231124.a66f86f.c1a6fd094bb9e34e69e10040d9b0ad2a already exists, skipping download.\n",
      "Initializing beir-v1.0.0-robust04.splade-pp-ed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPLADE: 100%|██████████| 50/50 [00:05<00:00,  8.93it/s]\n",
      "PRF + MiniLM:   4%|▍         | 2/50 [01:51<44:40, 55.84s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m PHASE_3_GRID \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrrf_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m60\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.8\u001b[39m],\n\u001b[1;32m      4\u001b[0m }\n\u001b[0;32m----> 6\u001b[0m phase_3_best \u001b[38;5;241m=\u001b[39m \u001b[43mrun_phase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMODEL 3 - PHASE 3: Score Blending & RRF Fusion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphase_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPHASE_3_GRID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPHASE_1_FIXED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_best_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase_2_best\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMultiBranchRetriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqrels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqrels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 77\u001b[0m, in \u001b[0;36mrun_phase\u001b[0;34m(phase_name, phase_num, param_grid, base_config, prev_best_configs, model_class, queries, qrels, top_k)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pending)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_key[:\u001b[38;5;241m60\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m---> 77\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_run(run, qrels, config_key)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Save tuned params + base params (no phase column)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 206\u001b[0m, in \u001b[0;36mMultiBranchRetriever.search\u001b[0;34m(self, queries, k)\u001b[0m\n\u001b[1;32m    203\u001b[0m         meta\u001b[38;5;241m.\u001b[39mappend(d)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pairs:\n\u001b[0;32m--> 206\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     neural_scores[qid] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maxp(s, meta)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/util.py:68\u001b[0m, in \u001b[0;36mcross_encoder_predict_rank_args_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mpop(deprecated_arg)\n\u001b[1;32m     64\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe CrossEncoder.predict `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` argument is deprecated and has no effect. It will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         )\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:713\u001b[0m, in \u001b[0;36mCrossEncoder.predict\u001b[0;34m(self, sentences, batch_size, show_progress_bar, activation_fn, apply_softmax, convert_to_numpy, convert_to_tensor, device, pool, chunk_size)\u001b[0m\n\u001b[1;32m    706\u001b[0m batch \u001b[38;5;241m=\u001b[39m sentences[start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m    707\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    708\u001b[0m     batch,\n\u001b[1;32m    709\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    710\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    711\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    712\u001b[0m )\n\u001b[0;32m--> 713\u001b[0m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    714\u001b[0m model_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfeatures, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    715\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(model_predictions\u001b[38;5;241m.\u001b[39mlogits)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:838\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    839\u001b[0m         k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(v\u001b[38;5;241m.\u001b[39mto) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    841\u001b[0m     }\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    843\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:839\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 839\u001b[0m         k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(v\u001b[38;5;241m.\u001b[39mto) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    841\u001b[0m     }\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    843\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PHASE_3_GRID = {\n",
    "    \"rrf_k\": [20, 40, 60],\n",
    "    \"alpha\": [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "}\n",
    "\n",
    "phase_3_best = run_phase(\n",
    "    phase_name=\"MODEL 3 - PHASE 3: Score Blending & RRF Fusion\",\n",
    "    phase_num=3,\n",
    "    param_grid=PHASE_3_GRID,\n",
    "    base_config=PHASE_1_FIXED,\n",
    "    prev_best_configs=phase_2_best,\n",
    "    model_class=MultiBranchRetriever,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    top_k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912eecdb",
   "metadata": {},
   "source": [
    "#### 6.2.5. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best overall config from Phase 3\n",
    "best_phase_3 = phase_3_best[0]\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 3 COMPLETE: Best Configuration (After 3-Phase Fine-Tuning)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  passage_size={best_phase_3['passage_size']}\")\n",
    "print(f\"  passage_stride={best_phase_3['passage_stride']}\")\n",
    "print(f\"  rerank_k={best_phase_3['rerank_k']}\")\n",
    "print(f\"  prf_fb_docs={best_phase_3['prf_fb_docs']}\")\n",
    "print(f\"  prf_top_passages={best_phase_3['prf_top_passages']}\")\n",
    "print(f\"  expansion_terms={best_phase_3['expansion_terms']}\")\n",
    "print(f\"  rrf_k={best_phase_3['rrf_k']}\")\n",
    "print(f\"  alpha={best_phase_3['alpha']}\")\n",
    "print(f\"  MAP={best_phase_3['map']:.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create final best model\n",
    "multi_branch_model = MultiBranchRetriever(\n",
    "    passage_size=int(best_phase_3['passage_size']),\n",
    "    passage_stride=int(best_phase_3['passage_stride']),\n",
    "    rerank_k=int(best_phase_3['rerank_k']),\n",
    "    prf_fb_docs=int(best_phase_3['prf_fb_docs']),\n",
    "    prf_top_passages=int(best_phase_3['prf_top_passages']),\n",
    "    expansion_terms=int(best_phase_3['expansion_terms']),\n",
    "    rrf_k=int(best_phase_3['rrf_k']),\n",
    "    alpha=float(best_phase_3['alpha']),\n",
    "    k1=best_k1,\n",
    "    b=best_b,\n",
    "    fb_terms=best_fb_terms,\n",
    "    fb_docs=best_fb_docs,\n",
    "    original_weight=best_orig_w,\n",
    "    expansion_token_budget=40,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Final model ready for test inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58802f78",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527a6da",
   "metadata": {},
   "source": [
    "### 7.1. Training Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a86717",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        \"Method\": \"BM25\",\n",
    "        \"MAP\": best_bm25[\"map\"],\n",
    "        \"Improvement\": \"baseline\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"BM25 + RM3\",\n",
    "        \"MAP\": best_rm3[\"map\"],\n",
    "        \"Improvement\": f\"+{(best_rm3['map'] - best_bm25['map']) / best_bm25['map'] * 100:.2f}%\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"Neural\",\n",
    "        \"MAP\": best_phase_3['map'],\n",
    "        \"Improvement\": f\"+{(best_phase_3['map'] - best_bm25['map']) / best_bm25['map'] * 100:.2f}%\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"3-MODEL PIPELINE TRAINING PERFORMANCE (50 queries with qrels)\")\n",
    "print(\"=\" * 100)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\nBest configurations:\")\n",
    "print(f\"  Model 1 (BM25): k1={best_k1}, b={best_b}\")\n",
    "print(f\"  Model 2 (RM3): fb_terms={best_fb_terms}, fb_docs={best_fb_docs}, original_weight={best_orig_w}\")\n",
    "print(f\"\\n  Model 3 (Neural - 3-Phase Fine-tuned):\")\n",
    "print(f\"    Passage: size={int(best_phase_3['passage_size'])}, stride={int(best_phase_3['passage_stride'])}\")\n",
    "print(f\"    PRF: fb_docs={int(best_phase_3['prf_fb_docs'])}, top_passages={int(best_phase_3['prf_top_passages'])}\")\n",
    "print(f\"    Expansion: terms={int(best_phase_3['expansion_terms'])}\")\n",
    "print(f\"    Reranking: rerank_k={int(best_phase_3['rerank_k'])}\")\n",
    "print(f\"    Fusion: rrf_k={int(best_phase_3['rrf_k'])}, alpha={best_phase_3['alpha']:.2f}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47c634",
   "metadata": {},
   "source": [
    "### 7.2. Visualization & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f817abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "methods = summary[\"Method\"].tolist()\n",
    "maps = summary[\"MAP\"].tolist()\n",
    "colors = [\"#3498db\", \"#2ecc71\", \"#e74c3c\"]\n",
    "\n",
    "bars = ax.bar(methods, maps, color=colors, edgecolor=\"black\")\n",
    "ax.set_ylabel(\"MAP\")\n",
    "ax.set_title(\"Training Performance Comparison (50 queries)\")\n",
    "ax.set_ylim(0, max(maps) * 1.15)\n",
    "\n",
    "for bar, m in zip(bars, maps):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f\"{m:.4f}\", ha=\"center\", va=\"bottom\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ All models tuned and evaluated on training queries\")\n",
    "print(\"\\n→ Ready to generate submission files for test queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493be89",
   "metadata": {},
   "source": [
    "## 8. Generate Submission Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc2b33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Final Step**: Using the best configurations from each model, we generate predictions for the 199 test queries (without relevance judgments)\n",
    "\n",
    "Each model produces a TREC-formatted run file with 1000 ranked documents per query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7959369",
   "metadata": {},
   "source": [
    "### 8.1. Run Inference on Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8eee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"GENERATING SUBMISSION FILES: {len(test_queries)} test queries (no qrels)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Run 1: Model 1 - BM25 Baseline\n",
    "print(\"Run 1: Model 1 - BM25 Baseline\")\n",
    "print(f\"  k1={best_k1}, b={best_b}\")\n",
    "run_1 = bm25_model.search(test_queries, k=1000)\n",
    "print(f\"  ✓ {len(run_1)} queries processed\")\n",
    "\n",
    "# Run 2: Model 2 - BM25 + RM3\n",
    "print(\"\\nRun 2: Model 2 - BM25 + RM3 Query Expansion\")\n",
    "print(f\"  k1={best_k1}, b={best_b}, fb_terms={best_fb_terms}, fb_docs={best_fb_docs}\")\n",
    "run_2 = rm3_model.search(test_queries, k=1000)\n",
    "print(f\"  ✓ {len(run_2)} queries processed\")\n",
    "\n",
    "# Run 3: Model 3 - Neural\n",
    "print(\"\\nRun 3: Model 3 - Neural Reranking\")\n",
    "print(f\"  Passage: size={int(best_phase_3['passage_size'])}, stride={int(best_phase_3['passage_stride'])}\")\n",
    "print(f\"  PRF: fb_docs={int(best_phase_3['prf_fb_docs'])}, top_passages={int(best_phase_3['prf_top_passages'])}\")\n",
    "print(f\"  Expansion: terms={int(best_phase_3['expansion_terms'])}\")\n",
    "print(f\"  Reranking: rerank_k={int(best_phase_3['rerank_k'])}\")\n",
    "print(f\"  Fusion: rrf_k={int(best_phase_3['rrf_k'])}, alpha={best_phase_3['alpha']:.2f}\")\n",
    "run_3 = multi_branch_model.search(test_queries, k=1000)\n",
    "print(f\"  ✓ {len(run_3)} queries processed\")\n",
    "\n",
    "print(\"\\n✓ All test queries processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f94a8",
   "metadata": {},
   "source": [
    "### 8.2. Export TREC Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2789317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_trec_run(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    filepath: str,\n",
    "    run_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Write run to TREC format.\n",
    "    Format: topic_id Q0 doc_id rank score run_name\n",
    "    \"\"\"\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for qid in sorted(run.keys(), key=lambda x: int(x)):\n",
    "            results = run[qid]\n",
    "            # Sort by score descending\n",
    "            sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "            for rank, (docid, score) in enumerate(sorted_results[:1000], start=1):\n",
    "                f.write(f\"{qid} Q0 {docid} {rank} {score:.6f} {run_name}\\n\")\n",
    "    \n",
    "    print(f\"✓ Written: {filepath}\")\n",
    "\n",
    "\n",
    "print(\"Writing submission files...\\n\")\n",
    "\n",
    "write_trec_run(run_1, \"run_1.res\", \"run_1\")\n",
    "write_trec_run(run_2, \"run_2.res\", \"run_2\")\n",
    "write_trec_run(run_3, \"run_3.res\", \"run_3\")\n",
    "\n",
    "print(\"\\n✓ All submission files written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38715c",
   "metadata": {},
   "source": [
    "### 8.3. Validate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_run_file(filepath: str, expected_queries: int = 199, docs_per_query: int = 1000):\n",
    "    \"\"\"Validate TREC run file format and contents.\"\"\"\n",
    "    query_docs = defaultdict(list)\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            qid, _, docid, rank, score, _ = parts\n",
    "            query_docs[qid].append((int(rank), float(score)))\n",
    "    \n",
    "    # Check query count\n",
    "    assert len(query_docs) == expected_queries, f\"Expected {expected_queries} queries, got {len(query_docs)}\"\n",
    "    \n",
    "    # Check docs per query and score ordering\n",
    "    for qid, docs in query_docs.items():\n",
    "        assert len(docs) == docs_per_query, f\"Query {qid}: expected {docs_per_query} docs, got {len(docs)}\"\n",
    "        scores = [s for _, s in sorted(docs, key=lambda x: x[0])]\n",
    "        assert scores == sorted(scores, reverse=True), f\"Query {qid}: scores not in decreasing order\"\n",
    "    \n",
    "    print(f\"✓ {filepath}: {len(query_docs)} queries × {docs_per_query} docs, scores non-increasing\")\n",
    "\n",
    "\n",
    "print(\"Validating submission files...\\n\")\n",
    "\n",
    "validate_run_file(\"run_1.res\")\n",
    "validate_run_file(\"run_2.res\")\n",
    "validate_run_file(\"run_3.res\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION FILES READY\")\n",
    "print(\"=\"*60)\n",
    "print(\"Files: run_1.res, run_2.res, run_3.res\")\n",
    "print(\"Format: TREC 6-column\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
