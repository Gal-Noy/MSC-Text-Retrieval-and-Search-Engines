{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b40988",
   "metadata": {},
   "source": [
    "# Robust04 Information Retrieval: 3-Phase Neural Reranking Pipeline\n",
    "\n",
    "**Objective**: Build and evaluate a progressive retrieval system on the TREC Robust04 collection.\n",
    "\n",
    "**Dataset**: \n",
    "- 249 queries total (50 with relevance judgments for training, 199 for testing)\n",
    "- ~528K documents from TREC disks 4 & 5\n",
    "\n",
    "**Methodology**: \n",
    "This notebook implements a 3-phase retrieval pipeline with systematic parameter tuning:\n",
    "1. **Phase 1 - BM25 Baseline**: Traditional term-based retrieval with tuned k1 and b parameters\n",
    "2. **Phase 2 - BM25 + RM3**: Query expansion using pseudo-relevance feedback (tuning fb_terms, fb_docs, original_weight)\n",
    "3. **Phase 3 - BM25 + RM3 + MiniLM**: Neural reranking with MiniLM-L6 cross-encoder fine-tuned on MS MARCO (tuning rerank_k, batch_size)\n",
    "\n",
    "**Evaluation**: \n",
    "- Metric: Mean Average Precision (MAP) on 50 training queries\n",
    "- Grid search for optimal hyperparameters at each phase\n",
    "- Results cached for reproducibility\n",
    "\n",
    "**Output**: \n",
    "Three TREC-format run files (run_1.res, run_2.res, run_3.res) for the 199 test queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ec658",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4888f",
   "metadata": {},
   "source": [
    "### 1.1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04b58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install -y openjdk-21-jdk\n",
    "# !update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-21-openjdk-amd64/bin/java 1\n",
    "# !update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac 1\n",
    "# !update-alternatives --set java /usr/lib/jvm/java-21-openjdk-amd64/bin/java\n",
    "# !update-alternatives --set javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6da3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install transformers\n",
    "# !pip install sentence-transformers\n",
    "# !pip install pytrec_eval\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install faiss-cpu --no-cache\n",
    "# !pip install pyserini==0.36.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cff9e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-01-15 00:42:27.355786485 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✓ Dependencies imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec6c3c",
   "metadata": {},
   "source": [
    "### 1.2. Load Pyserini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d89b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jan 15, 2026 12:42:28 AM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: robust04\n",
      "Total documents: 528,030\n",
      "Total terms: 174,540,872\n",
      "✓ Pyserini index loaded\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"robust04\"\n",
    "\n",
    "searcher = LuceneSearcher.from_prebuilt_index(INDEX_NAME)\n",
    "index_reader = IndexReader.from_prebuilt_index(INDEX_NAME)\n",
    "\n",
    "print(f\"Index: {INDEX_NAME}\")\n",
    "print(f\"Total documents: {index_reader.stats()['documents']:,}\")\n",
    "print(f\"Total terms: {index_reader.stats()['total_terms']:,}\")\n",
    "print(\"✓ Pyserini index loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b2621",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5a950",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note**: The 50 queries with qrels are used for training/tuning (parameter optimization via grid search). The remaining 199 queries without qrels are used for generating final submission files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5e6b1",
   "metadata": {},
   "source": [
    "### 2.1. Load Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac88eb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries loaded: 249\n",
      "\n",
      "Sample queries:\n",
      "  301: international organized crime\n",
      "  302: poliomyelitis post polio\n",
      "  303: hubble telescope achievements\n",
      "  304: endangered species mammals\n",
      "  305: dangerous vehicles\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "def load_queries(filepath: str) -> Dict[str, str]:\n",
    "    \"\"\"Load queries from file. Format: qid<tab>query_text\"\"\"\n",
    "    queries = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                qid, text = parts\n",
    "                queries[qid] = text\n",
    "    return queries\n",
    "\n",
    "all_queries = load_queries(os.path.join(DATA_DIR, \"queriesROBUST.txt\"))\n",
    "\n",
    "print(f\"Total queries loaded: {len(all_queries)}\")\n",
    "print(f\"\\nSample queries:\")\n",
    "for qid, text in list(all_queries.items())[:5]:\n",
    "    print(f\"  {qid}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16000c0a",
   "metadata": {},
   "source": [
    "### 2.2. Load Relevance Judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a1a9a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries with relevance judgments: 50\n",
      "Total judgments: 61,511\n",
      "\n",
      "Sample qrels for query 301:\n",
      "  FBIS3-10082: 1\n",
      "  FBIS3-10169: 0\n",
      "  FBIS3-10243: 1\n",
      "  FBIS3-10319: 0\n",
      "  FBIS3-10397: 1\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(filepath: str) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"Load qrels. Format: qid 0 docid relevance\"\"\"\n",
    "    qrels = defaultdict(dict)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 4:\n",
    "                qid, _, docid, rel = parts[:4]\n",
    "                qrels[qid][docid] = int(rel)\n",
    "    return dict(qrels)\n",
    "\n",
    "qrels = load_qrels(os.path.join(DATA_DIR, \"qrels_50_Queries\"))\n",
    "\n",
    "print(f\"Queries with relevance judgments: {len(qrels)}\")\n",
    "print(f\"Total judgments: {sum(len(v) for v in qrels.values()):,}\")\n",
    "print(f\"\\nSample qrels for query 301:\")\n",
    "sample_rels = list(qrels.get(\"301\", {}).items())[:5]\n",
    "for docid, rel in sample_rels:\n",
    "    print(f\"  {docid}: {rel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95332a",
   "metadata": {},
   "source": [
    "### 2.3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5779bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training queries: 50 (with qrels)\n",
      "Test queries: 199 (no qrels)\n",
      "\n",
      "Train QIDs: ['301', '302', '303', '304', '305', '306', '307', '308', '309', '310']...\n",
      "Test QIDs: ['351', '352', '353', '354', '355', '356', '357', '358', '359', '360']...\n"
     ]
    }
   ],
   "source": [
    "train_qids = sorted(qrels.keys())\n",
    "test_qids = [qid for qid in all_queries.keys() if qid not in train_qids]\n",
    "\n",
    "train_queries = {qid: all_queries[qid] for qid in train_qids}\n",
    "test_queries = {qid: all_queries[qid] for qid in test_qids}\n",
    "\n",
    "print(f\"Training queries: {len(train_queries)} (with qrels)\")\n",
    "print(f\"Test queries: {len(test_queries)} (no qrels)\")\n",
    "print(f\"\\nTrain QIDs: {train_qids[:10]}...\")\n",
    "print(f\"Test QIDs: {test_qids[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2aa8a3",
   "metadata": {},
   "source": [
    "## 3. Evaluation Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b429060",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Evaluation Strategy**: All experiments use MAP (Mean Average Precision) computed with pytrec_eval. Grid search explores parameter combinations, caching results to avoid redundant computation. Best parameters from each phase are carried forward to the next phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c0234",
   "metadata": {},
   "source": [
    "### 3.1. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab176d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def compute_map(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    qrels: Dict[str, Dict[str, int]]\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute MAP using pytrec_eval (TREC standard).\n",
    "\n",
    "    Returns:\n",
    "        map_score: float\n",
    "        per_query_ap: Dict[qid, AP]\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert run to pytrec_eval format: {qid: {docid: score}}\n",
    "    run_dict = {\n",
    "        qid: {docid: score for docid, score in docs}\n",
    "        for qid, docs in run.items()\n",
    "    }\n",
    "\n",
    "    # Convert qrels to pytrec_eval format: {qid: {docid: relevance}}\n",
    "    qrels_dict = {\n",
    "        qid: dict(docs)\n",
    "        for qid, docs in qrels.items()\n",
    "    }\n",
    "\n",
    "    # --- SAFETY CHECKS (critical) ---\n",
    "    missing_qids = set(qrels_dict) - set(run_dict)\n",
    "    assert not missing_qids, (\n",
    "        f\"Missing queries in run: {sorted(list(missing_qids))[:10]}\"\n",
    "    )\n",
    "\n",
    "    for qid, docs in run.items():\n",
    "        docids = [d for d, _ in docs]\n",
    "        assert len(docids) == len(set(docids)), (\n",
    "            f\"Duplicate docIDs in run for query {qid}\"\n",
    "        )\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels_dict, {\"map\"})\n",
    "    results = evaluator.evaluate(run_dict)\n",
    "\n",
    "    # Per-query AP\n",
    "    per_query_ap = {qid: m[\"map\"] for qid, m in results.items()}\n",
    "\n",
    "    # Aggregate MAP exactly as TREC does\n",
    "    map_score = pytrec_eval.compute_aggregated_measure(\n",
    "        \"map\",\n",
    "        list(per_query_ap.values())\n",
    "    )\n",
    "\n",
    "    return map_score, per_query_ap\n",
    "\n",
    "\n",
    "def evaluate_run(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    qrels: Dict[str, Dict[str, int]],\n",
    "    run_name: str = \"run\"\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate a run using pytrec_eval and return metrics.\"\"\"\n",
    "    map_score, per_query_ap = compute_map(run, qrels)\n",
    "    \n",
    "    return {\n",
    "        \"run_name\": run_name,\n",
    "        \"map\": map_score,\n",
    "        \"num_queries\": len(per_query_ap),\n",
    "        \"per_query_ap\": per_query_ap\n",
    "    }\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1ce08",
   "metadata": {},
   "source": [
    "### 3.2. Caching Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357dddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Caching utilities defined\n"
     ]
    }
   ],
   "source": [
    "def get_results_csv(phase_num: int = None) -> str:\n",
    "    \"\"\"Get CSV filename for a specific phase.\"\"\"\n",
    "    if phase_num is None:\n",
    "        return \"./experiments.csv\"\n",
    "    return f\"./experiments_phase{phase_num}.csv\"\n",
    "\n",
    "\n",
    "def generate_config_key(method: str, params: Dict) -> str:\n",
    "    \"\"\"Generate unique config key for caching.\"\"\"\n",
    "    parts = [method]\n",
    "    for k, v in sorted(params.items()):\n",
    "        parts.append(f\"{k}={v}\")\n",
    "    return \"__\".join(parts)\n",
    "\n",
    "\n",
    "def load_cached_result(config_key: str, phase_num: int = None) -> Optional[Dict]:\n",
    "    \"\"\"Load cached result if exists.\"\"\"\n",
    "    results_csv = get_results_csv(phase_num)\n",
    "    \n",
    "    if not os.path.exists(results_csv):\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(results_csv)\n",
    "    row = df[df[\"config_key\"] == config_key]\n",
    "    \n",
    "    if row.empty:\n",
    "        return None\n",
    "    \n",
    "    return row.iloc[0].to_dict()\n",
    "\n",
    "\n",
    "def save_experiment_result(result: Dict, phase_num: int = None):\n",
    "    \"\"\"Save experiment result to phase-specific cache.\"\"\"\n",
    "    results_csv = get_results_csv(phase_num)\n",
    "    df_row = pd.DataFrame([result])\n",
    "    \n",
    "    if not os.path.exists(results_csv):\n",
    "        df_row.to_csv(results_csv, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(results_csv, mode=\"a\", header=False, index=False)\n",
    "\n",
    "\n",
    "print(\"✓ Caching utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634949d",
   "metadata": {},
   "source": [
    "### 3.3. Base Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea6a3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BaseRetriever class defined\n"
     ]
    }
   ],
   "source": [
    "class BaseRetriever(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all retrieval models.\n",
    "    All models must implement search() and get_params().\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, index_name: str = \"robust04\"):\n",
    "        self.index_name = index_name\n",
    "    \n",
    "    @abstractmethod\n",
    "    def search(\n",
    "        self,\n",
    "        queries: Dict[str, str],\n",
    "        k: int = 1000\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        \"\"\"Search for all queries and return ranked results.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_params(self) -> Dict:\n",
    "        \"\"\"Return model parameters for logging.\"\"\"\n",
    "        return {}\n",
    "\n",
    "\n",
    "print(\"✓ BaseRetriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023aa3c",
   "metadata": {},
   "source": [
    "### 3.4. Experiment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b40836a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Experiment framework defined\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    config_key: str,\n",
    "    model_name: str,\n",
    "    model_class: type,\n",
    "    model_params: Dict,\n",
    "    queries: Dict[str, str],\n",
    "    qrels: Dict[str, Dict[str, int]]\n",
    ") -> Dict:\n",
    "    \"\"\"Run a single experiment using a retriever class.\"\"\"\n",
    "    model = model_class(**model_params)\n",
    "    run = model.search(queries, k=1000)\n",
    "    metrics = evaluate_run(run, qrels, config_key)\n",
    "    \n",
    "    return {\n",
    "        \"config_key\": config_key,\n",
    "        \"method\": model_name,\n",
    "        **model.get_params(),\n",
    "        \"map\": metrics[\"map\"],\n",
    "        \"num_queries\": metrics[\"num_queries\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def run_grid_search(\n",
    "    method_name: str,\n",
    "    model_class: type,\n",
    "    param_grid: Dict[str, list],\n",
    "    queries: Dict[str, str],\n",
    "    qrels: Dict[str, Dict[str, int]],\n",
    "    phase_num: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Grid search for retriever classes with phase-specific caching.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Print header\n",
    "    phase_str = f\"PHASE {phase_num}: \" if phase_num else \"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{phase_str}Tuning {method_name} parameters\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    \n",
    "    for combo in product(*param_values):\n",
    "        params = dict(zip(param_names, combo))\n",
    "        config_key = generate_config_key(method_name, params)\n",
    "        \n",
    "        # Check phase-specific cache\n",
    "        cached = load_cached_result(config_key, phase_num)\n",
    "        if cached is not None:\n",
    "            param_str = \", \".join([f\"{k}={v}\" for k, v in params.items()])\n",
    "            print(f\"{param_str} -> MAP={cached['map']:.4f} [CACHED]\")\n",
    "            results.append(cached)\n",
    "            continue\n",
    "        \n",
    "        # Run experiment\n",
    "        param_str = \", \".join([f\"{k}={v}\" for k, v in params.items()])\n",
    "        print(f\"{param_str} -> Running...\", end=\" \")\n",
    "        \n",
    "        result = run_experiment(\n",
    "            config_key=config_key,\n",
    "            model_name=method_name,\n",
    "            model_class=model_class,\n",
    "            model_params=params,\n",
    "            queries=queries,\n",
    "            qrels=qrels\n",
    "        )\n",
    "        \n",
    "        # Save to phase-specific cache\n",
    "        save_experiment_result(result, phase_num)\n",
    "        print(f\"MAP={result['map']:.4f}\")\n",
    "        results.append(result)\n",
    "    \n",
    "    # Create and sort results dataframe\n",
    "    df = pd.DataFrame(results).sort_values(\"map\", ascending=False)\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{method_name} Tuning Results (Top 5):\")\n",
    "    print(\"=\"*60)\n",
    "    display(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Experiment framework defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e034cf",
   "metadata": {},
   "source": [
    "## 4. PHASE 1: BM25 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efee42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Phase 1 Overview**: BM25 is a probabilistic ranking function based on term frequency and document length normalization. We tune k1 (term frequency saturation) and b (length normalization) to find optimal retrieval parameters for Robust04."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ad949",
   "metadata": {},
   "source": [
    "### 4.1. BM25 Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be810bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BM25Retriever class defined\n"
     ]
    }
   ],
   "source": [
    "class BM25Retriever(BaseRetriever):\n",
    "    \"\"\"BM25 retrieval model.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        k1: float = 0.9,\n",
    "        b: float = 0.4,\n",
    "        index_name: str = \"robust04\"\n",
    "    ):\n",
    "        super().__init__(index_name)\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self._searcher = None\n",
    "    \n",
    "    @property\n",
    "    def searcher(self) -> LuceneSearcher:\n",
    "        if self._searcher is None:\n",
    "            self._searcher = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "            self._searcher.set_bm25(k1=self.k1, b=self.b)\n",
    "        return self._searcher\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        queries: Dict[str, str],\n",
    "        k: int = 1000\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        results = {}\n",
    "        for qid, query_text in tqdm(queries.items(), desc=\"BM25 Search\"):\n",
    "            hits = self.searcher.search(query_text, k=k)\n",
    "            results[qid] = [(hit.docid, hit.score) for hit in hits]\n",
    "        return results\n",
    "    \n",
    "    def get_params(self) -> Dict:\n",
    "        return {\"k1\": self.k1, \"b\": self.b}\n",
    "\n",
    "\n",
    "print(\"✓ BM25Retriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882ee79",
   "metadata": {},
   "source": [
    "### 4.2. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df83f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 1: Tuning BM25 parameters\n",
      "============================================================\n",
      "\n",
      "k1=0.6, b=0.3 -> MAP=0.2463 [CACHED]\n",
      "k1=0.6, b=0.4 -> MAP=0.2475 [CACHED]\n",
      "k1=0.6, b=0.5 -> MAP=0.2453 [CACHED]\n",
      "k1=0.6, b=0.6 -> MAP=0.2422 [CACHED]\n",
      "k1=0.6, b=0.75 -> MAP=0.2392 [CACHED]\n",
      "k1=0.9, b=0.3 -> MAP=0.2450 [CACHED]\n",
      "k1=0.9, b=0.4 -> MAP=0.2455 [CACHED]\n",
      "k1=0.9, b=0.5 -> MAP=0.2442 [CACHED]\n",
      "k1=0.9, b=0.6 -> MAP=0.2415 [CACHED]\n",
      "k1=0.9, b=0.75 -> MAP=0.2374 [CACHED]\n",
      "k1=1.2, b=0.3 -> MAP=0.2426 [CACHED]\n",
      "k1=1.2, b=0.4 -> MAP=0.2427 [CACHED]\n",
      "k1=1.2, b=0.5 -> MAP=0.2424 [CACHED]\n",
      "k1=1.2, b=0.6 -> MAP=0.2400 [CACHED]\n",
      "k1=1.2, b=0.75 -> MAP=0.2334 [CACHED]\n",
      "k1=1.5, b=0.3 -> MAP=0.2392 [CACHED]\n",
      "k1=1.5, b=0.4 -> MAP=0.2396 [CACHED]\n",
      "k1=1.5, b=0.5 -> MAP=0.2384 [CACHED]\n",
      "k1=1.5, b=0.6 -> MAP=0.2360 [CACHED]\n",
      "k1=1.5, b=0.75 -> MAP=0.2307 [CACHED]\n",
      "k1=2.0, b=0.3 -> MAP=0.2320 [CACHED]\n",
      "k1=2.0, b=0.4 -> MAP=0.2330 [CACHED]\n",
      "k1=2.0, b=0.5 -> MAP=0.2324 [CACHED]\n",
      "k1=2.0, b=0.6 -> MAP=0.2292 [CACHED]\n",
      "k1=2.0, b=0.75 -> MAP=0.2250 [CACHED]\n",
      "\n",
      "============================================================\n",
      "BM25 Tuning Results (Top 5):\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>method</th>\n",
       "      <th>k1</th>\n",
       "      <th>b</th>\n",
       "      <th>map</th>\n",
       "      <th>num_queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25__b=0.4__k1=0.6</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.247464</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25__b=0.3__k1=0.6</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.246328</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BM25__b=0.4__k1=0.9</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.245466</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BM25__b=0.5__k1=0.6</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.245285</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BM25__b=0.3__k1=0.9</td>\n",
       "      <td>BM25</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.244954</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            config_key method   k1    b       map  num_queries\n",
       "1  BM25__b=0.4__k1=0.6   BM25  0.6  0.4  0.247464           50\n",
       "0  BM25__b=0.3__k1=0.6   BM25  0.6  0.3  0.246328           50\n",
       "6  BM25__b=0.4__k1=0.9   BM25  0.9  0.4  0.245466           50\n",
       "2  BM25__b=0.5__k1=0.6   BM25  0.6  0.5  0.245285           50\n",
       "5  BM25__b=0.3__k1=0.9   BM25  0.9  0.3  0.244954           50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BM25_PARAM_GRID = {\n",
    "    \"k1\": [0.6, 0.9, 1.2, 1.5, 2.0],\n",
    "    \"b\": [0.3, 0.4, 0.5, 0.6, 0.75]\n",
    "}\n",
    "\n",
    "# Run grid search with BM25Retriever class\n",
    "bm25_df = run_grid_search(\n",
    "    method_name=\"BM25\",\n",
    "    model_class=BM25Retriever,\n",
    "    param_grid=BM25_PARAM_GRID,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    phase_num=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522b7f3",
   "metadata": {},
   "source": [
    "### 4.3. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f82722cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1 COMPLETE: Best BM25 Parameters\n",
      "============================================================\n",
      "  k1 = 0.6\n",
      "  b = 0.4\n",
      "  MAP = 0.2475\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25 Search: 100%|██████████| 50/50 [00:07<00:00,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Best BM25 model ready for Phase 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_bm25 = bm25_df.iloc[0]\n",
    "best_k1, best_b = best_bm25[\"k1\"], best_bm25[\"b\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1 COMPLETE: Best BM25 Parameters\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  k1 = {best_k1}\")\n",
    "print(f\"  b = {best_b}\")\n",
    "print(f\"  MAP = {best_bm25['map']:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create best BM25 model\n",
    "bm25_model = BM25Retriever(k1=best_k1, b=best_b)\n",
    "bm25_run_train = bm25_model.search(train_queries, k=1000)\n",
    "\n",
    "print(\"\\n✓ Best BM25 model ready for Phase 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90683dc7",
   "metadata": {},
   "source": [
    "## 5. PHASE 2: BM25 + RM3 (Query Expansion)\n",
    "\n",
    "Using best BM25 parameters from Phase 1 to tune RM3 hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9d2ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Phase 2 Overview**: RM3 (Relevance Model 3) improves retrieval through pseudo-relevance feedback. It expands the original query with terms from top-ranked documents, then re-retrieves with the expanded query. Parameters: fb_terms (expansion terms), fb_docs (feedback documents), original_weight (balance between original and expanded query)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af424f9",
   "metadata": {},
   "source": [
    "### 5.1. RM3 Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "420d3e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RM3Retriever class defined\n"
     ]
    }
   ],
   "source": [
    "class RM3Retriever(BaseRetriever):\n",
    "    \"\"\"BM25 + RM3 pseudo-relevance feedback.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        k1: float = 0.9,\n",
    "        b: float = 0.4,\n",
    "        fb_terms: int = 10,\n",
    "        fb_docs: int = 10,\n",
    "        original_weight: float = 0.5,\n",
    "        index_name: str = \"robust04\"\n",
    "    ):\n",
    "        super().__init__(index_name)\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.fb_terms = fb_terms\n",
    "        self.fb_docs = fb_docs\n",
    "        self.original_weight = original_weight\n",
    "        self._searcher = None\n",
    "    \n",
    "    @property\n",
    "    def searcher(self) -> LuceneSearcher:\n",
    "        if self._searcher is None:\n",
    "            self._searcher = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "            self._searcher.set_bm25(k1=self.k1, b=self.b)\n",
    "            self._searcher.set_rm3(\n",
    "                fb_terms=self.fb_terms,\n",
    "                fb_docs=self.fb_docs,\n",
    "                original_query_weight=self.original_weight\n",
    "            )\n",
    "        return self._searcher\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        queries: Dict[str, str],\n",
    "        k: int = 1000\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        results = {}\n",
    "        for qid, query_text in tqdm(queries.items(), desc=\"RM3 Search\"):\n",
    "            hits = self.searcher.search(query_text, k=k)\n",
    "            results[qid] = [(hit.docid, hit.score) for hit in hits]\n",
    "        return results\n",
    "    \n",
    "    def get_params(self) -> Dict:\n",
    "        return {\n",
    "            \"k1\": self.k1,\n",
    "            \"b\": self.b,\n",
    "            \"fb_terms\": self.fb_terms,\n",
    "            \"fb_docs\": self.fb_docs,\n",
    "            \"original_weight\": self.original_weight\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✓ RM3Retriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0426d107",
   "metadata": {},
   "source": [
    "### 5.2. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91071833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best BM25 params: k1=0.6, b=0.4\n",
      "\n",
      "============================================================\n",
      "PHASE 2: Tuning RM3 parameters\n",
      "============================================================\n",
      "\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=5, original_weight=0.4 -> MAP=0.2591 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=5, original_weight=0.5 -> MAP=0.2654 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=5, original_weight=0.6 -> MAP=0.2660 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=10, original_weight=0.4 -> MAP=0.2520 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=10, original_weight=0.5 -> MAP=0.2570 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=10, original_weight=0.6 -> MAP=0.2599 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=15, original_weight=0.4 -> MAP=0.2548 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=15, original_weight=0.5 -> MAP=0.2597 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=10, fb_docs=15, original_weight=0.6 -> MAP=0.2615 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=5, original_weight=0.4 -> MAP=0.2699 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=5, original_weight=0.5 -> MAP=0.2709 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=5, original_weight=0.6 -> MAP=0.2702 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=10, original_weight=0.4 -> MAP=0.2631 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=10, original_weight=0.5 -> MAP=0.2666 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=10, original_weight=0.6 -> MAP=0.2641 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=15, original_weight=0.4 -> MAP=0.2688 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=15, original_weight=0.5 -> MAP=0.2738 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=25, fb_docs=15, original_weight=0.6 -> MAP=0.2698 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=5, original_weight=0.4 -> MAP=0.2722 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=5, original_weight=0.5 -> MAP=0.2713 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=5, original_weight=0.6 -> MAP=0.2685 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=10, original_weight=0.4 -> MAP=0.2633 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=10, original_weight=0.5 -> MAP=0.2657 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=10, original_weight=0.6 -> MAP=0.2659 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=15, original_weight=0.4 -> MAP=0.2737 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=15, original_weight=0.5 -> MAP=0.2717 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=50, fb_docs=15, original_weight=0.6 -> MAP=0.2680 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=5, original_weight=0.4 -> MAP=0.2725 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=5, original_weight=0.5 -> MAP=0.2719 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=5, original_weight=0.6 -> MAP=0.2675 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=10, original_weight=0.4 -> MAP=0.2625 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=10, original_weight=0.5 -> MAP=0.2647 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=10, original_weight=0.6 -> MAP=0.2670 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=15, original_weight=0.4 -> MAP=0.2699 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=15, original_weight=0.5 -> MAP=0.2709 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=75, fb_docs=15, original_weight=0.6 -> MAP=0.2695 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=5, original_weight=0.4 -> MAP=0.2767 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=5, original_weight=0.5 -> MAP=0.2727 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=5, original_weight=0.6 -> MAP=0.2671 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=10, original_weight=0.4 -> MAP=0.2665 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=10, original_weight=0.5 -> MAP=0.2655 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=10, original_weight=0.6 -> MAP=0.2660 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=15, original_weight=0.4 -> MAP=0.2698 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=15, original_weight=0.5 -> MAP=0.2745 [CACHED]\n",
      "k1=0.6, b=0.4, fb_terms=100, fb_docs=15, original_weight=0.6 -> MAP=0.2686 [CACHED]\n",
      "\n",
      "============================================================\n",
      "RM3 Tuning Results (Top 5):\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>method</th>\n",
       "      <th>k1</th>\n",
       "      <th>b</th>\n",
       "      <th>fb_terms</th>\n",
       "      <th>fb_docs</th>\n",
       "      <th>original_weight</th>\n",
       "      <th>map</th>\n",
       "      <th>num_queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.276720</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RM3__b=0.4__fb_docs=15__fb_terms=100__k1=0.6__...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.274451</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RM3__b=0.4__fb_docs=15__fb_terms=25__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.273836</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RM3__b=0.4__fb_docs=15__fb_terms=50__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.273685</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...</td>\n",
       "      <td>RM3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.272742</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           config_key method   k1    b  \\\n",
       "36  RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...    RM3  0.6  0.4   \n",
       "43  RM3__b=0.4__fb_docs=15__fb_terms=100__k1=0.6__...    RM3  0.6  0.4   \n",
       "16  RM3__b=0.4__fb_docs=15__fb_terms=25__k1=0.6__o...    RM3  0.6  0.4   \n",
       "24  RM3__b=0.4__fb_docs=15__fb_terms=50__k1=0.6__o...    RM3  0.6  0.4   \n",
       "37  RM3__b=0.4__fb_docs=5__fb_terms=100__k1=0.6__o...    RM3  0.6  0.4   \n",
       "\n",
       "    fb_terms  fb_docs  original_weight       map  num_queries  \n",
       "36       100        5              0.4  0.276720           50  \n",
       "43       100       15              0.5  0.274451           50  \n",
       "16        25       15              0.5  0.273836           50  \n",
       "24        50       15              0.4  0.273685           50  \n",
       "37       100        5              0.5  0.272742           50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RM3_PARAM_GRID = {\n",
    "    \"k1\": [best_k1],        # fixed from BM25 tuning\n",
    "    \"b\": [best_b],          # fixed from BM25 tuning\n",
    "    \"fb_terms\": [10, 25, 50, 75, 100],\n",
    "    \"fb_docs\": [5, 10, 15],\n",
    "    \"original_weight\": [0.4, 0.5, 0.6],\n",
    "}\n",
    "\n",
    "# Run grid search with RM3Retriever class\n",
    "print(f\"Using best BM25 params: k1={best_k1}, b={best_b}\\n\")\n",
    "\n",
    "rm3_df = run_grid_search(\n",
    "    method_name=\"RM3\",\n",
    "    model_class=RM3Retriever,\n",
    "    param_grid=RM3_PARAM_GRID,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    phase_num=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71ccf2",
   "metadata": {},
   "source": [
    "### 5.3. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aa11dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 2 COMPLETE: Best RM3 Parameters\n",
      "============================================================\n",
      "  BM25 k1 = 0.6\n",
      "  BM25 b = 0.4\n",
      "  fb_terms = 100\n",
      "  fb_docs = 5\n",
      "  original_weight = 0.4\n",
      "  MAP = 0.2767\n",
      "\n",
      "  Improvement over BM25: +11.82%\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RM3 Search: 100%|██████████| 50/50 [00:07<00:00,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Best RM3 model ready for Phase 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_rm3 = rm3_df.iloc[0]\n",
    "best_fb_terms = int(best_rm3[\"fb_terms\"])\n",
    "best_fb_docs = int(best_rm3[\"fb_docs\"])\n",
    "best_orig_w = best_rm3[\"original_weight\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2 COMPLETE: Best RM3 Parameters\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  BM25 k1 = {best_k1}\")\n",
    "print(f\"  BM25 b = {best_b}\")\n",
    "print(f\"  fb_terms = {best_fb_terms}\")\n",
    "print(f\"  fb_docs = {best_fb_docs}\")\n",
    "print(f\"  original_weight = {best_orig_w}\")\n",
    "print(f\"  MAP = {best_rm3['map']:.4f}\")\n",
    "\n",
    "improvement = (best_rm3['map'] - best_bm25['map']) / best_bm25['map'] * 100\n",
    "print(f\"\\n  Improvement over BM25: {improvement:+.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create best RM3 model\n",
    "rm3_model = RM3Retriever(\n",
    "    k1=best_k1, b=best_b,\n",
    "    fb_terms=best_fb_terms, fb_docs=best_fb_docs, original_weight=best_orig_w\n",
    ")\n",
    "rm3_run_train = rm3_model.search(train_queries, k=1000)\n",
    "\n",
    "print(\"\\n✓ Best RM3 model ready for Phase 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece92b96",
   "metadata": {},
   "source": [
    "## 6. PHASE 3: Multi-Branch Fusion + Neural PRF + MiniLM MaxP Reranking\n",
    "\n",
    "Using best BM25/RM3 parameters from Phases 1-2 with advanced neural reranking pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5ddf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Phase 3 Overview**: Advanced 4-stage neural reranking pipeline:\n",
    "\n",
    "1. **Multi-Branch Candidate Generation**: Run BM25, RM3, and optionally SPLADE (learned sparse retrieval) in parallel to gather diverse initial candidates\n",
    "\n",
    "2. **First-Stage RRF Fusion**: Apply Reciprocal Rank Fusion (RRF) to merge the multi-branch results, combining complementary ranking signals\n",
    "\n",
    "3. **Neural PRF + MiniLM MaxP Reranking**: \n",
    "   - Extract top passages from feedback documents using MiniLM scoring (extractive neural pseudo-relevance feedback)\n",
    "   - Expand query with selected passages (no templates, direct concatenation)\n",
    "   - Rerank top-k documents using MaxP (maximum passage score) with expanded query\n",
    "   - Preserve tail documents (append unreranked documents to maintain depth)\n",
    "\n",
    "4. **Final RRF Fusion**: Merge first-stage fusion with reranked results for final ranking\n",
    "\n",
    "**Key Innovations**:\n",
    "- **Passage-level MaxP scoring**: Documents scored by their best passage rather than full text\n",
    "- **Extractive Neural PRF**: Query expansion uses neural model to select informative passages (no prompt templates)\n",
    "- **Multi-stage RRF**: Fusion applied both pre- and post-reranking for robustness\n",
    "\n",
    "- **SPLADE integration**: Optional learned sparse retrieval branch (requires prebuilt impact index)**Model**: cross-encoder/ms-marco-MiniLM-L-6-v2 (MS MARCO fine-tuned for passage ranking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6917a4b",
   "metadata": {},
   "source": [
    "### 6.1. Multi-Branch Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MultiBranchRetriever ready (RM3 + SPLADE + RRF + Neural PRF + MaxP)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# PHASE 3: RRF Fusion + Neural PRF + MiniLM MaxP (RM3 + SPLADE only)\n",
    "# ================================\n",
    "#\n",
    "# High-level pipeline:\n",
    "# 1. Multi-branch candidate generation (RM3, SPLADE)\n",
    "# 2. First-stage RRF fusion\n",
    "# 3. Neural PRF using passage-level MiniLM scoring (batched)\n",
    "# 4. MiniLM MaxP reranking over passages\n",
    "# 5. Final RRF fusion (fused vs. reranked)\n",
    "# ================================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyserini.search.lucene import LuceneSearcher, LuceneImpactSearcher\n",
    "from pyserini.encode import SpladeQueryEncoder\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "class MultiBranchRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    End-to-end retrieval pipeline for Robust04 using:\n",
    "      - RM3 lexical expansion\n",
    "      - SPLADE sparse neural retrieval\n",
    "      - RRF fusion\n",
    "      - Neural PRF\n",
    "      - MiniLM MaxP reranking\n",
    "    \"\"\"\n",
    "\n",
    "    TEXT_RE = re.compile(r\"<TEXT>(.*?)</TEXT>\", re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Initialization\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # RM3 parameters\n",
    "        fb_terms: int = 80,\n",
    "        fb_docs: int = 10,\n",
    "        original_weight: float = 0.3,\n",
    "        # RRF\n",
    "        rrf_k: int = 60,\n",
    "        # Neural PRF\n",
    "        prf_fb_docs: int = 8,\n",
    "        prf_passages: int = 5,\n",
    "        # Passage splitting\n",
    "        passage_size: int = 256,\n",
    "        passage_stride: int = 128,\n",
    "        # Reranking\n",
    "        rerank_k: int = 200,\n",
    "        batch_size: int = 32,\n",
    "        # Indexes\n",
    "        index_name: str = \"robust04\",\n",
    "        splade_index_name: str = \"beir-v1.0.0-robust04.splade-pp-ed\",\n",
    "        # Reranker model\n",
    "        model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    ):\n",
    "        super().__init__(index_name)\n",
    "\n",
    "        self.fb_terms = fb_terms\n",
    "        self.fb_docs = fb_docs\n",
    "        self.original_weight = original_weight\n",
    "        self.rrf_k = rrf_k\n",
    "        self.prf_fb_docs = prf_fb_docs\n",
    "        self.prf_passages = prf_passages\n",
    "        self.passage_size = passage_size\n",
    "        self.passage_stride = passage_stride\n",
    "        self.rerank_k = rerank_k\n",
    "        self.batch_size = batch_size\n",
    "        self.splade_index_name = splade_index_name\n",
    "        self.model_name = model_name\n",
    "        self.fusion_method = fusion_method\n",
    "\n",
    "        self._rm3 = None\n",
    "        self._splade = None\n",
    "        self._doc_searcher = None\n",
    "        self._reranker = None\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Searcher accessors\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    @property\n",
    "    def rm3(self) -> LuceneSearcher:\n",
    "        if self._rm3 is None:\n",
    "            s = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "            s.set_rm3(\n",
    "                fb_terms=self.fb_terms,\n",
    "                fb_docs=self.fb_docs,\n",
    "                original_query_weight=self.original_weight,\n",
    "            )\n",
    "            self._rm3 = s\n",
    "        return self._rm3\n",
    "\n",
    "    @property\n",
    "    def splade(self) -> LuceneImpactSearcher:\n",
    "        if self._splade is None:\n",
    "            self._splade = LuceneImpactSearcher.from_prebuilt_index(\n",
    "                self.splade_index_name,\n",
    "                SpladeQueryEncoder(\n",
    "                    \"naver/splade-cocondenser-ensembledistil\",\n",
    "                    device=DEVICE,\n",
    "                ),\n",
    "            )\n",
    "        return self._splade\n",
    "\n",
    "    @property\n",
    "    def doc_searcher(self) -> LuceneSearcher:\n",
    "        if self._doc_searcher is None:\n",
    "            self._doc_searcher = LuceneSearcher.from_prebuilt_index(self.index_name)\n",
    "        return self._doc_searcher\n",
    "\n",
    "    @property\n",
    "    def reranker(self) -> CrossEncoder:\n",
    "        if self._reranker is None:\n",
    "            self._reranker = CrossEncoder(\n",
    "                self.model_name,\n",
    "                max_length=512,\n",
    "                device=DEVICE,\n",
    "            )\n",
    "        return self._reranker\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Document & passage utilities\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    @lru_cache(maxsize=10000)\n",
    "    def _get_doc_content(self, docid: str) -> str:\n",
    "        doc = self.doc_searcher.doc(docid)\n",
    "        if doc is None:\n",
    "            return \"\"\n",
    "        raw = doc.raw() or \"\"\n",
    "        texts = self.TEXT_RE.findall(raw)\n",
    "        content = \" \".join(texts) if texts else re.sub(r\"<[^>]+>\", \" \", raw)\n",
    "        return re.sub(r\"\\s+\", \" \", content).strip()\n",
    "\n",
    "    @lru_cache(maxsize=10000)\n",
    "    def _doc_passages(self, docid: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split document into overlapping word-level passages.\n",
    "        \"\"\"\n",
    "        text = self._get_doc_content(docid)\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        words = text.split()\n",
    "        passages = []\n",
    "        for i in range(0, len(words), self.passage_stride):\n",
    "            chunk = words[i : i + self.passage_size]\n",
    "            if len(chunk) >= 30:\n",
    "                passages.append(\" \".join(chunk))\n",
    "        return passages\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Fusion helpers\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _rrf_fuse(\n",
    "        self,\n",
    "        runs: List[Dict[str, List[Tuple[str, float]]]],\n",
    "        top_k: int,\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        \"\"\"\n",
    "        Reciprocal Rank Fusion over multiple runs.\n",
    "        \"\"\"\n",
    "        fused: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "        qids = set()\n",
    "        for r in runs:\n",
    "            qids.update(r.keys())\n",
    "\n",
    "        for qid in qids:\n",
    "            for run in runs:\n",
    "                ranked = run.get(qid, [])\n",
    "                for rank, (docid, _) in enumerate(ranked, start=1):\n",
    "                    fused[qid][docid] += 1.0 / (self.rrf_k + rank)\n",
    "\n",
    "        return {\n",
    "            qid: sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "            for qid, scores in fused.items()\n",
    "        }\n",
    "\n",
    "    def _maxp_aggregate(\n",
    "        self,\n",
    "        scores: List[float],\n",
    "        docids: List[str],\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        MaxP aggregation:\n",
    "        For each document, keep the maximum passage score.\n",
    "        \"\"\"\n",
    "        doc_scores = defaultdict(lambda: -1e9)\n",
    "        for score, docid in zip(scores, docids):\n",
    "            doc_scores[docid] = max(doc_scores[docid], float(score))\n",
    "        return doc_scores\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Neural PRF\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _build_expansion(self, query: str, docids: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Neural pseudo-relevance feedback:\n",
    "        Score passages from top documents and concatenate the best ones.\n",
    "        \"\"\"\n",
    "        pairs, passages = [], []\n",
    "\n",
    "        for docid in docids[: self.prf_fb_docs]:\n",
    "            for p in self._doc_passages(docid):\n",
    "                pairs.append([query, p])\n",
    "                passages.append(p)\n",
    "\n",
    "        if not pairs:\n",
    "            return \"\"\n",
    "\n",
    "        scores = self.reranker.predict(\n",
    "            pairs,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "\n",
    "        ranked = sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)\n",
    "        return \" \".join(p for p, _ in ranked[: self.prf_passages])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Main search\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        queries: Dict[str, str],\n",
    "        k: int = 1000,\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Stage 1: Candidate generation (RM3 + SPLADE)\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        rm3_run, splade_run = {}, {}\n",
    "\n",
    "        for qid, q in tqdm(queries.items(), desc=\"RM3\"):\n",
    "            rm3_run[qid] = [(h.docid, float(h.score)) for h in self.rm3.search(q, k)]\n",
    "\n",
    "        for qid, q in tqdm(queries.items(), desc=\"SPLADE\"):\n",
    "            splade_run[qid] = [(h.docid, float(h.score)) for h in self.splade.search(q, k)]\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Stage 2: First-stage RRF\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        fused = self._rrf_fuse([rm3_run, splade_run], top_k=k)\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Stage 3–4: Neural PRF + MiniLM MaxP reranking\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        reranked = {}\n",
    "\n",
    "        for qid, q in tqdm(queries.items(), desc=\"Neural PRF + MaxP\"):\n",
    "            cand = fused.get(qid, [])\n",
    "            docids = [d for d, _ in cand]\n",
    "\n",
    "            # Neural PRF query expansion\n",
    "            expansion = self._build_expansion(q, docids)\n",
    "            expanded_query = f\"{q} {expansion}\".strip() if expansion else q\n",
    "\n",
    "            # Build passage pairs for reranking\n",
    "            pairs, meta = [], []\n",
    "            for docid in docids[: self.rerank_k]:\n",
    "                for p in self._doc_passages(docid):\n",
    "                    pairs.append([expanded_query, p])\n",
    "                    meta.append(docid)\n",
    "\n",
    "            if pairs:\n",
    "                scores = self.reranker.predict(\n",
    "                    pairs,\n",
    "                    batch_size=self.batch_size,\n",
    "                    show_progress_bar=False,\n",
    "                )\n",
    "                doc_scores = self._maxp_aggregate(scores, meta)\n",
    "                head = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            else:\n",
    "                head = []\n",
    "\n",
    "            seen = {d for d, _ in head}\n",
    "            tail = [(d, s) for d, s in cand if d not in seen]\n",
    "            reranked[qid] = head + tail\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Stage 5: Final RRF\n",
    "        # --------------------------------------------------------------\n",
    "        return self._rrf_fuse([fused, reranked], top_k=k)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Params\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def get_params(self) -> Dict:\n",
    "        return {\n",
    "            \"fb_terms\": self.fb_terms,\n",
    "            \"fb_docs\": self.fb_docs,\n",
    "            \"original_weight\": self.original_weight,\n",
    "            \"rrf_k\": self.rrf_k,\n",
    "            \"prf_fb_docs\": self.prf_fb_docs,\n",
    "            \"prf_passages\": self.prf_passages,\n",
    "            \"passage_size\": self.passage_size,\n",
    "            \"passage_stride\": self.passage_stride,\n",
    "            \"rerank_k\": self.rerank_k,\n",
    "            \"batch_size\": self.batch_size,\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✓ MultiBranchRetriever ready (RM3 + SPLADE + RRF + Neural PRF + MaxP)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfde70e",
   "metadata": {},
   "source": [
    "### 6.2. Parameter Tuning\n",
    "\n",
    "**Fixed from Phases 1-2**: k1, b, fb_terms, fb_docs, original_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9740f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best RM3 params from Phase 2:\n",
      "  RM3: fb_terms=100, fb_docs=5, original_weight=0.4\n",
      "\n",
      "Running Phase 3 multi-branch (RM3 + SPLADE + RRF + Neural PRF + MaxP) grid search...\n",
      "\n",
      "============================================================\n",
      "PHASE 3: Tuning MultiBranch_RRF parameters\n",
      "============================================================\n",
      "\n",
      "fb_terms=100, fb_docs=5, original_weight=0.4, rrf_k=40, prf_fb_docs=5, prf_passages=3, passage_size=192, passage_stride=128, rerank_k=200, batch_size=32 -> Running... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RM3: 100%|██████████| 50/50 [00:05<00:00,  9.72it/s]\n",
      "SPLADE:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize pre-built index beir-v1.0.0-robust04.splade-pp-ed.\n",
      "/home/galnoy/.cache/pyserini/indexes/lucene-inverted.beir-v1.0.0-robust04.splade-pp-ed.20231124.a66f86f.c1a6fd094bb9e34e69e10040d9b0ad2a already exists, skipping download.\n",
      "Initializing beir-v1.0.0-robust04.splade-pp-ed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPLADE: 100%|██████████| 50/50 [00:07<00:00,  7.04it/s]\n",
      "Neural PRF + MaxP: 100%|██████████| 50/50 [06:35<00:00,  7.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP=0.3025\n",
      "fb_terms=100, fb_docs=5, original_weight=0.4, rrf_k=40, prf_fb_docs=5, prf_passages=3, passage_size=192, passage_stride=128, rerank_k=400, batch_size=32 -> Running... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RM3: 100%|██████████| 50/50 [00:05<00:00,  9.16it/s]\n",
      "SPLADE:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize pre-built index beir-v1.0.0-robust04.splade-pp-ed.\n",
      "/home/galnoy/.cache/pyserini/indexes/lucene-inverted.beir-v1.0.0-robust04.splade-pp-ed.20231124.a66f86f.c1a6fd094bb9e34e69e10040d9b0ad2a already exists, skipping download.\n",
      "Initializing beir-v1.0.0-robust04.splade-pp-ed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPLADE: 100%|██████████| 50/50 [00:05<00:00,  8.81it/s]\n",
      "Neural PRF + MaxP: 100%|██████████| 50/50 [13:01<00:00, 15.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP=0.3027\n",
      "fb_terms=100, fb_docs=5, original_weight=0.4, rrf_k=40, prf_fb_docs=5, prf_passages=3, passage_size=192, passage_stride=128, rerank_k=600, batch_size=32 -> Running... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RM3: 100%|██████████| 50/50 [00:05<00:00,  9.71it/s]\n",
      "SPLADE:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize pre-built index beir-v1.0.0-robust04.splade-pp-ed.\n",
      "/home/galnoy/.cache/pyserini/indexes/lucene-inverted.beir-v1.0.0-robust04.splade-pp-ed.20231124.a66f86f.c1a6fd094bb9e34e69e10040d9b0ad2a already exists, skipping download.\n",
      "Initializing beir-v1.0.0-robust04.splade-pp-ed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPLADE: 100%|██████████| 50/50 [00:05<00:00,  9.89it/s]\n",
      "Neural PRF + MaxP:  62%|██████▏   | 31/50 [11:46<06:52, 21.74s/it]"
     ]
    }
   ],
   "source": [
    "MULTI_BRANCH_PARAM_GRID = {\n",
    "    # ----- RM3 (fixed from Phase 2) -----\n",
    "    \"fb_terms\": [best_fb_terms],\n",
    "    \"fb_docs\": [best_fb_docs],\n",
    "    \"original_weight\": [best_orig_w],\n",
    "\n",
    "    # ----- RRF -----\n",
    "    \"rrf_k\": [40, 60],\n",
    "\n",
    "    # ----- Neural PRF -----\n",
    "    \"prf_fb_docs\": [5, 8],\n",
    "    \"prf_passages\": [3, 5],\n",
    "\n",
    "    # ----- Passage extraction (fixed) -----\n",
    "    \"passage_size\": [192],\n",
    "    \"passage_stride\": [128],\n",
    "\n",
    "    # ----- Reranking depth -----\n",
    "    \"rerank_k\": [200, 400, 600],\n",
    "\n",
    "    # ----- Runtime -----\n",
    "    \"batch_size\": [32],\n",
    "}\n",
    "\n",
    "print(\"Using best RM3 params from Phase 2:\")\n",
    "print(f\"  RM3: fb_terms={best_fb_terms}, fb_docs={best_fb_docs}, original_weight={best_orig_w}\")\n",
    "print(\"\\nRunning Phase 3 multi-branch (RM3 + SPLADE + RRF + Neural PRF + MaxP) grid search...\\n\")\n",
    "\n",
    "multi_branch_df = run_grid_search(\n",
    "    method_name=\"MultiBranch_RRF\",\n",
    "    model_class=MultiBranchRetriever,\n",
    "    param_grid=MULTI_BRANCH_PARAM_GRID,\n",
    "    queries=train_queries,\n",
    "    qrels=qrels,\n",
    "    phase_num=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6009bbc",
   "metadata": {},
   "source": [
    "### 6.3. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5359b6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3 COMPLETE: Best Multi-Branch Pipeline Parameters\n",
      "================================================================================\n",
      "  Stage 1 - Multi-Branch Candidate Generation:\n",
      "    BM25: k1=0.6, b=0.4\n",
      "    RM3: fb_terms=100, fb_docs=5, orig_w=0.4\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'use_splade'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'use_splade'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    BM25: k1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_k1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, b=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    RM3: fb_terms=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_fb_terms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, fb_docs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_fb_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, orig_w=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_orig_w\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    SPLADE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[43mbest_multi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_splade\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Stage 2 - First RRF Fusion:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    rrf_k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_rrf_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/pandas/core/series.py:1133\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/pandas/core/series.py:1249\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1249\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'use_splade'"
     ]
    }
   ],
   "source": [
    "best_multi = multi_branch_df.iloc[0]\n",
    "best_rrf_k = int(best_multi[\"rrf_k\"])\n",
    "best_prf_fb_docs = int(best_multi[\"prf_fb_docs\"])\n",
    "best_prf_passages = int(best_multi[\"prf_passages\"])\n",
    "best_passage_size = int(best_multi[\"passage_size\"])\n",
    "best_passage_stride = int(best_multi[\"passage_stride\"])\n",
    "best_rerank_k = int(best_multi[\"rerank_k\"])\n",
    "best_batch_size = int(best_multi[\"batch_size\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3 COMPLETE: Best Multi-Branch Pipeline Parameters\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Stage 1 - Multi-Branch Candidate Generation:\")\n",
    "print(f\"    BM25: k1={best_k1}, b={best_b}\")\n",
    "print(f\"    RM3: fb_terms={best_fb_terms}, fb_docs={best_fb_docs}, orig_w={best_orig_w}\")\n",
    "print(f\"    SPLADE: {'disabled' if not best_multi['use_splade'] else 'enabled'}\")\n",
    "print(f\"  Stage 2 - First RRF Fusion:\")\n",
    "print(f\"    rrf_k={best_rrf_k}\")\n",
    "print(f\"  Stage 3 - Neural PRF + MiniLM MaxP Reranking:\")\n",
    "print(f\"    prf_fb_docs={best_prf_fb_docs}, prf_passages={best_prf_passages}\")\n",
    "print(f\"    passage_size={best_passage_size}, passage_stride={best_passage_stride}\")\n",
    "print(f\"    rerank_k={best_rerank_k}, batch_size={best_batch_size}\")\n",
    "print(f\"  Stage 4 - Final RRF Fusion\")\n",
    "print(f\"\\n  FINAL MAP = {best_multi['map']:.4f}\")\n",
    "\n",
    "improvement_vs_bm25 = (best_multi['map'] - best_bm25['map']) / best_bm25['map'] * 100\n",
    "improvement_vs_rm3 = (best_multi['map'] - best_rm3['map']) / best_rm3['map'] * 100\n",
    "print(f\"\\n  Improvement over BM25: {improvement_vs_bm25:+.2f}%\")\n",
    "print(f\"  Improvement over RM3: {improvement_vs_rm3:+.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create best multi-branch model\n",
    "multi_branch_model = MultiBranchRetriever(\n",
    "    k1=best_k1,\n",
    "    b=best_b,\n",
    "    fb_terms=best_fb_terms,\n",
    "    fb_docs=best_fb_docs,\n",
    "    original_weight=best_orig_w,\n",
    "    rrf_k=best_rrf_k,\n",
    "    prf_fb_docs=best_prf_fb_docs,\n",
    "    prf_passages=best_prf_passages,\n",
    "    passage_size=best_passage_size,\n",
    "    passage_stride=best_passage_stride,\n",
    "    rerank_k=best_rerank_k,\n",
    "    batch_size=best_batch_size,\n",
    "    use_splade=best_multi['use_splade']\n",
    ")\n",
    "multi_branch_run_train = multi_branch_model.search(train_queries, k=1000)\n",
    "\n",
    "print(\"\\n✓ All 3 phases complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58802f78",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff00ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Summary**: This section compares the three methods on the 50 training queries with relevance judgments. Each phase builds on the previous one, showing incremental improvements in MAP scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527a6da",
   "metadata": {},
   "source": [
    "### 7.1. Training Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a86717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "3-PHASE TRAINING PERFORMANCE SUMMARY (50 queries with qrels)\n",
      "================================================================================\n",
      "  Phase                            Method      MAP Improvement\n",
      "Phase 1                              BM25 0.247464    baseline\n",
      "Phase 2                        BM25 + RM3 0.271283      +9.63%\n",
      "Phase 3 Hybrid Neural (BM25+RM3+Reranker) 0.251136      +1.48%\n",
      "================================================================================\n",
      "\n",
      "Best configurations:\n",
      "  Phase 1 (BM25): k1=0.6, b=0.4\n",
      "  Phase 2 (RM3): fb_terms=20, fb_docs=15, orig_w=0.5\n",
      "  Phase 3 (Hybrid): rerank_depth=100, rrf_k=30\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        \"Phase\": \"Phase 1\",\n",
    "        \"Method\": \"BM25\",\n",
    "        \"MAP\": best_bm25[\"map\"],\n",
    "        \"Improvement\": \"baseline\"\n",
    "    },\n",
    "    {\n",
    "        \"Phase\": \"Phase 2\",\n",
    "        \"Method\": \"BM25 + RM3\",\n",
    "        \"MAP\": best_rm3[\"map\"],\n",
    "        \"Improvement\": f\"+{(best_rm3['map'] - best_bm25['map']) / best_bm25['map'] * 100:.2f}%\"\n",
    "    },\n",
    "    {\n",
    "        \"Phase\": \"Phase 3\",\n",
    "        \"Method\": \"Multi-Branch + Neural PRF + MiniLM MaxP\",\n",
    "        \"MAP\": best_multi[\"map\"],\n",
    "        \"Improvement\": f\"+{(best_multi['map'] - best_bm25['map']) / best_bm25['map'] * 100:.2f}%\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"3-PHASE TRAINING PERFORMANCE SUMMARY (50 queries with qrels)\")\n",
    "print(\"=\"*80)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest configurations:\")\n",
    "print(f\"  Phase 1 (BM25): k1={best_k1}, b={best_b}\")\n",
    "print(f\"  Phase 2 (RM3): fb_terms={best_fb_terms}, fb_docs={best_fb_docs}, orig_w={best_orig_w}\")\n",
    "print(f\"  Phase 3 (Multi-Branch): rrf_k={best_rrf_k}, prf_fb_docs={best_prf_fb_docs}, \")\n",
    "print(f\"                          prf_passages={best_prf_passages}, rerank_k={best_rerank_k}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47c634",
   "metadata": {},
   "source": [
    "### 7.2. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f817abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ All methods tuned and evaluated on training queries\n",
      "\n",
      "→ Ready to generate submission files for test queries\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "methods = summary[\"Method\"].tolist()\n",
    "maps = summary[\"MAP\"].tolist()\n",
    "colors = [\"#3498db\", \"#2ecc71\", \"#e74c3c\"]\n",
    "\n",
    "bars = ax.bar(methods, maps, color=colors, edgecolor=\"black\")\n",
    "ax.set_ylabel(\"MAP\")\n",
    "ax.set_title(\"Training Performance Comparison (50 queries)\")\n",
    "ax.set_ylim(0, max(maps) * 1.15)\n",
    "\n",
    "for bar, m in zip(bars, maps):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f\"{m:.4f}\", ha=\"center\", va=\"bottom\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ All methods tuned and evaluated on training queries\")\n",
    "print(\"\\n→ Ready to generate submission files for test queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493be89",
   "metadata": {},
   "source": [
    "## 8. Generate Submission Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc2b33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Final Step**: Using the best configurations from each phase, we generate predictions for the 199 test queries (without relevance judgments). Each method produces a TREC-formatted run file with 1000 ranked documents per query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7959369",
   "metadata": {},
   "source": [
    "### 8.1. Run Inference on Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8eee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING SUBMISSION FILES: 199 test queries (no qrels)\n",
      "================================================================================\n",
      "\n",
      "Run 1: BM25 (Phase 1 best)\n",
      "  Parameters: k1=0.6, b=0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25 Search: 100%|██████████| 199/199 [00:16<00:00, 12.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 199 queries processed\n",
      "\n",
      "Run 2: BM25 + RM3 (Phase 2 best)\n",
      "  Parameters: k1=0.6, b=0.4, fb_terms=20, fb_docs=15, orig_w=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RM3 Search:  39%|███▊      | 77/199 [00:06<00:10, 11.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRun 2: BM25 + RM3 (Phase 2 best)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Parameters: k1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_k1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, b=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, fb_terms=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_fb_terms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, fb_docs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_fb_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, orig_w=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_orig_w\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m run_2 \u001b[38;5;241m=\u001b[39m \u001b[43mrm3_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ✓ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(run_2)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m queries processed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Method 3: Hybrid Neural (Phase 3 best)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 40\u001b[0m, in \u001b[0;36mRM3Retriever.search\u001b[0;34m(self, queries, k)\u001b[0m\n\u001b[1;32m     38\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qid, query_text \u001b[38;5;129;01min\u001b[39;00m tqdm(queries\u001b[38;5;241m.\u001b[39mitems(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRM3 Search\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     hits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     results[qid] \u001b[38;5;241m=\u001b[39m [(hit\u001b[38;5;241m.\u001b[39mdocid, hit\u001b[38;5;241m.\u001b[39mscore) \u001b[38;5;28;01mfor\u001b[39;00m hit \u001b[38;5;129;01min\u001b[39;00m hits]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/pyserini/search/lucene/_searcher.py:145\u001b[0m, in \u001b[0;36mLuceneSearcher.search\u001b[0;34m(self, q, k, query_generator, fields, strip_segment_id, remove_dups)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fields:\n\u001b[0;32m--> 145\u001b[0m         hits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m         hits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobject\u001b[38;5;241m.\u001b[39msearch_fields(q, jfields, k)\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:1187\u001b[0m, in \u001b[0;36mjnius.JavaMultipleMethod.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:897\u001b[0m, in \u001b[0;36mjnius.JavaMethod.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:986\u001b[0m, in \u001b[0;36mjnius.JavaMethod.call_method\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_conversion.pxi:352\u001b[0m, in \u001b[0;36mjnius.convert_jarray_to_python\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_conversion.pxi:232\u001b[0m, in \u001b[0;36mjnius.convert_jobject_to_python\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:2017\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:20\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39m_handle\u001b[38;5;241m.\u001b[39mis_done()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_stopped\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Python 3.12 and earlier has this\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Thread__stopped\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(f\"GENERATING SUBMISSION FILES: {len(test_queries)} test queries (no qrels)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Method 1: BM25 (Phase 1 best)\n",
    "print(\"Run 1: BM25 (Phase 1 best)\")\n",
    "print(f\"  Parameters: k1={best_k1}, b={best_b}\")\n",
    "run_1 = bm25_model.search(test_queries, k=1000)\n",
    "print(f\"  ✓ {len(run_1)} queries processed\")\n",
    "\n",
    "# Method 2: BM25 + RM3 (Phase 2 best)\n",
    "print(\"\\nRun 2: BM25 + RM3 (Phase 2 best)\")\n",
    "print(f\"  Parameters: k1={best_k1}, b={best_b}, fb_terms={best_fb_terms}, fb_docs={best_fb_docs}, orig_w={best_orig_w}\")\n",
    "run_2 = rm3_model.search(test_queries, k=1000)\n",
    "print(f\"  ✓ {len(run_2)} queries processed\")\n",
    "\n",
    "# Method 3: Multi-Branch Pipeline (Phase 3 best)\n",
    "print(\"\\nRun 3: Multi-Branch + Neural PRF + MiniLM MaxP (Phase 3 best)\")\n",
    "print(f\"  BM25/RM3: k1={best_k1}, b={best_b}, fb_terms={best_fb_terms}, fb_docs={best_fb_docs}, orig_w={best_orig_w}\")\n",
    "print(f\"  Pipeline: rrf_k={best_rrf_k}, prf_fb_docs={best_prf_fb_docs}, prf_passages={best_prf_passages}\")\n",
    "print(f\"  Reranking: rerank_k={best_rerank_k}, batch_size={best_batch_size}\")\n",
    "run_3 = multi_branch_model.search(test_queries, k=1000)\n",
    "print(f\"  ✓ {len(run_3)} queries processed\")\n",
    "\n",
    "print(\"\\n✓ All test queries processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f94a8",
   "metadata": {},
   "source": [
    "### 8.2. Export TREC Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2789317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_trec_run(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    filepath: str,\n",
    "    run_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Write run to TREC format.\n",
    "    Format: topic_id Q0 doc_id rank score run_name\n",
    "    \"\"\"\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for qid in sorted(run.keys(), key=lambda x: int(x)):\n",
    "            results = run[qid]\n",
    "            # Sort by score descending\n",
    "            sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "            for rank, (docid, score) in enumerate(sorted_results[:1000], start=1):\n",
    "                f.write(f\"{qid} Q0 {docid} {rank} {score:.6f} {run_name}\\n\")\n",
    "    \n",
    "    print(f\"✓ Written: {filepath}\")\n",
    "\n",
    "\n",
    "print(\"Writing submission files...\\n\")\n",
    "\n",
    "write_trec_run(run_1, \"run_1.res\", \"run_1\")\n",
    "write_trec_run(run_2, \"run_2.res\", \"run_2\")\n",
    "write_trec_run(run_3, \"run_3.res\", \"run_3\")\n",
    "\n",
    "print(\"\\n✓ All submission files written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38715c",
   "metadata": {},
   "source": [
    "### 8.3. Validate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_run_file(filepath: str, expected_queries: int = 199, docs_per_query: int = 1000):\n",
    "    \"\"\"Validate TREC run file format and contents.\"\"\"\n",
    "    query_docs = defaultdict(list)\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            qid, _, docid, rank, score, _ = parts\n",
    "            query_docs[qid].append((int(rank), float(score)))\n",
    "    \n",
    "    # Check query count\n",
    "    assert len(query_docs) == expected_queries, f\"Expected {expected_queries} queries, got {len(query_docs)}\"\n",
    "    \n",
    "    # Check docs per query and score ordering\n",
    "    for qid, docs in query_docs.items():\n",
    "        assert len(docs) == docs_per_query, f\"Query {qid}: expected {docs_per_query} docs, got {len(docs)}\"\n",
    "        scores = [s for _, s in sorted(docs, key=lambda x: x[0])]\n",
    "        assert scores == sorted(scores, reverse=True), f\"Query {qid}: scores not in decreasing order\"\n",
    "    \n",
    "    print(f\"✓ {filepath}: {len(query_docs)} queries × {docs_per_query} docs, scores non-increasing\")\n",
    "\n",
    "\n",
    "print(\"Validating submission files...\\n\")\n",
    "\n",
    "validate_run_file(\"run_1.res\")\n",
    "validate_run_file(\"run_2.res\")\n",
    "validate_run_file(\"run_3.res\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION FILES READY\")\n",
    "print(\"=\"*60)\n",
    "print(\"Files: run_1.res, run_2.res, run_3.res\")\n",
    "print(\"Format: TREC 6-column\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
