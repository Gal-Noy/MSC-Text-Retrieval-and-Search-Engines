{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61ec658",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4888f",
   "metadata": {},
   "source": [
    "### 1.1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6da3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyserini\n",
    "# !pip install faiss-cpu\n",
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cff9e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galnoy/git-projects/MSC-Text-Retrieval-and-Search-Engines/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[0;93m2026-01-09 15:18:52.217615681 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✓ Dependencies imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec6c3c",
   "metadata": {},
   "source": [
    "### 1.2. Load Pyserini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d89b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: robust04\n",
      "Total documents: 528,030\n",
      "Total terms: 174,540,872\n",
      "✓ Pyserini index loaded\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"robust04\"\n",
    "\n",
    "searcher = LuceneSearcher.from_prebuilt_index(INDEX_NAME)\n",
    "index_reader = IndexReader.from_prebuilt_index(INDEX_NAME)\n",
    "\n",
    "print(f\"Index: {INDEX_NAME}\")\n",
    "print(f\"Total documents: {index_reader.stats()['documents']:,}\")\n",
    "print(f\"Total terms: {index_reader.stats()['total_terms']:,}\")\n",
    "print(\"✓ Pyserini index loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b2621",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5e6b1",
   "metadata": {},
   "source": [
    "### 2.1. Load Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac88eb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries loaded: 249\n",
      "\n",
      "Sample queries:\n",
      "  301: international organized crime\n",
      "  302: poliomyelitis post polio\n",
      "  303: hubble telescope achievements\n",
      "  304: endangered species mammals\n",
      "  305: dangerous vehicles\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "def load_queries(filepath: str) -> Dict[str, str]:\n",
    "    \"\"\"Load queries from file. Format: qid<tab>query_text\"\"\"\n",
    "    queries = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                qid, text = parts\n",
    "                queries[qid] = text\n",
    "    return queries\n",
    "\n",
    "all_queries = load_queries(os.path.join(DATA_DIR, \"queriesROBUST.txt\"))\n",
    "\n",
    "print(f\"Total queries loaded: {len(all_queries)}\")\n",
    "print(f\"\\nSample queries:\")\n",
    "for qid, text in list(all_queries.items())[:5]:\n",
    "    print(f\"  {qid}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16000c0a",
   "metadata": {},
   "source": [
    "### 2.2. Load Relevance Judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1a9a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries with relevance judgments: 50\n",
      "Total judgments: 61,511\n",
      "\n",
      "Sample qrels for query 301:\n",
      "  FBIS3-10082: 1\n",
      "  FBIS3-10169: 0\n",
      "  FBIS3-10243: 1\n",
      "  FBIS3-10319: 0\n",
      "  FBIS3-10397: 1\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(filepath: str) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"Load qrels. Format: qid 0 docid relevance\"\"\"\n",
    "    qrels = defaultdict(dict)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 4:\n",
    "                qid, _, docid, rel = parts[:4]\n",
    "                qrels[qid][docid] = int(rel)\n",
    "    return dict(qrels)\n",
    "\n",
    "qrels = load_qrels(os.path.join(DATA_DIR, \"qrels_50_Queries\"))\n",
    "\n",
    "print(f\"Queries with relevance judgments: {len(qrels)}\")\n",
    "print(f\"Total judgments: {sum(len(v) for v in qrels.values()):,}\")\n",
    "print(f\"\\nSample qrels for query 301:\")\n",
    "sample_rels = list(qrels.get(\"301\", {}).items())[:5]\n",
    "for docid, rel in sample_rels:\n",
    "    print(f\"  {docid}: {rel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95332a",
   "metadata": {},
   "source": [
    "### 2.3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5779bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training queries: 50 (with qrels)\n",
      "Test queries: 199 (no qrels)\n",
      "\n",
      "Train QIDs: ['301', '302', '303', '304', '305', '306', '307', '308', '309', '310']...\n",
      "Test QIDs: ['351', '352', '353', '354', '355', '356', '357', '358', '359', '360']...\n"
     ]
    }
   ],
   "source": [
    "train_qids = sorted(qrels.keys())\n",
    "test_qids = [qid for qid in all_queries.keys() if qid not in train_qids]\n",
    "\n",
    "train_queries = {qid: all_queries[qid] for qid in train_qids}\n",
    "test_queries = {qid: all_queries[qid] for qid in test_qids}\n",
    "\n",
    "print(f\"Training queries: {len(train_queries)} (with qrels)\")\n",
    "print(f\"Test queries: {len(test_queries)} (no qrels)\")\n",
    "print(f\"\\nTrain QIDs: {train_qids[:10]}...\")\n",
    "print(f\"Test QIDs: {test_qids[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2aa8a3",
   "metadata": {},
   "source": [
    "## 3. Evaluation Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8609b",
   "metadata": {},
   "source": [
    "### 3.1. MAP Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab176d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MAP evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def compute_ap(ranked_docs: List[str], relevance: Dict[str, int]) -> float:\n",
    "    \"\"\"Compute Average Precision for a single query.\"\"\"\n",
    "    relevant = {d for d, r in relevance.items() if r > 0}\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    \n",
    "    hits = 0\n",
    "    precision_sum = 0.0\n",
    "    \n",
    "    for i, doc in enumerate(ranked_docs):\n",
    "        if doc in relevant:\n",
    "            hits += 1\n",
    "            precision_sum += hits / (i + 1)\n",
    "    \n",
    "    return precision_sum / len(relevant)\n",
    "\n",
    "\n",
    "def compute_map(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    qrels: Dict[str, Dict[str, int]]\n",
    ") -> float:\n",
    "    \"\"\"Compute Mean Average Precision over all queries.\"\"\"\n",
    "    aps = []\n",
    "    for qid, results in run.items():\n",
    "        if qid in qrels:\n",
    "            ranked_docs = [doc for doc, _ in results]\n",
    "            ap = compute_ap(ranked_docs, qrels[qid])\n",
    "            aps.append(ap)\n",
    "    return np.mean(aps) if aps else 0.0\n",
    "\n",
    "\n",
    "print(\"✓ MAP evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89fd1df",
   "metadata": {},
   "source": [
    "### 3.2. Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf8c09a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation utilities defined\n"
     ]
    }
   ],
   "source": [
    "def search_batch(\n",
    "    searcher: LuceneSearcher,\n",
    "    queries: Dict[str, str],\n",
    "    k: int = 1000\n",
    ") -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"Run batch search and return results.\"\"\"\n",
    "    results = {}\n",
    "    for qid, query_text in tqdm(queries.items(), desc=\"Searching\"):\n",
    "        hits = searcher.search(query_text, k=k)\n",
    "        results[qid] = [(hit.docid, hit.score) for hit in hits]\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_run(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    qrels: Dict[str, Dict[str, int]],\n",
    "    run_name: str = \"run\"\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate a run and return metrics.\"\"\"\n",
    "    map_score = compute_map(run, qrels)\n",
    "    \n",
    "    # Per-query AP\n",
    "    per_query = {}\n",
    "    for qid in run:\n",
    "        if qid in qrels:\n",
    "            ranked = [d for d, _ in run[qid]]\n",
    "            per_query[qid] = compute_ap(ranked, qrels[qid])\n",
    "    \n",
    "    return {\n",
    "        \"run_name\": run_name,\n",
    "        \"map\": map_score,\n",
    "        \"num_queries\": len(per_query),\n",
    "        \"per_query_ap\": per_query\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e034cf",
   "metadata": {},
   "source": [
    "## 4. Method 1: BM25 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ad949",
   "metadata": {},
   "source": [
    "### 4.1. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be810bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bm25_searcher(k1: float = 0.9, b: float = 0.4) -> LuceneSearcher:\n",
    "    \"\"\"Create BM25 searcher with specified parameters.\"\"\"\n",
    "    s = LuceneSearcher.from_prebuilt_index(INDEX_NAME)\n",
    "    s.set_bm25(k1=k1, b=b)\n",
    "    return s\n",
    "\n",
    "\n",
    "print(\"✓ BM25 searcher factory defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882ee79",
   "metadata": {},
   "source": [
    "### 4.2. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_param_grid = {\n",
    "    \"k1\": [0.6, 0.9, 1.2, 1.5, 2.0],\n",
    "    \"b\": [0.3, 0.4, 0.5, 0.6, 0.75]\n",
    "}\n",
    "\n",
    "bm25_results = []\n",
    "\n",
    "print(\"Tuning BM25 parameters...\\n\")\n",
    "\n",
    "for k1, b in product(bm25_param_grid[\"k1\"], bm25_param_grid[\"b\"]):\n",
    "    s = create_bm25_searcher(k1=k1, b=b)\n",
    "    run = search_batch(s, train_queries, k=1000)\n",
    "    metrics = evaluate_run(run, qrels, f\"BM25_k1={k1}_b={b}\")\n",
    "    \n",
    "    bm25_results.append({\n",
    "        \"k1\": k1,\n",
    "        \"b\": b,\n",
    "        \"map\": metrics[\"map\"]\n",
    "    })\n",
    "    print(f\"k1={k1}, b={b} -> MAP={metrics['map']:.4f}\")\n",
    "\n",
    "bm25_df = pd.DataFrame(bm25_results).sort_values(\"map\", ascending=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BM25 Tuning Results (Top 5):\")\n",
    "print(\"=\"*50)\n",
    "print(bm25_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522b7f3",
   "metadata": {},
   "source": [
    "### 4.3. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82722cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bm25 = bm25_df.iloc[0]\n",
    "best_k1, best_b = best_bm25[\"k1\"], best_bm25[\"b\"]\n",
    "\n",
    "print(f\"Best BM25 Parameters:\")\n",
    "print(f\"  k1 = {best_k1}\")\n",
    "print(f\"  b = {best_b}\")\n",
    "print(f\"  MAP = {best_bm25['map']:.4f}\")\n",
    "\n",
    "bm25_searcher = create_bm25_searcher(k1=best_k1, b=best_b)\n",
    "bm25_run_train = search_batch(bm25_searcher, train_queries, k=1000)\n",
    "\n",
    "print(\"\\n✓ BM25 baseline configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90683dc7",
   "metadata": {},
   "source": [
    "## 5. Method 2: BM25 + RM3 (Query Expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af424f9",
   "metadata": {},
   "source": [
    "### 5.1. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rm3_searcher(\n",
    "    k1: float = 0.9,\n",
    "    b: float = 0.4,\n",
    "    fb_terms: int = 10,\n",
    "    fb_docs: int = 10,\n",
    "    original_weight: float = 0.5\n",
    ") -> LuceneSearcher:\n",
    "    \"\"\"Create BM25+RM3 searcher with specified parameters.\"\"\"\n",
    "    s = LuceneSearcher.from_prebuilt_index(INDEX_NAME)\n",
    "    s.set_bm25(k1=k1, b=b)\n",
    "    s.set_rm3(fb_terms=fb_terms, fb_docs=fb_docs, original_query_weight=original_weight)\n",
    "    return s\n",
    "\n",
    "\n",
    "print(\"✓ RM3 searcher factory defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0426d107",
   "metadata": {},
   "source": [
    "### 5.2. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91071833",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm3_param_grid = {\n",
    "    \"fb_terms\": [10, 20, 30],\n",
    "    \"fb_docs\": [5, 10, 15],\n",
    "    \"original_weight\": [0.3, 0.5, 0.7]\n",
    "}\n",
    "\n",
    "rm3_results = []\n",
    "\n",
    "print(\"Tuning RM3 parameters (using best BM25 params)...\\n\")\n",
    "\n",
    "for fb_terms, fb_docs, orig_w in product(\n",
    "    rm3_param_grid[\"fb_terms\"],\n",
    "    rm3_param_grid[\"fb_docs\"],\n",
    "    rm3_param_grid[\"original_weight\"]\n",
    "):\n",
    "    s = create_rm3_searcher(\n",
    "        k1=best_k1, b=best_b,\n",
    "        fb_terms=fb_terms, fb_docs=fb_docs, original_weight=orig_w\n",
    "    )\n",
    "    run = search_batch(s, train_queries, k=1000)\n",
    "    metrics = evaluate_run(run, qrels)\n",
    "    \n",
    "    rm3_results.append({\n",
    "        \"fb_terms\": fb_terms,\n",
    "        \"fb_docs\": fb_docs,\n",
    "        \"original_weight\": orig_w,\n",
    "        \"map\": metrics[\"map\"]\n",
    "    })\n",
    "    print(f\"fb_terms={fb_terms}, fb_docs={fb_docs}, orig_w={orig_w} -> MAP={metrics['map']:.4f}\")\n",
    "\n",
    "rm3_df = pd.DataFrame(rm3_results).sort_values(\"map\", ascending=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RM3 Tuning Results (Top 5):\")\n",
    "print(\"=\"*50)\n",
    "print(rm3_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71ccf2",
   "metadata": {},
   "source": [
    "### 5.3. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa11dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rm3 = rm3_df.iloc[0]\n",
    "best_fb_terms = int(best_rm3[\"fb_terms\"])\n",
    "best_fb_docs = int(best_rm3[\"fb_docs\"])\n",
    "best_orig_w = best_rm3[\"original_weight\"]\n",
    "\n",
    "print(f\"Best RM3 Parameters:\")\n",
    "print(f\"  fb_terms = {best_fb_terms}\")\n",
    "print(f\"  fb_docs = {best_fb_docs}\")\n",
    "print(f\"  original_weight = {best_orig_w}\")\n",
    "print(f\"  MAP = {best_rm3['map']:.4f}\")\n",
    "\n",
    "rm3_searcher = create_rm3_searcher(\n",
    "    k1=best_k1, b=best_b,\n",
    "    fb_terms=best_fb_terms, fb_docs=best_fb_docs, original_weight=best_orig_w\n",
    ")\n",
    "rm3_run_train = search_batch(rm3_searcher, train_queries, k=1000)\n",
    "\n",
    "improvement = (best_rm3['map'] - best_bm25['map']) / best_bm25['map'] * 100\n",
    "print(f\"\\nImprovement over BM25: {improvement:+.2f}%\")\n",
    "print(\"\\n✓ RM3 configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece92b96",
   "metadata": {},
   "source": [
    "## 6. Method 3: Hybrid Neural Re-ranking (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365fa660",
   "metadata": {},
   "source": [
    "### 6.1. First-Stage Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76b171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RERANK_DEPTH = 100  # Number of candidates to re-rank\n",
    "\n",
    "def get_candidates(\n",
    "    searcher: LuceneSearcher,\n",
    "    queries: Dict[str, str],\n",
    "    k: int = RERANK_DEPTH\n",
    ") -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"Get BM25 candidates for re-ranking.\"\"\"\n",
    "    return search_batch(searcher, queries, k=k)\n",
    "\n",
    "\n",
    "print(f\"✓ First-stage retrieval configured (depth={RERANK_DEPTH})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8870f3",
   "metadata": {},
   "source": [
    "### 6.2. Cross-Encoder Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32542fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "print(f\"Loading cross-encoder: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "cross_encoder = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "cross_encoder = cross_encoder.to(DEVICE)\n",
    "cross_encoder.eval()\n",
    "\n",
    "print(f\"✓ Cross-encoder loaded on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c44372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_text(docid: str) -> str:\n",
    "    \"\"\"Retrieve document text from index.\"\"\"\n",
    "    doc = searcher.doc(docid)\n",
    "    if doc is None:\n",
    "        return \"\"\n",
    "    raw = doc.raw()\n",
    "    # Extract text content\n",
    "    if raw:\n",
    "        return raw[:2000]  # Truncate for efficiency\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def rerank_with_cross_encoder(\n",
    "    query: str,\n",
    "    candidates: List[Tuple[str, float]],\n",
    "    batch_size: int = 32\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"Re-rank candidates using cross-encoder.\"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "    \n",
    "    docids = [d for d, _ in candidates]\n",
    "    docs = [get_doc_text(d) for d in docids]\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch_docs = docs[i:i+batch_size]\n",
    "        pairs = [[query, doc] for doc in batch_docs]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            pairs,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        outputs = cross_encoder(**inputs)\n",
    "        batch_scores = outputs.logits.squeeze(-1).cpu().numpy()\n",
    "        scores.extend(batch_scores.tolist() if batch_scores.ndim > 0 else [batch_scores.item()])\n",
    "    \n",
    "    # Sort by neural score\n",
    "    reranked = sorted(zip(docids, scores), key=lambda x: x[1], reverse=True)\n",
    "    return reranked\n",
    "\n",
    "\n",
    "print(\"✓ Cross-encoder re-ranking functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6917a4b",
   "metadata": {},
   "source": [
    "### 6.3. Reciprocal Rank Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(\n",
    "    runs: List[List[Tuple[str, float]]],\n",
    "    k: int = 60\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Combine multiple ranked lists using RRF.\n",
    "    RRF_score(d) = sum(1 / (k + rank_i(d))) for each run i\n",
    "    \"\"\"\n",
    "    rrf_scores = defaultdict(float)\n",
    "    \n",
    "    for run in runs:\n",
    "        for rank, (docid, _) in enumerate(run):\n",
    "            rrf_scores[docid] += 1.0 / (k + rank + 1)\n",
    "    \n",
    "    fused = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return fused\n",
    "\n",
    "\n",
    "print(\"✓ RRF fusion function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(\n",
    "    queries: Dict[str, str],\n",
    "    bm25_searcher: LuceneSearcher,\n",
    "    rerank_depth: int = 100,\n",
    "    rrf_k: int = 60,\n",
    "    final_k: int = 1000\n",
    ") -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"Full hybrid pipeline: BM25 -> Neural Re-rank -> RRF Fusion.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for qid, query_text in tqdm(queries.items(), desc=\"Hybrid search\"):\n",
    "        # First-stage: BM25\n",
    "        bm25_hits = bm25_searcher.search(query_text, k=rerank_depth)\n",
    "        bm25_candidates = [(hit.docid, hit.score) for hit in bm25_hits]\n",
    "        \n",
    "        # Second-stage: Neural re-ranking\n",
    "        neural_reranked = rerank_with_cross_encoder(query_text, bm25_candidates)\n",
    "        \n",
    "        # Fusion: RRF\n",
    "        fused = reciprocal_rank_fusion([bm25_candidates, neural_reranked], k=rrf_k)\n",
    "        \n",
    "        # Ensure we have 1000 results (pad with BM25 if needed)\n",
    "        if len(fused) < final_k:\n",
    "            extra_hits = bm25_searcher.search(query_text, k=final_k)\n",
    "            seen = {d for d, _ in fused}\n",
    "            for hit in extra_hits:\n",
    "                if hit.docid not in seen:\n",
    "                    fused.append((hit.docid, 0.0))\n",
    "                    if len(fused) >= final_k:\n",
    "                        break\n",
    "        \n",
    "        results[qid] = fused[:final_k]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✓ Hybrid search pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfde70e",
   "metadata": {},
   "source": [
    "### 6.4. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9740f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_param_grid = {\n",
    "    \"rerank_depth\": [50, 100],\n",
    "    \"rrf_k\": [30, 60, 90]\n",
    "}\n",
    "\n",
    "hybrid_results = []\n",
    "\n",
    "print(\"Tuning hybrid parameters...\\n\")\n",
    "\n",
    "for rerank_depth, rrf_k in product(\n",
    "    hybrid_param_grid[\"rerank_depth\"],\n",
    "    hybrid_param_grid[\"rrf_k\"]\n",
    "):\n",
    "    run = hybrid_search(\n",
    "        train_queries,\n",
    "        bm25_searcher,\n",
    "        rerank_depth=rerank_depth,\n",
    "        rrf_k=rrf_k\n",
    "    )\n",
    "    metrics = evaluate_run(run, qrels)\n",
    "    \n",
    "    hybrid_results.append({\n",
    "        \"rerank_depth\": rerank_depth,\n",
    "        \"rrf_k\": rrf_k,\n",
    "        \"map\": metrics[\"map\"]\n",
    "    })\n",
    "    print(f\"rerank_depth={rerank_depth}, rrf_k={rrf_k} -> MAP={metrics['map']:.4f}\")\n",
    "\n",
    "hybrid_df = pd.DataFrame(hybrid_results).sort_values(\"map\", ascending=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Hybrid Tuning Results:\")\n",
    "print(\"=\"*50)\n",
    "print(hybrid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab84a474",
   "metadata": {},
   "source": [
    "### 6.5. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hybrid = hybrid_df.iloc[0]\n",
    "best_rerank_depth = int(best_hybrid[\"rerank_depth\"])\n",
    "best_rrf_k = int(best_hybrid[\"rrf_k\"])\n",
    "\n",
    "print(f\"Best Hybrid Parameters:\")\n",
    "print(f\"  rerank_depth = {best_rerank_depth}\")\n",
    "print(f\"  rrf_k = {best_rrf_k}\")\n",
    "print(f\"  MAP = {best_hybrid['map']:.4f}\")\n",
    "\n",
    "hybrid_run_train = hybrid_search(\n",
    "    train_queries,\n",
    "    bm25_searcher,\n",
    "    rerank_depth=best_rerank_depth,\n",
    "    rrf_k=best_rrf_k\n",
    ")\n",
    "\n",
    "improvement = (best_hybrid['map'] - best_bm25['map']) / best_bm25['map'] * 100\n",
    "print(f\"\\nImprovement over BM25: {improvement:+.2f}%\")\n",
    "print(\"\\n✓ Hybrid neural re-ranking configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58802f78",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527a6da",
   "metadata": {},
   "source": [
    "### 7.1. Training Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a86717",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        \"Method\": \"BM25\",\n",
    "        \"MAP\": best_bm25[\"map\"],\n",
    "        \"Parameters\": f\"k1={best_k1}, b={best_b}\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"BM25 + RM3\",\n",
    "        \"MAP\": best_rm3[\"map\"],\n",
    "        \"Parameters\": f\"fb_terms={best_fb_terms}, fb_docs={best_fb_docs}, orig_w={best_orig_w}\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"Hybrid Neural (Advanced)\",\n",
    "        \"MAP\": best_hybrid[\"map\"],\n",
    "        \"Parameters\": f\"rerank_depth={best_rerank_depth}, rrf_k={best_rrf_k}\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING PERFORMANCE SUMMARY (50 queries)\")\n",
    "print(\"=\"*80)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47c634",
   "metadata": {},
   "source": [
    "### 7.2. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f817abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "methods = summary[\"Method\"].tolist()\n",
    "maps = summary[\"MAP\"].tolist()\n",
    "colors = [\"#3498db\", \"#2ecc71\", \"#e74c3c\"]\n",
    "\n",
    "bars = ax.bar(methods, maps, color=colors, edgecolor=\"black\")\n",
    "ax.set_ylabel(\"MAP\")\n",
    "ax.set_title(\"Training Performance Comparison (50 queries)\")\n",
    "ax.set_ylim(0, max(maps) * 1.15)\n",
    "\n",
    "for bar, m in zip(bars, maps):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f\"{m:.4f}\", ha=\"center\", va=\"bottom\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ All methods tuned and evaluated on training queries\")\n",
    "print(\"\\n→ Ready to generate submission files for test queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493be89",
   "metadata": {},
   "source": [
    "## 8. Generate Submission Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7959369",
   "metadata": {},
   "source": [
    "### 8.1. Run Inference on Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8eee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running inference on {len(test_queries)} test queries...\\n\")\n",
    "\n",
    "# Method 1: BM25\n",
    "print(\"Method 1: BM25\")\n",
    "run_1 = search_batch(bm25_searcher, test_queries, k=1000)\n",
    "print(f\"  ✓ {len(run_1)} queries processed\")\n",
    "\n",
    "# Method 2: BM25 + RM3\n",
    "print(\"\\nMethod 2: BM25 + RM3\")\n",
    "run_2 = search_batch(rm3_searcher, test_queries, k=1000)\n",
    "print(f\"  ✓ {len(run_2)} queries processed\")\n",
    "\n",
    "# Method 3: Hybrid Neural\n",
    "print(\"\\nMethod 3: Hybrid Neural Re-ranking\")\n",
    "run_3 = hybrid_search(\n",
    "    test_queries,\n",
    "    bm25_searcher,\n",
    "    rerank_depth=best_rerank_depth,\n",
    "    rrf_k=best_rrf_k,\n",
    "    final_k=1000\n",
    ")\n",
    "print(f\"  ✓ {len(run_3)} queries processed\")\n",
    "\n",
    "print(\"\\n✓ All test queries processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f94a8",
   "metadata": {},
   "source": [
    "### 8.2. Export TREC Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2789317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_trec_run(\n",
    "    run: Dict[str, List[Tuple[str, float]]],\n",
    "    filepath: str,\n",
    "    run_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Write run to TREC format.\n",
    "    Format: topic_id Q0 doc_id rank score run_name\n",
    "    \"\"\"\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for qid in sorted(run.keys(), key=lambda x: int(x)):\n",
    "            results = run[qid]\n",
    "            # Sort by score descending\n",
    "            sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "            for rank, (docid, score) in enumerate(sorted_results[:1000], start=1):\n",
    "                f.write(f\"{qid} Q0 {docid} {rank} {score:.6f} {run_name}\\n\")\n",
    "    \n",
    "    print(f\"✓ Written: {filepath}\")\n",
    "\n",
    "\n",
    "print(\"Writing submission files...\\n\")\n",
    "\n",
    "write_trec_run(run_1, \"run_1.res\", \"run_1\")\n",
    "write_trec_run(run_2, \"run_2.res\", \"run_2\")\n",
    "write_trec_run(run_3, \"run_3.res\", \"run_3\")\n",
    "\n",
    "print(\"\\n✓ All submission files written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38715c",
   "metadata": {},
   "source": [
    "### 8.3. Validate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_run_file(filepath: str, expected_queries: int = 199, docs_per_query: int = 1000):\n",
    "    \"\"\"Validate TREC run file format and contents.\"\"\"\n",
    "    query_docs = defaultdict(list)\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            qid, _, docid, rank, score, _ = parts\n",
    "            query_docs[qid].append((int(rank), float(score)))\n",
    "    \n",
    "    # Check query count\n",
    "    assert len(query_docs) == expected_queries, f\"Expected {expected_queries} queries, got {len(query_docs)}\"\n",
    "    \n",
    "    # Check docs per query and score ordering\n",
    "    for qid, docs in query_docs.items():\n",
    "        assert len(docs) == docs_per_query, f\"Query {qid}: expected {docs_per_query} docs, got {len(docs)}\"\n",
    "        scores = [s for _, s in sorted(docs, key=lambda x: x[0])]\n",
    "        assert scores == sorted(scores, reverse=True), f\"Query {qid}: scores not in decreasing order\"\n",
    "    \n",
    "    print(f\"✓ {filepath}: {len(query_docs)} queries × {docs_per_query} docs, scores non-increasing\")\n",
    "\n",
    "\n",
    "print(\"Validating submission files...\\n\")\n",
    "\n",
    "validate_run_file(\"run_1.res\")\n",
    "validate_run_file(\"run_2.res\")\n",
    "validate_run_file(\"run_3.res\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION FILES READY\")\n",
    "print(\"=\"*60)\n",
    "print(\"Files: run_1.res, run_2.res, run_3.res\")\n",
    "print(\"Format: TREC 6-column\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
